\documentclass[12pt]{article}
% \usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
% \geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\usepackage{graphicx}
\usepackage[font=small,skip=5pt]{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
% \usepackage{color}
\usepackage{multirow}
\usepackage{rotating}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\usepackage{hyperref}
\graphicspath{{figure/}}
% \usepackage{endfloat} % Figures to the end of the document

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%---------------------------------------------------
%                 Editing Commands
\newcommand{\hh}[1]{{\color{magenta} #1}}
\newcommand{\st}[1]{{\color{orange} #1}}

%---------------------------------------------------
%                 Placing Figures


%---------------------------------------------------
% Define new environment
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}
%---------------------------------------------------

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Model Visualization Techniques for a Social Network Model}
  \author{Samantha Tyner\thanks{
    The authors gratefully acknowledge funding from the National Science Foundation Grant \# DMS 1007697. All data collection has been conducted with approval from the Institutional Review Board IRB 10-347}\hspace{.2cm}\\
    Department of Statistics and Statistical Laboratory, Iowa State University\\
    and \\
    Heike Hofmann\\
    Department of Statistics and Statistical Laboratory, Iowa State University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Model Visualization Techniques for a Social Network Model}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
 \hh{XXX just a placeholder}
\end{abstract}

\noindent%
{\it Keywords:} social network analysis, model visualization, dynamic networks, network visualization, network mapping, animation
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\tableofcontents
\newpage

<<setup, echo = FALSE, message = FALSE, warning = FALSE>>=
options(replace.assign=TRUE,width=70, digits=3)
require(knitr)
opts_chunk$set(fig.path='figure/', cache.path='cache/', fig.align='center', fig.pos='h', out.width='.99\\linewidth', par=TRUE, cache=FALSE, concordance=TRUE, autodep=TRUE, message=F, warning=F, echo = FALSE)#, root.dir = "~/Desktop/Dissertation/SAOM-removing-blindfold/")
@

<<pkgs>>=
library(tidyverse)
library(RSiena)
library(network)
library(sna)
library(geomnet)
library(GGally)
library(netvizinf)
library(RColorBrewer)
library(gridExtra)
library(cowplot)

ThemeNoNet <- theme_bw() %+replace% 
            theme(plot.title = element_text(size = rel(2),
                                            face = 'bold', angle = 0), 
                  axis.title.x = element_text(size = rel(2),
                                            face = 'plain', angle = 0),
                  axis.title.y = element_text(size = rel(2),
                                            face = 'plain', angle = 90),
                  axis.text.x.top = element_text(size = rel(1.5),
                                            face = 'plain', angle = 0), 
                  axis.text.x.bottom = element_text(size = rel(1.5),
                                            face = 'plain', angle = 0), 
                  axis.text.y.left = element_text(size = rel(1.5),
                                            face = 'plain', angle = 0),
                  axis.text.y.right = element_text(size = rel(1.5),
                                            face = 'plain', angle = 0),
                  strip.text.x = element_text(size = rel(2),
                                            face = 'plain', angle = 0, 
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = rel(2),
                                            face = 'plain', angle = 90,margin = margin(t = 3, r = 3, b = 3, l = 3, unit = "pt")),
                  strip.background = element_rect(colour = "black", fill = "white")
                    )
ThemeNet <- theme_net() %+replace% 
            theme(plot.title = element_text(size = rel(2),
                                            face = 'bold', angle = 0), 
                  strip.text.x = element_text(size = rel(2),
                                            face = 'plain', angle = 0, 
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = rel(2),
                                            face = 'plain', angle = 0,margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.background = element_rect(colour = "black", fill = "white"))
@

\section{Introduction} 
% \st{
% \begin{itemize}
% %\item What? - model visualization and SAOMs. briefly dsecribe what these things are
% %\item Why? - bring light to the RSiena blackbox. Need ways to visualize network models, collections of networks. 
% \item How?
% \end{itemize}
% %also talk about other dynamic network models briefly to put SAOMs in context.
% }
\hh{XXX - talk a bit about social networks first, why we want to model them, and that usually the models are not dynamic.}
\st{XXX - okay, here is what i have done for that:
Social networks have been studied for decades, beginning with a few foundational works, including the 1967 study, "The Small World Problem" by Stanley Milgram \citep{goldenberg09}. Examples of social networks include collaboration networks between academic researchers, friendship networks in a school or university, and trade networks between nations. In recent years, the study of social networks has grown in popularity due to an increase in the availability and access to social network data. There are many kinds of social networks, but there are not as many statistical models for social network data.} Some network models that have been applied to social networks include the exponential random graph model and latent space models. These models, however, are only for single instance networks. If we only have one network observation, or only care about one state of a complex network in time, like a snapshot of the World Wide Web, using the well-established models for single network observating is not a problem. If, on the other hand, we have many observations of social network over time, these models may not be appropriate because they do not explicitly allow for the network to change as time passes. When studying a network over time, referred to as a \textit{dynamic network}, we need a model that can take the time aspect of the network into account. 
Models for dynamic social networks have a great deal of modelling potential because of how realistic their structure can be. A social network does not form spontaneously: it evolves over time. Ties are formed and dissolved, and new actors join the social structure. Modeling the underlying mechanisms that create network changes over time is very complex but also provides potential to uncover hidden truths. 

\par In this paper, we concentrate on one type of model for dynamic social networks: the stochastic actor-oriented models (SAOMs), introduced by \citet{saompaper}. These models are fundamentally different from other social network models because they allow us to incorporate network \textit{and} actor statistics, where other models only rely on the network statistics, to model the changes in the network. Allowing the actor-level statistics to directly effect the structure of the network leads to a more practical and relevant approach to model change in a social network. In the ``real" world we expect people with common interests to be more likely to form relationships, and SAOMs allow us to incorporate this intuition in the modeling process. 

\st{
%\par In order to estimate the parameters of  SAOMs the software SIENA, and its \texttt{R} implementation \texttt{RSiena}, was developed \cite{RSiena}. This software marks a huge contribution to the field of social network analysis., and when we began working with it we were eager to understand how it works. HH: we are still interested in this.
\hh{The next sentence is very vague - make it more specific: what aspect of SAOMs is not well understood? I think what you want is something like ``SAOMs are not very tractable analytically with traditional methods, e.g. likelihood functions turn into very complex objects because of the sheer number of parameters involved. Therefore, computationally more tractable solutions are used to fit estimators, in particular, SAOMs are fit using ... }
Unlike other network models, SAOMs are not very well understood. \st{They are relatively new, especially compared to the classic exponential random graph models, and they are not very tractable analytically. Likelihood functions quickly very complex objects to analyze due to the dependency structure inherent in the data. Therefore, computationally more tractable solutions are used to fit estimators, and in particular, % Traditional modeling steps, like determining the likelihood function, are very complex. Because of this, 
SAOMs are often fit to data using a series of Markov chain Monte Carlo (MCMC) phases for finding method of moments estimators. In order to estimate the parameters of SAOMs, we use the software SIENA, and its \texttt{R} implementation \texttt{RSiena}, which was developed by \citet{RSiena}. This software marks a huge contribution to the field of social network analysis, but the many moving pieces involved in parameter estimation are largely ``behind the scenes" and hidden from the software user.} In this paper, in order to better understand the model-fitting process, we attempt to bring SAOM fits and the fitting process out of their black boxes, by combining the principals of network visualization with those of model visualization as discussed in \citet{modelvis}. %Here, we explore RSiena and SAOMs with innovative network visualizations, both static and dynamic.
By bringing some light to the underlying methods and structures that are ``behind the scenes" of when fitting SAOMs to network data, we aim to help researchers working with the models better understand the implications and analyses of these models.
}

\par The guiding principles of model visualization introduced in \citet{modelvis} are: 
\begin{enumerate}
\item Display the model in the data space.
\item Collections of models are more informative than singletons. 
\item Explore the fitting \textit{process} rather than just looking at the final result.
\end{enumerate}
Stochastic actor-oriented models are a prime example of a set of models that can benefit greatly from application of model visualization. For instance, the models themselves include a continuous-time Markov chain (CTMC) that is completely hidden from the analyst in the model fitting process. Bringing the CTMC out of the black box and into the light through model visualization can provide researchers with insights into the underlying features of the model. Furthermore, SAOMs can include a great deal of parameters to be added to the model structure, each of which is attached to a network statistic. These statistics are often somewhat, if not highly, correlated, which causes high correlation between the associated parameters in a SAOM. By visualizing collections of SAOMs, we gain a better understanding of these correlations and find ways to deal with them and rectify their effects in the model. In addition, the estimation of the parameters in a SAOM relies on a Robbins-Monro algorithm, and the convergence checks for these estimates rely on simulation from the fitted model. Again, each of these steps are largely kept in the background of the estimation process. With the help of static and dynamic visualizations we bring the hidden model fitting processes into the foreground, eventually leading to a better understanding and higher accessibility of stochastic actor-oriented models for social network analysts. 

In Section~\ref{sec:netintro}, we introduce basic concepts of networks and network visualizations. In Section~\ref{sec:saoms}, we present the family of stochastic actor-oriented models for social network analysis. In Section~\ref{sec:ModelVis}, we combine concepts from Sections~\ref{sec:netintro} and \ref{sec:saoms} in an application of the model-vis paradigm, and conclude with a discussion in Section~\ref{sec:discussion}.

\section{Networks and their Visualizations} \label{sec:netintro}

\subsection{Introduction to Network Structures}

Network data is of frequent interest to researchers in a wide array of fields. There are technological networks, like power grids or the internet, information networks, such as citation networks or the World Wide Web, biological networks, like neural networks, and social networks, just to name a few \citep{newman}. Each of these examples have one thing in common: their data \textit{structure}. There are always units of observation: the power stations, websites, neurons, and people, which we refer to throughout this paper as \textit{nodes} or \textit{actors}. There are also always connections of some kind between those units: the power lines, hyperlinks, electrical signals, and relationships, which we will call \textit{edges} or \textit{ties}. Networks might change over time, like when new websites and hyperlinks are added on the World Wide Web, or when there are new people and relationships in a friendship network. The nodes and edges themselves can also have inherent variables of interest, e.g.\ the institution of authors in a co-authorship network, or the number of times two authors have collaborated. 

The multiple layers of network data structures pose unique problems to network analysts. Some questions that network researchers may aim to answer are: How does the strength of a tie between two nodes affect the overall structure of the network? Do node-level differences affect the formation or dissolution of edges? Which views of the data are most informative for communicating significant effects and other results of statistical analyses of the network of interest? These are, of course, just a few broad questions, and we focus here on the latter, which we aim to answer through visual exploration of network data and models. 

\subsection{Visualizing Network Data} \label{sec:netvizintro}

Network visualization, also called network mapping, is a prominent subfield of network analysis. Visualizing network data is uniquely difficult because of the structure of the data itself.  Most, if not all, data visualizations rely on well-defined axes inherited from the data. If variables are numerical, histograms, scatterplots, or time series plots are straightforward to construct. If the variables are categorical, bar charts and mosaic plots are available to to the researcher. If the data are spatial, there is a well-defined region in which to view information. Network data, however, are much less cut-and-dried.

There are two primary methods used to visualize networks: node-link diagrams and adjacency matrix visualization \citep{knuth2000, adjmatpro}. As a toy example, let us assume that we have five nodes, $\{1,2,3,4,5\}$, connected by five directed edges:  $\{2 \to 4, 3 \to 4, 1 \to 5, 3 \to 5, 5 \to 4\}$ We use this toy data set to demonstrate the two visualization methods in Figure~\ref{fig:toyplots}.

<<toyex, echo = FALSE>>=
set.seed(81393)
nodes <- data.frame(id = 1:5, group = sample(LETTERS[1:2], size = 5, replace = T), stringsAsFactors = F)
g <- igraph::as_data_frame(igraph::random.graph.game(n = 5, p = .4))
toynet <- list(nodes = nodes, edges = rbind(g, c(5,4)))
@

The first method, the node-link diagram, represents nodes with points in 2D Euclidean space and then represents edges by connecting the points with lines when there is an edge between the two nodes. These lines can also have arrows on them indicating the direction of the edge for directed networks. But because there is typically no natural placement of the points unless they have important spatial locations, a random placement of the points is used, then adjusted via a layout algorithm, of which there are many \citep{drawingalgs}. 

<<toyplots, fig.show='hold', fig.cap="On the left, a node-link diagram of our directed toy network, with nodes placed using the Kamada-Kawai algorithm. On the right, the adjacency matrix visualization for that same network.", out.width='.49\\linewidth'>>=
toynetdat <- fortify(as.edgedf(toynet$edges), toynet$nodes)
ggplot(data = toynetdat) + 
  geom_net(aes(from_id=from_id, to_id=to_id), labelon = T, size = 11, labelcolour = 'white', ecolour = 'black', vjust = .5, hjust = .5, fontsize = 9, arrowgap = .05, directed = T) +
  ThemeNet + 
  labs(title = "Node-Link Diagram")

ggplot(data = toynet$edges) +
  #geom_tile(aes(x = from, y = -to), color = 'grey20', fill = 'black') +
  geom_tile(aes(x = to, y = -from), color = 'grey20', fill = 'black') +
  geom_segment(aes(x = x, xend = xend), y = -0.5, yend=-5.5, colour="grey30", data = data.frame(x = 0:5+.5, xend= 0:5+.5)) + 
  geom_segment(aes(y=-y, yend=-yend), x = 0.5, xend=5.5, colour="grey30", data = data.frame(y = -0:5+.5, yend= -0:5+.5)) + 
  scale_x_continuous(breaks = 1:5, labels = 1:5, limits = c(0.5,5.5), position = "top") +
  scale_y_continuous(breaks = (-5):(-1), labels = 5:1, limits = c(-5.5,-.5)) +
  ThemeNoNet + 
  theme(panel.grid = element_blank(), aspect.ratio = 1) +
  labs(title = "Adjacency Matrix Visualization", x = "to (alter)", y = "from (ego)")
@
%\hh{XXX Explain each of these visualizations and either comment on the differences in the visualizations (colors) or don't show them. }

Some commonly used layout algorithms, such as  the Kamada-Kawai layout \citep{kamadakawai} and the Fruchterman-Reingold layout \citep{fruchterman-reingold}, were designed to mimic physical systems, drawing the graphs based on the ``forces" connecting them. In these algorithms, the edges of the network act as springs pushing and pulling the nodes in a low dimensional (usually two-dimensional) space. Another algorithm uses multi-dimensional scaling, relying on distance metric and computing a matrix whose entries represent the ``distance" between every pair of nodes. There are also layout algorithms that use properties of the adjacency matrix, like its eigenstructure, to place the nodes in 2D space \citep{drawingalgs}. The node-link diagram using the Kamada-Kawai layout algorithm for our toy network is shown in Figure~\ref{fig:toyplots}. Unless otherwise stated, all other node-link diagrams in this paper will use the Kamada-Kawai layout. %Other algorithms use properties of the network's adjaceny matrix in order to place the nodes.
%hh{XXX how does the adjacency matrix feature here? The adjacency matrix is a different mathematical representation of a network, but describes the same network as a node-link diagram.} \st{XXX I'm taking this to mean I should describe adjacency matrices more, so I've added a discussion below. XXX }


\st{The second primary method for network visualization uses the adjacency matrix of the network. The adjaceny matrix of a network, $\mathbf{A}$, describes the edges of a network in matrix form. An entry $A_{ij}$ of $\mathbf{A}$, for two nodes $i\neq j$ in the network is defined as 
\[
  A_{ij} =
  \begin{cases}
                                   1 &  \text{if an edge exists } i\to j \\
                                   0 & otherwise
  \end{cases}
\]
Note here that our edge variables are binary: we only consider the presence or absence of an edge. If the network has weighted edges, for example an email network where edge weights represent the number of emails sent from one person to another, the entries in the adjacency matrix are the edge weights instead of zeroes and ones. In an undirected network, $A_{ij} = A_{ji}$, but in a directed graph this is only true if there is an edge from $i$ to $j$ and from $j$ to $i$. Thus, $\mathbf{A}$ is always symmetric for undirected networks, and is symmetric for directed networks only if every edge between two nodes is reciprocated.}
\hh{XXX what is an adjacency matrix? - explain at this point in mathematical terms} \st{XXX see above XXX}
An adjacency matrix visualization for our toy example is also shown in Figure~\ref{fig:toyplots}.


Each type of visualization comes with its own advantages and disadvantages. For example, paths between two nodes in a network are easier to determine with node-link diagrams than with adjacency matrix visualizations \citep{adjmatviz}. In node-link diagrams, node-level information can be incorporated into the visualization by coloring or changing the shape of the points representing the nodes, and edge-level information can be incorporated by coloring the lines, or changing their thickness, linetype, or color. Incorporating a node-level variable into an adjacency matrix visualization is not as straightforward or simple, which is more focused on edges. Adjacency matrix visualization has been found to be particularly useful when the network is very complex, dense, or large, and experimental studies have shown adjacency matrix visualization to be superior to node-link diagrames for large networks. For example, for basic perceptual tasks on networks, including node and edge count, adjacency matrix visualizations outperform node-link diagrams as the size and density of the network increases \citep{adjmatviz}. One drawback of the adjacency matrix visualization that \citeauthor{adjmatviz} found was that edges are overrepresented for undirected graphs, due to the symmetry of $\mathbf{A}$: the edge $x_{ij}$ for $i \neq j$ appears in $\mathbf{A}$ twice: in $A_{ij}$ and $A_{ji}$, and so it also appears twice in the adjacency matrix visualization.  \hh{XXX What is the exact logic here that Ghoniem used to come to the conclusion that edges in undirected graphs are over-represented? Why would directed graphs make that better? wouldn't that just add to the number of edges? } \st{XXX in undirected nets, the adj mat is symmetric, where one edge ($x_{ij}$) is represented in entry $i,j$ in the matrix and in entry $j,i$. So, there are many repetitions in the matrix, which makes the network appear more dense than it really is.} \st{This, however, may actually be an advantage for \emph{directed} graphs, where exactly the correct number of edges is represented in a matrix visualization, due to the fact that the edges $x_{ij}$ and $x_{ji}$ are not interchangeable. A node-link diagram, however, may underrepresent the edge count if the edges $x_{ij}$ and $x_{ji}$ both exist and are drawn on top of one another. Ultimately, however t}here is not one "correct" way to visualize network information, and we will be using both \st{the node-link and adjacency matrix visualization} methods throughout this paper to explore social networks and stochastic actor-oriented models. 

\section{Stochastic Actor-Oriented Models for Longitudinal Social Networks} \label{sec:saoms}

% \st{What do I need to communicate in this section? 
% \begin{itemize}
% \item What the models are for: dynamic social networks with actor-level covariates
% \item What are the model components? rate function, objective function (network statistics, actor covariates, interactions between covariates and network statistics), time points ($t_m = 1, \dots, M$)
% \item What is RSiena? What methods of estimation, convergence checks does it use? What are the algorithms? What kinds of models can it fit? 
% \end{itemize}}


%The stochastic actor-oriented models are very aptly named: they model dynamic networks that can account for the individual node characteristics when making the edge changes. The object of analysis is a social network that has been observed at several discrete time points. The nodes in the network can have associated covariate variables that may be incorporated into modeling the tie changes in the network. The model itself has two primary pieces: the rate of change of ties, and the objective function of the nodes, which determines the exact change that gets made. 

%We start with a set of network observations at $M$ discrete time points: $x(t_1), x(t_2) \dots, x(t_M)$. The first observation, $x(t_1)$ is always conditioned on. The number of nodes / actors / vertices is constant in all time points and is denoted $n$.  The tie variables are directed and  binary, and are denoted at $x_{ij}(t_m)$ for time point $m$, and this is sometimes abbreviated to $x_{ij}$ in notation. If there is a tie from node $i$ to node $j$, then $x_{ij}=1$, and $x_{ij}=0$ otherwise. 

\hh{mostly lit review plus a detailed description of the model and fitting process.}

A Stochastic Actor-Oriented Model (SAOM) is a model that incorporates all three components of dynamic networks: edge, node, and time information. It models the change of a network over time, allowing for changes in network structure due to actor-level covariates. This model was first introduced by Snijders in 1996 \citep{saompaper}. The two titular properties of SAOMs, stochasticity and actor-orientation, are crucial to understanding networks as they exist naturally. Most social networks, even holding constant the set of actors over time, are ever-changing as relationships decay or grow in seemingly random ways, and most actors (or nodes) in social networks have inherent properties that could affect how they change their role within the network, and vice versa. 

\subsection{Definitions, Terminology, and Notation}

In this paper, the term \textit{dynamic network} refers to a network, consisting of a fixed set of $n$ nodes, that is changing over time, and is observed at $M$ discrete time points, $t_1, \dots, t_M$ with $t_1 \le t_2 \le \dots \le t_M$. We denote the network observation at timepoint $t_k$ by $x(t_k)$. In the modelling process, we condition on the first observation, $x(t_1)$. The SAOM assumes that this longitudinal network of discrete observations is embedded within a continuous time Markov chain (CTMC), which we will denote $X(T)$. This process is almost entirely unobserved: we assume that the beginning of the process, $X(0)$, is equivalent to the first network observation $x(t_1)$, while the end of the process,$X(\infty)$, is equivalent to the last observation $x(t_M)$. Nearly all other parts of the process are unseen, with the exception of $x(t_2), \dots, x(t_{M-1})$. Unlike the first and last observations of the network, these ``in-between" observations do not have direct correspondence with steps in the continuous time Markov chain. Thus, the ``in-between" observations are considered to be ``snapshots" of the network at some point between two steps in the CTMC. The whole process $X(T)$ is a series of single tie changes that happen according to some pre-defined rate function, where one actor at a time is given the opportunity to add or remove one outgoing tie, or to not make any changes. Once an actor is chosen at random according to the rate function, it is ``given" the chance to change a tie, and it tries to maximize its utility function based on the current and near future states of the network. We expand on the model description further in the subsequent sections. 

\subsubsection{The Rate Function}

For the network $x$ and each actor $i$ in the network, the number of times that an actor $i$ gets to change its ties, $x_{ij}$, to other nodes $j \neq i$ in the network is dictated by a \emph{rate function} $\rho(x, \mathbf{z}, \boldsymbol{\alpha})$, where $\boldsymbol{\alpha}$ are the parameters in the function $\rho$, and $x$ is the current network state, with covariates of interest $\mathbf{z}$. For this paper, we assume a simple rate \st{``function"}, $\rho(x, \mathbf{z}, \boldsymbol{\alpha}) = \alpha_m$ that is constant across all actors between observations at time $t_m$ to $t_{m+1}$, \st{thus our rate function is just a rate parameter in the overall model}. In general, SAOMs can incorporate covariate values and network statistics into the model, so that each node will have a different rate of change.  \hh{XXX that means that the function is essentially just a rate parameter, right?} \st{XXX yes XXX}
Other modelling scenarios allow this rate to be more flexible,  e.g.\ a function that depends on the time period of observation, some actor-level covariates or some actor-level network statistics. 
\hh{How is the rate parameter different from a rate parameter?} \st{I think you meant how is a rate fn different from a rate parameter? I updated the language XXX->}%In our model with the simple rate function, 
\st{In our simple model with a simple rate parameter instead of a rate function,} the rate parameter dictates how quickly an actor $i$ gets an opportunity to change one of its ties to the other nodes in the network, $x_{ij}$, for $j \in \{1, \dots, n\}$ in the time period from $t_{m}$ to $t_{m+1}$. If $j = i$, no change in the network is made. The model also assumes that the actors $i$ are conditionally independent given their ties, $x_{i1}, \dots, x_{in}$ at the current network state. \st{Let $\tau(i|x,m) $ be the wait time until actor $i$ makes its next change from its current state in the network $x$. Note that $m$ indicates the number of the wave that is conditioned on in the SAOM. For any time point, $T$, where $t_{m} \leq T < t_{m+1}$, the waiting time to the next change opportunity by actor $i$ is exponentially distributed with expected value $\alpha_m^{-1}$. The conditional independence assumption is expressed in Equation~\ref{eq:c1}.

\begin{equation} \label{eq:c1}
 \tau(i|x,m) | x_{i1}(m), \dots, x_{in}(m) \stackrel{\text{iid}}{\sim} Exp(\alpha_m)
\end{equation}

 The waiting time to the next change opportunity by \textit{any} actor in the network is also exponentially distributed with expected value $(n\alpha_m)^{-1}$. The distribution of waiting time for the whole network to change, $\tau(x|m) = \sum_i \tau_i(m) |x_{i1}(m), \dots, x_{in}(m) $ can then be written as 

\begin{equation} \label{eq:c2}
 \tau(x|m) \sim Exp(n\alpha_m)
\end{equation}

} The parameter for the wait time for the whole network $n\alpha_m$ is the rate at which any tie change occurs. The estimation of this parameter is straightforward: a the method of moments is used to estimate the rate with the statistic $$C = \sum_i\sum_j |x_{ij}(t_{m+1}) = x_{ij}(t_m)|$$ which is the total number of changes from observation at time $t_m$ to the observation at time $t_{m+1}$. %In order to achieve the memorylessness property of a Markov process, 

\hh{XXX we tried to estimate the rate based on the simulations - you could include a histogram of the number of changes for the 1000 simulations in the viz part XXX}
\st{XXX I have the fitted models from the 1000 simulations (see Figure \ref{fig:distests}). Is that what you mean? XXX}
 
\subsubsection{The Objective Function} 

\hh{shouldn't that be independence? - make the conditional independence assumption above a labelled formula (e.g. C1) and then call it here }
Because of the conditional \st{in}dependence assumptions given in Equation~\ref{eq:c1}, we can consider the objective function for each node separately, as only one tie from one node is \st{allowed to change} at a time. \st{The node $i$, which is the node that is chosen to change at the current time point, is called the \textit{ego} node. It has the potential to interact with all other nodes in the network, $j \neq i$. These nodes $j$, are referred to as \textit{alter} nodes, or simply \textit{alters}. These nodes are acted upon by the ego node, and they only act when they become the ego node at a subsequent time point in the CTMC.} For the ego node, $i$, in the current network state $x$, its objective function, which it tries to maximize, is written as 
\begin{equation} \label{eq:of1}
f_i(\boldsymbol{\beta}, x) = \sum_k \beta_k s_{ik}(x, \mathbf{Z}),
\end{equation}

for $x \in \mathcal{X}$, the space of all possible directed networks with the $n$ nodes, and $\mathbf{Z}$, the matrix of covariates. The vector $\boldsymbol{\beta}$ contains the parameters of the model with corresponding network and covariate statistics, $s_{ik}(x, \mathbf{Z})$, for $k = 1,\dots, K$. \hh{XXX ego node isn't defined yet - the description of the charts would be a good place for it.} \st{XX I added it above the obj function equation. Is it ok there?} Given the ego node, $i$, there are $n$ possible steps for the actor $i$ to take: either one of all current ties $x_{ij} = 1$ will be destroyed, a new tie will be created thatis currently $x_{ij}=0$, or no change will occur. 

The parameters, $\boldsymbol{\beta}$, correspond to various actor-level network statistics, $s_{ik}(x)$. \hh{'There are always' ... that seems to come from a Snijder paper? Re-phrase this to be less vague: In the current implementation of RSiena (snijder) a model has minimally two parameters...} 
According to \citet[p. 371]{snijders01}, there should be at least two parameters included in the model: $\beta_1$ for the outdegree of a node, and $\beta_2$ for the number of reciprocal ties held by a node. \st{These effects should seem familiar to readers used to working with the classical exponential random graph model (ERGM) for networks. The outdegree represents the propensity of nodes with a lot of outgoing ties to form more outgoing ties (the "rich get richer" effect), and the reciprocity parameter measures the tendency of outgoing ties to be returned within a network. The statistics corresponding to these effects are written in terms of the edge variables $x_{ij}$, for $i \neq j$.} In the \texttt{RSiena} software that we use to fit the SAOMs, there are over 80 possible parameters to add to the model. The formulas for the effects are provided in \citet{RSienamanual}. The parameters, $\beta_k$, in the model can be split up into two groups: first, the structural effects, whose estimation depends only on the structure of the network, like the outdegree and reciprocity parameters mentioned above. The parameters are included when the researcher hypothesizes that they will model underlying mechanisms of network change. They hope to answer questions such as, ``How does the existing network structure influence change in the network?" \hh{XXX move the last colored section to the front of this paragraph. That sets up the structure nicely, e.g. `In determining the covariates of a SAOM, two sources of parameters are considered: ...} \st{XXX as I'm going through and uncommenting your comments (sorry....) this one has lost meaning a little bit. Could you give me more detail? I'm not sure which section is which at this point. (sorry again...)}
% and ``How do the behavior and characteristics of the nodes influence change in the network?" put this sentence below somewhere
The second set of effects are referred to as the actor-level or covariate effects. These covariate effects also depend on the structure of the network, with the additional inclusion of node-level covariates of interest. The covariate effects are written in terms of the tie variables $x_{ij}$, but also in terms of the covariates, $\mathbf{Z}$. A table of some possible structural and covariate effects is given in Table~\ref{tab:effects}. For a complete list of the network and covariate statistics that can currently be included in the objective function, see \citet{RSienamanual}.

\begin{table}
\centering
\begin{tabular}{m{4.5cm}m{4.5cm}m{3cm}}
\multicolumn{3}{l}{\textbf{Structural Effects}} \\ \hline\hline
outdegree  & $s_{i1}(x) = \sum_j x_{ij}$ & \includegraphics[width=2cm]{img/outdegree}\\ \hline
reciprocity  & $s_{i2}(x) = \sum_j x_{ij}x_{ji}$ & \includegraphics[width=2cm]{img/reciprocity}\\ \hline
transitive triplets  & $s_{i3}(x) = \sum_{j,h} x_{ij}x_{jh}x_{ih}$ & \includegraphics[width=1.5cm]{img/transtrip}  \\
\\
\multicolumn{3}{l}{\textbf{Covariate Effects}}\\  \hline \hline
covariate-alter  & $s_{i4}(x) = \sum_j x_{ij}z_j$ & \includegraphics[width=3cm]{img/covaralter} \\  \hline
covariate-ego  & $s_{i5}(x) = z_i\sum_j x_{ij}$ & \includegraphics[width=3cm]{img/covarego} \\  \hline
same covariate & $s_{i6}(x) = \sum_j x_{ij} \mathbb{I}(z_i = z_j)$ & \includegraphics[width=3cm]{img/covarsim}%\\
%jumping transitive triplets  & $s_{i7}(x) = \sum_{j \neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(z_i = z_h \neq z_j)$
\end{tabular}
\caption{\label{tab:effects} Some of the possible effects to be included in the stochastic actor-oriented models in RSiena. There are many more possible effects, but we only consider a select few here. For a complete list, see the RSiena manual \citep{RSiena}.}
\end{table}

When node $i$ is given the chance to change a tie, %\hh{`we assume that they wish' sounds strange - don't assume wishes, state assumptions}
it attempts to maximize the value of its objective function $f_i(\boldsymbol{\beta}, x)$ plus a random element, $U_i(x)$, to account for unknown attraction between nodes. In \citet{modelsSnijders}, it is recommended that the $U_i(x)$ be random draws from a type 1 extreme value distribution. The additional random element is included to account for any random, unexplainable change in the network ties. % XX What is the thought behind that? } ``the type 1 extreme value distribution (or Gumbel distribution) with mean 0 and scale parameter 1" \citep[p. 368]{snijders01}. 
This distribution, which is also known as the log-Weibull distribution, has probability distribution function, using $\mu$ for the mean parameter and $\sigma$ for the scale parameter, of

\begin{equation} \label{eq:dist1}
g(u|\mu, \sigma) = \frac{1}{\sigma}\exp\left\{-\left(\frac{u-\mu}{\sigma} + \exp^{-\frac{u-\mu}{\sigma}}\right)\right\}.
\end{equation}

<<logweibull, fig.align='center', out.width = '.5\\linewidth', fig.cap='The probability distribution function for the type 1 extreme value distribution, also known as the log-weibull or Gumbel distribution with location parameter $ \\mu = 0$ and scale parameter $ \\sigma = 0$.'>>=
library(reliaR)
x <- seq(-3,10,.01)
y <- dgumbel(x = seq(-3,10,.01), sigma = 1, mu=0)
qplot(x = x, y= y, geom = "line") + 
  annotate('text', x = 6, y = .25, 
        label = "g(u) = exp(-(u + exp(-u)))",size=5)  + 
  labs(x = "u", y = "g(u) (density)")
@

The probability density function for the type 1 extreme value distribution is shown in Figure~\ref{fig:logweibull}. Using the distribution given in Equation~\ref{eq:dist1} with mean $\mu = 0$ and scale $\sigma = 1$ as \citet{modelsSnijders} suggests \hh{XXX I'm not following - which distribution do you mean by `this' distribution? The density above is the type 1 extreme value density. I got lost at the  ``objective function $f_i(\boldsymbol{\beta}, x)$ plus a random element, $U_i(x)$"} \st{XXX I hope it is clearer now above}
is convenient because it \st{leads to a simple probability formula for} the probability that actor $i$ chooses to change its tie to actor $j$ \st{that can be written \textit{only}} in terms of the objective function. Let $p_{ij}(\boldsymbol{\beta}, x)$ be the probability that actor $i$ chooses to change its tie to actor $j$. Next, we write the network $x$ in its potential future state, $x(i \leadsto j)$, where the tie $x_{ij}$ has changed to $1-x_{ij}$. Then, the probility that the tie $x_{ij}$ changes is 

$$p_{ij}(\boldsymbol{\beta}, x) = \dfrac{\exp\left\{f_i(\boldsymbol{\beta}, x(i\leadsto j))\right\}}{\sum_{h \neq i} \exp\left\{f_i(\boldsymbol{\beta}, x(i \leadsto h))\right\}}$$

When $i = j$ in $p_{ij}$, the numerator represents the exponential of the value of the objective function when evaluated at the current network state. When the value of the objective function is high at the current state, the probability of not making a change in a microstep is also high. In the CTMC, when actor $i$ may make a change, it chooses which tie $x_{i1, \dots, x_{in}}$ to change at random according to the probabilities $p_{ij}(\boldsymbol{\beta}, x)$.
\hh{XXX This paragraph ends a bit abruptly - what is the future state probability used for?} \st{XX The future state probability is used within the continuous time markov chain to move through the different network states. This needs more fleshing out. Adding to the to-do list.}


\subsubsection{Continuous Time Markov Chain}
\st{
In the continuous-time Markov chain literature, see for instance \citet{ctmc}, the chains are characterized by their \textit{generator} or \textit{intensity} matrix $\mathbf{Q}$. This matrix describes the rate of change between two states of the CTMC process, and the rows of this matrix always add to zero. For directed networks with binary edge variables like the ones we will be working with, there are a very large number of possible states, $2^{n(n-1)}$: there are two possible states for an edge, and there are $n(n-1)$ edge relationships because self-ties are not allowed.  The intensity matrix for a CTMC in a SAOM is then a square matrix of dimension $2^{n(n-1)} \times 2^{n(n-1)}$. Since only one tie changes at a time in the CTMC, there are only $n(n-1)$ reachable states from the current network state. Thus, the intensity matrix $\mathbf{Q}$ is very sparse, with only $n(n-1) + 1$ non-zero entries in each row. Note that $n(n-1)$ of these represent the possible states that are one edge different from a given state, and the additional non-zero entry is for the state to remain unchanged. All other entries in a row are zero because those network states cannot be reached from the current state by just one change. 

The two pieces of a SAOM, the rate function/parameter and the objective function, each contribute to the entries of the intensity matrix to describe the rate of change between two network states. The entries of $\mathbf{Q}$ are defined as follows: let $b \neq c \in \{1, 2, \dots, 2^{n(n-1)} \}$ be indices of two different possible states of the network, $x^b, x^c \in \mathcal{X}$.  Then the $bc^{th}$ entry of $Q$ is:
\[ q_{bc} = \begin{cases} 
     q_{ij}= \alpha_m p_{ij}(\boldsymbol{\beta}, x^b) & \text{if } x^c \in \{x^b(i \leadsto j) | \text{ any } i \neq j \in \{1, \dots, n\}\} \\
      0 & \text{if } \sum_i \sum_j |x_{ij}^c -x_{ij}^b | > 1 \\
      -\sum_{i\neq j} q_{ij} & \text{if } x^b = x^c 
   \end{cases}
\]

Thus, the rate of change between any two states, $x^b$ and $x^c$, that differ by only one tie $x_{ij}$, is the product of the rate at which actor $i$ gets to change a tie and the probability that the tie that will change is the tie to node $j$. 

XXX Need a transistion to next section XXX 
}

%### A SAOM as a CTMC {#saomctmc}

%In order to fit this model definition back into the original context of the CTMC described in Section \@ref(dynamicnets), it must be written in terms of its intensity matrix, $\mathbf{Q}$.  This matrix describes the rate of change between states of the process. For networks, there are a very large number of possible states, $2^{n(n-1)}$, so the intensity matrix is a square matrix of that dimension. But, thanks to the property of SAOMs that the states are allowed to change only one tie at a time, there are only $n$ possible states given the current state, $n-1$ of which are uniquely determined by the node $i$ that is given the opportunity to change. Thus, the intensity matrix $\mathbf{Q}$ is very sparse, with only $n(n-1) + 1$ non-zero entries in each row. Note that $n(n-1)$ of these represent the possible states that are one edge different from a given state, and the additional non-zero entry is for the state to remain the same. All other entries in a row are zero because those column states cannot be reached from the row state by just one change as dictated by the SAOM. The entries of $\mathbf{Q}$ are defined as follows: let $b \neq c \in \{1, 2, \dots, 2^{n(n-1)} \}$ be indices of two different possible states of the network, $x^b, x^c \in \mathcal{X}$.  Then the $bc^{th}$ entry of $Q$ is:
% \[ q(x^b, x^c) = \begin{cases} 
%       q_{ij}(\alpha, \rho, \boldsymbol{\beta}, x^b) = \lambda_i(\alpha, \rho, x^b, m)p_{ij}(\boldsymbol{\beta}, x^b) & \text{if } x^c \in \{x^b(i \leadsto j) | \text{ any } i \neq j \in \mathcal{N}\} \\
%       0 & \text{if } x^c \text{ differs from } x^b \text{ by more than 1 tie} \\
%       -\sum_{i\neq j} q_{ij}(\alpha, \rho, \boldsymbol{\beta}, x^b)  & \text{if } x^b = x^c 
%    \end{cases}
% \]
% 
% Thus, the rate of change between any two states that differ by only one tie, $x_{ij}$, is the product of the rate at which actor $i$ gets to change a tie and the probability that the tie that will change is the tie to node $j$.^[Just to be clear, the change is from $x^b_{ij}$ to $x^c_{ij} = 1 - x^b_{ij}$.] Furthermore, the theory of continuous time Markov chains gives that the matrix of transition probabilities between observation times $t_{m-1}$ and $t_{m}$ is dependent only on the difference between timepoints, $t_m - t_{m-1}$. Following the same definition for transition probabilities in Section \@ref(dynamicnets), the matrix of transition probabilities is 
% $$e^{(t_m - t_{m-1})\mathbf{Q}},$$ 
% where $\mathbf{Q}$ is the matrix defined above and $e^X$ for a real or complex square matrix $X$ is equal to $\sum_{k=0}^{\infty} \frac{1}{k!} X^k$.

\subsection{Fitting Models to Data}

To fit a SAOM to observations of a dynamic network, we use the package \texttt{RSiena} \citep{RSiena}. This package uses simulation methods to estimate parameter values using either the method of moments or maximum likelihood estimation. In this paper, use the method of moments estimation because the theory behind it was established in \citet{saompaper}, while the maximum likelihood estimation methods were not fully established until \citet{saomsml2010}, though \texttt{RSiena} contains capabilities to use maximum likelihood estimation. We also use the score function method for estimating the derivatives of the expected values, as opposed to the finite differences method, both of which are outlined in detail in \citet{SienaAlgs}.

\par For the score function method, the SIENA software uses a Robbins-Monro algorithm (see \citet{robbinsmonro}) to estimate the solution of the moment equation
$$ E_{\theta}S = s_{obs}$$
where $\theta$ is the vector of rate and objective function parameters, and $s_{obs}$ is the observed vector of model statistics, $S$. The entire algorithm is provided in \citet{SienaAlgs}.

There are three phases in the the SIENA algorithm, as described in \citet{RSienamanual, SienaAlgs}. The first phase performs initial estimation of the score functions for use in the Robbins-Monro procedure for method-of-moments estimation. The second phase carries out the Robbins-Monro algorithm and obtains estimates of the parameter values through iterative updates and simulation from the CTMC at current parameter values. The third phase uses the parameter vector estimated in phase two to estimate the score functions and covariance matrix of the parameter estimate, and also carries out convergence checks. In each of the the first two phases, the estimation procedure also uses ``microsteps" that simulate from the model as it exists in its current state in order to update either the score functions or the parameter estimates. These simulated microsteps are observed instances of the continuous-time Markov chain that is the backbone of the stochastic actor-oriented model. In Section~\ref{sec:ModelVis}, we further explore these phases in the SIENA method-of-moments algorithm through visualization, bringing them out of the ``black-box" and into the light. 

\subsection{Model Goodness-of-Fit}

The \texttt{RSiena} software that fits the models to data also comes equipped with the \texttt{sienaGOF()} function for examining model fit. This function ``assess[es] the fit of the model with respect to auxiliary statistics of networks" \citep[p.~53]{RSienamanual}. Examples of the auxiliary statistics are the out- or indegree distribution on the nodes, with the option for users to input their own statistics to examine. The fit is tested using Mahalanobis distance to determine a $p$-value when comparing the statistics calculated from the observed data (excluding the first observation that is conditioned on) to the distribution of the statistics when calculated on the simulated waves. So, good fit is acheived when $p$ is large. \st{XX more detail about mahalanobis distance and calculating p-value XX}

The \texttt{plot.sienaGOF()} function allows us to visualize this fit. Below is an example of what the goodness-of-fit plot output looks like for indegree and outdegree statistics for a small model. Box plots and violin plots are drawn on top of each other in Figure~\ref{fig:sienagof} in order to show the distribution of the simulated network observations compared to the true data shown in red points connected by red lines. If the red points lie "well within" the simulated values, the model is a good fit to the data. The model examined in Figure~\ref{fig:sienagof} appears to do a better job of capturing the outdegree distribution than the indegree distribution of the data. 

<<sienagof, fig.cap="Goodness of fit measures for two network statistics: indegree and outdegree cumulative distributions. On the x-axis is the degree value, and on the y-axis is the number of times that degree value occurs in the data. \\st{The p-value shown on the x-axis is .... }", cache = TRUE, message = FALSE, warning = FALSE>>=
load("data/ansnullpaper.rda")
library(lattice)
gof1 <- sienaGOF(ansnull, OutdegreeDistribution, varName ="friendship", join = FALSE)
gof2 <- sienaGOF(ansnull, IndegreeDistribution, varName ="friendship", join = FALSE)
p1 <- plot(gof1, period = 1)
p2 <- plot(gof2, period = 1)
gridExtra::grid.arrange(p1, p2)
# sum(gof1$`Period 1`$SimulatedTestStat >= gof1$`Period 1`$ObservedTestStat)/1000
# [1] 0.154
# > sum(gof2$`Period 1`$SimulatedTestStat >= gof2$`Period 1`$ObservedTestStat)/1000
# [1] 0.063

@

\subsection{Example Data}

To guide our visual exploration of stochastic actor-oriented models, we used a subset of the 50 actor dataset from the ``Teenage Friends and Lifestyle Study" that is provided on the \texttt{RSiena} webpage. These data come from \citet{friendsdata}, and we chose to only work with a subset of the data to make network visualizations less busy, with changes that are more noticeable. The subset contained actors 20 through 35 and the ties between them, as well as the drinking behavior of each actor at each of the three waves. This specific subset was chosen because it showed somewhat higher connectivity than other subsets, as we've emphasized in the visualizations of the three network adjacency matrices in Figure~\ref{fig:adjmatviz}. For model fitting, we condition on wave 1 and estimate the parameters of the models from the second and third waves. We will also be working with one actor level categorical covariate, drinking behavior. This variable has five values in the original data: (1) does not drink, (2) drinks once or twice a year, (3) drinks once a month, (4) drinks once a week, and (5) drinks more than once a week. The actor covariates and networks are visualized in Figures~\ref{fig:adjmatviz} and~\ref{fig:alldataviz}. 
<<adjmatviz, fig.align='center', fig.cap="A visualization of the adjacency matrices of the three waves of network observations in the ``Teenage Friends and Lifestyle Study'' data. The subset we will be using is outlined in red.", fig.width=8, results='hide',fig.show='asis', fig.width=8, fig.height=3>>=
source("code/00e_small_friends.R")
view_rs2$wave <- factor(view_rs2$wave)
levels(view_rs2$wave) <- c("Wave 1", "Wave 2", "Wave 3")
ggplot(data= view_rs2, aes(x = x, y=y)) + 
  geom_tile(aes(fill = as.factor(value))) + 
  scale_fill_manual("i likes j", labels=c("no", "yes"), 
                    values = c('white', 'black')) +
  geom_rect(data= NULL, inherit.aes = FALSE, color = 'red',
            aes(xmin = 19.5, xmax = 35.5, ymin = 19.5, ymax = 35.5, fill =NA)) + 
  facet_wrap(~wave) + theme_bw() +
  labs(x = "i", y = "j") + 
  ThemeNoNet + 
  theme(aspect.ratio=1, axis.text = element_blank(), axis.ticks = element_blank()) 
@

<<alldataviz, fig.cap="The smaller friendship network data we will be modelling throughout the paper.", fig.width=8, fig.height=3>>=
actual1 <- merge(data.frame(as.edgelist(as.network(fd2.w1))), 
                 data.frame(id = 1:16, drink = drink2[,1]),
                 by.x = "X1", by.y = "id", all = T)
actual1$Wave <- 1 
actual2 <- merge(data.frame(as.edgelist(as.network(fd2.w2))),
                 data.frame(id = 1:16,drink = drink2[,2]),
                 by.x = "X1", by.y = "id", all = T)
actual2$Wave <- 2
actual3 <- merge(data.frame(as.edgelist(as.network(fd2.w3))),
                 data.frame(id = 1:16,drink = drink2[,3]),
                 by.x = "X1", by.y = "id", all = T)
actual3$Wave <- 3
alldat <- rbind(actual1, actual2, actual3)
#alldat$X1 <- paste0("V", alldat$X1)
alldat$X1 <- as.factor(alldat$X1)
#alldat$X2 <- ifelse(!is.na(alldat$X2), paste0("V", alldat$X2), NA)
alldat$X2 <- as.factor(alldat$X2)
#write_csv(alldat, "smallfriends4Geomnet.csv")
alldataviz <- ggplot(data = alldat, 
       aes(from_id = X1, to_id = X2, color = as.ordered(drink), group = as.ordered(drink))) +
  geom_net(fiteach = FALSE, directed=TRUE, layout.alg = 'fruchtermanreingold', size = 4, arrowsize = .25, arrowgap = .03, ecolour = 'grey40', labelon=T, labelcolour = 'black', fontsize = 4, vjust = .5) +
  scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
  ThemeNet +
  facet_wrap(~Wave, labeller = "label_both") + 
  theme(legend.position = 'bottom', panel.background = element_rect(fill = NA, color = "grey30"))
alldataviz
@

\section{Model Visualizations} \label{sec:ModelVis}

The concept of model visualization was developed to complement traditional model diagnostic tools. Typically, numerical summaries such as $R^2$ are used to assess model fit, and visualizations, like residual plots, are restricted to the data. Every good data analysis includes both numerical and visual summaries of the data, so why should model description be restricted to numerical summaries? \citeauthor{modelvis} outline the three different parts of a model: the model family, the model form, and the fitted model. The latter is primarily what one thinks of first when considering a model in a data analysis, where a model is fit to data, and parameter estimates are reported. In the context of SAOMs, the fitted model contains the estimated rate parameters and the estimated objective function parameters. The model form describes the the model before the fitting process and how variables describe the data in the model. In SAOMs, the model form includes description of the rate and objective functions and the variables therein that describe how the network evolves over time. Finally, the model family is the most broad description of the model. This is the type of model that you wish to fit to the data, and is chosen based on the problem and knowledge at had. For example, we chose to use a SAOM to model network data over an exponential random graph model (ERGM) because we believe that actor-level variables effect network dynamics. These three parts of a "model" are should be visualized according to the three principals of model visualization: display the model in the data space, look at a collection of models, and explore the process of fitting the model, not just the end result. 

\subsection{Explore algorithms, not just end result}

The model fitting process in \texttt{RSiena} involves several simulation steps that are hidden from the user. This is practical and efficient if a researcher is primarily interested in fitting one model to a set of longitudinal network data and obtaining parameter estimates. We are more interested in how the models are fit, so we extracted and explored the different steps of that process. 

A key component of each step of the SIENA method of moments algorithm is the ``microstep." This step simulates from the model in its current state, $x(t_m)$ with current parameter values $\theta_{0}$, to the next state, $x(t_{m+1})$. This microstep process stops when the simulated network has acheived the same number of differences, $C$, from $x(t_m)$ as $x(t_{m+1})$, where 
$$C = \sum_{i \neq j} |x_{ij}(t_{m+1}) - x_{ij}(t_{m})|.$$ 
This simulation process follows the steps of the continuous-time Markov chain. First, an ``ego node'' is selected to make a change, and then it randomly makes one change in its ties according to the probability, $p_{ij}$, determined by its objective function. The options for change are (1) removing a current tie, (2) adding a new tie, or (3) making no change at all. Saving all of these steps is not efficient if you're only interested in estimating parameter values, but they can be saved and extracted using options in \texttt{RSiena}, which is exactly what we did to create our visualizations. Between two network observations $x(t_m)$ and $x(t_{m+1})$, there can be dozens, hundreds, or even thousands of microsteps, depending on the size of the network. We wanted to see these in-between steps in order to better understand the behavior of the underlying continuous-time Markov chain. 

The first vizualization we present here is an animation of microsteps simulated to emulate the transition steps from wave 1 to wave 2 the subset of the friends data shown in Figure ~\ref{fig:alldataviz}. Movies similar to our animation were used to visualize the changes of dynamic networks in \citet{dynnetviz}. When each ego node is selected in a microstep, it is emphasized in the animation, then the associated edge either appears or disappears. If there are no changes at a particular microstep, no changes are seen. Some frames of the animations are shown in Figure~\ref{fig:makinggif}, and the full movie can be viewed at INSERT LINK TO GIF. 

<<makinggif, fig.cap='A selection of images in the microstep animation. The selected edges and nodes are emphasized by changing the size and the color, then when edges are removed, they fade out, shrinking in size, while the nodes change color and shrink to blend in with the other nodes.', fig.width=8, cache=TRUE, message = FALSE, warning = FALSE>>=
# Need old version of dplyr to get this to work: 
if (!require("dplyr050",character.only = TRUE)) {
      devtools::install_github("sctyner/dplyr050")
}

friend.data.w1 <- as.matrix(read.table("data/s50_data/s50-network1.dat"))
friend.data.w2 <- as.matrix(read.table("data/s50_data/s50-network2.dat"))
friend.data.w3 <- as.matrix(read.table("data/s50_data/s50-network3.dat"))
drink <- as.matrix(read.table("data/s50_data/s50-alcohol.dat"))
fd2.w1 <- friend.data.w1[20:35,20:35]
fd2.w2 <- friend.data.w2[20:35,20:35]
fd2.w3 <- friend.data.w3[20:35,20:35]
friendshipData <- array(c(fd2.w1, fd2.w2,fd2.w3), dim = c(16, 16, 3))

# running Siena
#friendship <- sienaDependent(friendshipData)
#alcohol <- varCovar(drink[20:35,])
#mydata <- sienaDataCreate(friendship, alcohol)
#myeffnull <- getEffects(mydata)
#myalgorithm <- sienaAlgorithmCreate(projname = 's16_3')
set.seed(823746)

ansnullchains <- get_chain_info(ansnull)
ansnullchains %>%
  filter(period == 1) %>%  #only look at chains from wave 1 to wave 2
  group_by(rep) %>%
  dplyr::select(rep, from = ego, to = alter) %>%
  mutate(val = as.numeric(!from == to),
         from = paste0("V", parse_number(from)+1), # make the chains
         to = paste0("V", parse_number(to)+1)) -> ansnullchainsw1w2
colnames(fd2.w1) <- paste0("V", 1:16)
rownames(fd2.w1) <- paste0("V", 1:16)
wave1friends <- fortify(as.adjmat(fd2.w1))
ms1 <- listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 1))
microsteps <- ms1[1:20]

microsteps_df <- getMicrostepsDF(microsteps)
pte <- pretween_edges(microsteps = microsteps)
ptv <- pretween_vertices(pte = pte, layoutparams = list(n = 16))
# step 1 is remove, step 9 is add. 
# get tweened data for all microsteps
pall <- tween_microsteps(ptv, pte, microsteps_df)

static_pall <- filter(pall, .frame %in% c(15,20,22,24,26:28, 71:73, 75, 79, 81, 84))

static_pall$type <- NA
static_pall$type[which(static_pall$.frame %in% c(15,20,22,24,26:28))] <- "Remove Edge"
static_pall$type[which(static_pall$.frame %in% c(71:73, 75, 79, 81, 84))] <- "Add Edge"

set.seed(34569234)
create_net_animate(static_pall) + 
  facet_wrap(~.frame, nrow=2, labeller = 'label_both') + 
  ThemeNet + 
  theme(panel.background = element_rect(color = 'black'))
@

The top row of Figure~\ref{fig:makinggif} shows an edge being removed, and the bottom row shows one being added. In both cases, the ego nodes chosen to act change color from black to red, and they also increase greatly in size. In the case of an edge being removed, the edge that currently exists is emphasized with the color and size change that the node also gets, and as the animation proceeds the edge shrinks to disappearing, while the node shrinks and changes color back to the original black. If the edge is being added, it appears colored red from nothing and grows to a large size, then changes color to match the rest of the edges, while the node shrinks and changes color to match the other nodes.  

We also use animation to view the changing structure of the adjacency matrix the microsteps. The adjacency matrices for the three waves of friendship data as shown in Figure~\ref{fig:alldataviz} are ordered by node id. There are 16 nodes in the data, numbered 1-16, and that order is used on the $x$ and $y$ axes for the matrix visualization. Viewing the adjacency matrices with this arbitrary ordering does not provide much information to the viewer about the underlying structure of the network. This lack of perceived structure would be exacerbated in an animation. So, we adjust the ordering so that the viewer can perceive more of the structure of the graph. This process is known as matrix seriation \citep{seriation}. 

To reorder the vertices for the matrix visualization, we first constructed a cumulative adjaceny matrix, $\mathbf{A}^{cum}$, for the series of microsteps simulating the network from $x(t_m)$ to $x(t_{m+1})$. A single entry in the cumulative adjacency matrix, $\mathbf{A}^{cum}_{ij}$, is the total number of times the edge from node $i$ to node $j$ appears in the network from the initial observation, $x(t_m) \equiv X(0)$ to the final reult of the last microstep, $X(C)$: 
$$ \mathbf{A}^{cum}_{ij} = \sum_{c = 0}^C X_{ij}(c). $$
We then performed a principal component analysis on $\mathbf{A}^{cum}$, and used the values of the first principal component to order the vertices on the $x$ and $y$ axes for the adjacency matrix animation. For one such series of microsteps simulated by \texttt{RSiena}, we present the adjacency matrix ordered by the (arbitrary) vertex id alongside the seriated adjacency microsteps using the first principal component loading on the cumulative adjacency matrix, $\mathbf{A}^{cum}$, in Figure~\ref{fig:adjmatorder}. 

<<adjmatorder, message = FALSE, warning = FALSE, fig.cap="On the left, the starting friendship network represented in adjacency matrix form, ordered by vertex id. On the right, the same adjacency matrix is presented after ordering the vertices by one repitition of the microstep simulation process from wave one to wave two.", fig.width=8, fig.height=3>>=
# first present the original wave 1 adj mat, then present the adj mat after being ordered by the 1st pc of the big adjmat A. 
# reinstall new dplyr
numordersf <- data.frame(fd2.w1) %>% 
  mutate(from = rownames(fd2.w1)) %>% 
  gather(key = to ,value = value, -from)
pcaordersf <- numordersf

numordersf$from <- factor(numordersf$from, levels = paste0("V", 1:16))
numordersf$to <- factor(numordersf$to, levels = rev(paste0("V", 1:16)))

p1 <- ggplot(data = numordersf, aes(x = from, y = to)) + geom_tile(color = 'black', aes(fill = as.factor(value))) + 
  scale_fill_manual(values = c('white', 'black')) +
  scale_x_discrete(position = "top", labels = as.character(1:16)) +
  scale_y_discrete(labels = rev(as.character(1:16)))  + 
  ThemeNoNet + 
  theme(legend.position = 'none') + 
  coord_fixed() 

sfmsall <- read_csv("data/smallfriendsmicrosteps.csv")

sfmsall %>% 
  group_by(from, to) %>% 
  filter(rep == 1) %>% 
  summarise(count = n()) -> sfmsall_summ
sfmsall_summ$from <- factor(sfmsall_summ$from, levels = paste0('V', 1:16))
sfmsall_summ$to <- factor(sfmsall_summ$to, levels = paste0('V', 1:16))
sfmsall_summ %>% 
  spread(to, count, fill = 0) %>% 
  data.frame -> allmsrep1_adjmat
allmsrep1_adjmat$V16 <- 0
allmsrep1_adjmat <- allmsrep1_adjmat[,-1]
allmsrep1_adjmat <- as.matrix(allmsrep1_adjmat)
rownames(allmsrep1_adjmat) <- paste0("V", 1:16)

pca1 <- prcomp(allmsrep1_adjmat)
order1 <- names(sort(pca1$rotation[,1], decreasing = T))
order2 <- names(sort(abs(pca1$rotation[,1]), decreasing = T))

pcaordersf$from <- factor(pcaordersf$from, levels = order1)
pcaordersf$to <- factor(pcaordersf$to, levels = rev(order1))

p2 <- ggplot(data = pcaordersf, aes(x = from, y = to)) + geom_tile(color = 'black', aes(fill = as.factor(value))) + 
  scale_fill_manual(values = c('white', 'black')) +
  scale_x_discrete(position = "top", labels = str_replace(order1, "V", "")) + 
  scale_y_discrete(labels = rev(str_replace(order1, "V", ""))) + 
  ThemeNoNet + 
  theme(legend.position = 'none') +
  coord_fixed() 

grid.arrange(p1,p2, nrow = 1)
@

Using the principal component analysis on $\mathbf{A}^{cum}$ to order the rows and columns of the adjacency matrix visualization clearly shows the two distinct connected components in the first wave of the network, which are difficult to find in the arbitrarily ordered visualization. 

We also attempt to better understand the microstep process by visualizing all transition probabilites for the first microstep in the process. We only do the first step of many for now because the \texttt{RSiena} transition probabilities after the first step are only comparable for identical steps due to the conditioning on the current network state in the model. Thus we have one step for each of the 1,000 repititions to visualize. In Figure ~\ref{histogramalters}, we present each acting node, (ego node) and the resulting probabilities of change. The probability shown by the bars is the theoretical probability according to the objective function of the ego node changing the tie to the alter node. The probability shown by the points is the empirical probability of that change being made, calculated by counting all instances of the ego node first, then computing the proprotion of each change. In most cases, they are almost identical, showing that the algorithm is performing as expected. 

<<histogramalters, fig.width = 8, fig.height=11, fig.cap="Each panel shows the theoretical (as lines) and empirical (as points) probabilities of the chosen ego node changing its tie to each of the other nodes. The color of the line indicates whether the tie from the ego to the alter node is being added, removed, or if there is no change to the network in this step.">>=
ansnullchains %>% group_by(period, rep) %>% 
  mutate(ms = row_number()) %>% 
  filter(ms == 1, period == 1) %>% 
  mutate(alterProb = exp(logAlterProb),
         ego = as.numeric(as.character(ego)),
         alter = as.numeric(as.character(alter))) %>% 
  ungroup() %>% 
  group_by(ego, alter) %>% 
  summarize(count = n(), alterProb = unique(alterProb)) %>% ungroup() %>% 
  group_by(ego) %>% 
  mutate(csum = cumsum(alterProb)) %>% 
  arrange(ego, csum) %>% 
  mutate(totego = sum(count), 
         empprob = count / totego) -> cumprobs 
cumprobs$ego <- factor(paste0("V",cumprobs$ego+1), levels = paste0("V", 1:16))
cumprobs$alter <- factor(paste0("V",cumprobs$alter+1), levels = paste0("V", 1:16))

# is the tie that's added new, removed, or does the network stay the same 

changes <- cumprobs %>% dplyr::select(ego, alter) %>% unique 

wave1friends2 <- wave1friends[,-3]
wave1friends2$type <- "remove"
changes2 <- left_join(changes, wave1friends2, c("ego" = "from", "alter" = "to"))

changes2$type[is.na(changes2$type)] <- ifelse(changes2$ego[is.na(changes2$type)] == changes2$alter[is.na(changes2$type)], "noChange", "add")

cumprobs %>% left_join(changes2) -> changes3

changes3$ego <- factor(changes3$ego, levels = paste0("V", 1:16), labels = 1:16) 
changes3$alter <- factor(changes3$alter, levels = paste0("V", 1:16), labels = 1:16) 

ggplot(data = changes3) + 
  geom_segment(
    aes(x = alter , xend = alter, y = 0, yend = alterProb, color=type), size = 2) + 
  geom_point(aes(x = alter, y = empprob), shape = 4, size = 2, alpha = .5) + 
  labs(x = "alter node", y = "probability of alter node") + 
  coord_flip() + 
  facet_wrap(~ego, labeller = "label_both") + 
  ThemeNoNet
# 
# cumprobs %>% ggplot() + geom_histogram(aes(x = alter, weight = count), color = 'black', fill = 'white', binwidth = 1) + scale_x_continuous(labels = paste0("V",1:16), breaks = 0:15) + coord_flip() + facet_wrap(~ego, labeller = "label_both") + theme_gray()

# cprobslist <- split(cumprobs, cumprobs$ego)
# plots <- list()
# for(i in 1:length(cprobslist)){
#   cprobslist[[i]] %>%
#   ggplot() + 
#   geom_histogram(aes(x = as.numeric(as.character(alter)),
#                      y = alterProb), stat = 'identity', fill = 'white', color = 'black') + 
#   geom_line(aes(x = as.numeric(as.character(alter)), 
#                 y = csum)) + 
#   geom_point(aes(x = as.numeric(as.character(alter)), 
#                 y = csum)) +
#   scale_x_continuous(limits = c(-.5, 15), breaks = 0:15, labels = 0:15) +   
#   geom_hline(yintercept = 1, color = 'red', linetype = 'dashed') + 
#   labs(x = "alter", y = expression(p[ij]), title = paste("Ego node:", names(cprobslist)[i])) + # ylab(expression(p[ij])) + 
#   theme_gray() + 
#     theme(plot.title = element_text(size = 10), 
#           axis.title = element_text(size = 8), 
#           plot.margin = unit(rep(0,4),"pt")) -> p
#   plots[[i]] <- p
# }
# grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]],
#              plots[[5]], plots[[6]], plots[[7]], plots[[8]],
#              #plots[[9]], plots[[10]], plots[[11]], plots[[12]],
#              #plots[[13]], plots[[14]], plots[[15]], plots[[16]], 
#              ncol = 2, padding = unit(0, 'line'))
@

Another way to view the transition probabilities is through the adjacency matrix visualization. In Figure~\ref{fig:heatmap}, we build on the concept of the ordered adjacency matrix of Figure~\ref{fig:adjmatorder}. This heatmap shows the transition probabilities of all ties that are changed in the first microstep of the 1,000 simulations. The heatmap is noticeably sparse, showing the lack of coverage in the model.  Of the 256 possible steps, only 81, or about 32\%, are taken in the 1,000 simulated chains. XXX Check this again.... XXX
<<heatmap, fig.align='center', fig.cap='A heatmap showing the empirical transition probabilities for the first microstep in 1,000 simulations. The acting node is on the x-axis, and the receiving node is on the y-axis. Missing squares represent ties that did not occur.', fig.height=3>>=
ansnullchains %>% 
  filter(rep == 1, period == 1) %>% 
  mutate(ego = paste0("V",as.numeric(as.character(ego))+1),
         alter = paste0("V",as.numeric(as.character(alter))+1), 
         prob = exp(logAlterProb)) -> probsDat

ggplot(data = probsDat, 
       aes(x = factor(ego, levels = order1), 
             y = factor(alter, levels = rev(order1)), 
             fill = prob)) + 
  geom_tile(color = 'black') + 
  scale_fill_gradient(low = 'white', high = 'blue', limits = c(0,1)) +
  labs(x = "ego", y = "alter")+
  scale_x_discrete(position = 'top', labels = str_replace(order1, "V", "")) +
  scale_y_discrete(labels = rev(str_replace(order1, "V", ""))) +
  ThemeNoNet + theme(panel.grid = element_blank()) + 
  coord_fixed()
@

We also wanted to view the complete microstep process from the first wave, which we condition on, to the second wave. The number of steps taken from wave 1 to wave 2 varies. In the simulations from Model 1, the smallest number of steps taken was 58, and the longest was 248, with a mean of 106 and a median of 103. The standard deviation of the 1000 simulations' number of microsteps was 22.8. In Figure~\ref{fig:allsteps}, we see three simulations of the process from wave 1 to wave 2, with wave 1 shown on the left, and wave 2 shown on the right. In each of the three plots, the $y$-axis contains the edges sorted by how often they appear in the networks along the way. We can see that some edges are there in the beginning, but disappear and never come back, while others appear a few steps in, only to dispappear again. There are also some edges that exist in wave 2 that don't appear at all in the microstep process in a given repitition. 

<<allsteps, fig.cap="Three simulations (out of 1,000) of the microstep process from wave 1 to wave 2. The $x$ axis is the microstep number, with step 0 representing the first wave of data and the final step representing the second wave of data. We can see that many edges are underrepresented in this process: they are in the second wave, but never appear in the microsteps.">>=
smallfriends <- read_csv("data/smallfriends4Geomnet.csv")
names(smallfriends)[1:2] <- c("from", "to")
msall1 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 1))
msall1_df <- plyr::rbind.fill(lapply(X = seq_along(msall1), FUN = function(i, x) {
        x[[i]]$ms <- i - 1
        return(x[[i]])
    }, msall1))
msall1_df <- msall1_df %>% dplyr::select(from, to, ms)
wave2 <- smallfriends %>% filter(Wave == 2) %>% na.omit %>% data.frame()
msall1_df <- msall1_df %>% add_row(from = paste0("V",wave2$from),
                      to = paste0("V",wave2$to), 
                      ms = max(msall1_df$ms)+1)
edges <- msall1_df %>% 
  group_by(from, to) %>%
  summarize(edgeappears = n()) %>% 
  arrange(desc(edgeappears)) %>% ungroup() %>%
  mutate(eid = row_number())

msall1_df2 <- left_join(msall1_df, edges, by = c("from" = "from", "to" = "to"))

p1 <- ggplot(data = msall1_df2) + 
  geom_tile(aes(x = ms, y = -eid, fill = ms)) + 
  geom_vline(xintercept = c(0.5,81.5)) + 
  scale_fill_continuous(low = "red", high = "blue") + 
  ThemeNoNet + 
  theme(legend.position = 'none', axis.ticks.y = element_blank(),
        axis.text.y = element_blank(), axis.title = element_blank()) +
  labs(x = "Microstep")


# rep 2 
msall2 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 2))
msall2_df <- plyr::rbind.fill(lapply(X = seq_along(msall2), FUN = function(i, x) {
        x[[i]]$ms <- i - 1
        return(x[[i]])
    }, msall2))
msall2_df <- msall2_df %>% dplyr::select(from, to, ms)
msall2_df <- msall2_df %>% add_row(from = paste0("V",wave2$from),
                      to = paste0("V",wave2$to), 
                      ms = max(msall2_df$ms)+1)

edges2 <- msall2_df %>% 
  group_by(from, to) %>%
  summarize(edgeappears = n()) %>% 
  arrange(desc(edgeappears)) %>% ungroup() %>%
  mutate(eid = row_number())

msall2_df2 <- left_join(msall2_df, edges2, by = c("from" = "from", "to" = "to"))

p2 <- ggplot(data = msall2_df2) + 
  geom_tile(aes(x = ms, y = -eid, fill = ms)) + 
  geom_vline(xintercept = c(0.5,49.5)) + 
  scale_fill_continuous(low = "red", high = "blue") +
  ThemeNoNet + 
  theme(legend.position = 'none', axis.ticks.y = element_blank(),
        axis.text.y = element_blank(), axis.title = element_blank()) #+
  #labs(x = "Microstep", y = "Edges (most frequent at top)")

# rep 3 
msall3 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 3))
msall3_df <- plyr::rbind.fill(lapply(X = seq_along(msall3), FUN = function(i, x) {
        x[[i]]$ms <- i - 1
        return(x[[i]])
    }, msall3))
msall3_df <- msall3_df %>% dplyr::select(from, to, ms)
msall3_df <- msall3_df %>% add_row(from = paste0("V",wave2$from),
                      to = paste0("V",wave2$to), 
                      ms = max(msall3_df$ms)+1)

edges3 <- msall3_df %>% 
  group_by(from, to) %>%
  summarize(edgeappears = n()) %>% 
  arrange(desc(edgeappears)) %>% ungroup() %>%
  mutate(eid = row_number())

msall3_df2 <- left_join(msall3_df, edges3, by = c("from" = "from", "to" = "to"))

p3 <- ggplot(data = msall3_df2) + 
  geom_tile(aes(x = ms, y = -eid, fill = ms)) + 
  geom_vline(xintercept = c(0.5,106.5)) + 
  scale_fill_continuous(low = "red", high = "blue") +
  ThemeNoNet + 
  theme(legend.position = 'none', axis.ticks.y = element_blank(),
        axis.text.y = element_blank(), axis.title = element_blank()) #+
  #labs(x = "Microstep", y = "Edges (most frequent at top)")

grid.arrange(p1, p2, p3, nrow = 3)
@

\st{The algorithms here are many!
\begin{enumerate}
%\item First, the microsteps / CTMC between each time point (gifs, other animations)
\item Phases of RSiena: phase 1 = initial parameter estimate
\item phase 2 = iterative update of parameters - trace plots
\item phase 3 = simulations for convergence diagnostics
\end{enumerate}
 } 

\hh{The visualizations should address most elements of structure described before, i.e. follow along in the setup (for descriptive and teaching purposes), the fitting (understanding the model and interpreting the results) and then diagnostics (how well does the model fit the data - where are the most differences to the actual data/what are the sensitive parameters?) }

\hh{Some of the visualizations we talked about
\begin{enumerate}
\item visualization of the MCMC process:  movie of individual actors' step-by-step decisions; could be done with the ndtv package, as long as we can get individual steps out of RSiena
\item global picture: 1000 simulated networks + alpha blending should give a pretty good idea of the most important edges
%\item global structure + principal components (tensor pca)
\end{enumerate}}

\subsection{View the model in the data space}

In \citeauthor{modelvis}, they define the \textit{data space} as ``the region over which we can reliably make inferences, usually a hypercube containing the data" \citep[p.~206]{modelvis}. What does this definition mean for network data? For dynamic social networks, there are a few different data ``spaces." First, we have the actors and their corresponding covariates. Second, we have the dyad variables that describe the relationships between the variables. Finally, we have the time over which the network is allowed to evolve. These three data pieces can be visualized together in various ways. The traditional node-link visualization uses one of many algorithms to layout the actors as points in 2D space, then draws segments connecting the points in 2D if the dyad between two nodes has value greater than zero, and draws nothing otherwise. The time aspect can be visualized by drawing each network observation in time and placing the observed timepoints side-by-side. 

One tool that can bring these three data spaces together in this way is the \texttt{R} package \texttt{geomnet} \citep{geomnet}. Different aesthesic aspects of each piece of the node-link diagram can be tied to the underlying node or edge data. The color, size, and shape of the points can be used to represent variables in the node data, and the color, size, and linetype of the links between points can be used to represent the edge variables. In a social network, node data might be age, gender, and occupation of the person in the network, and node data might be length of connection between two people, how the people first met (school, work, church, etc.), and how often they interact. Pulling all of this information together with \texttt{geomnet} allows the entire data space to be viewed at once. 

XXX INSERT AN EXAMPLE OF DATA WITH LOTS OF EDGE, NODE VARS OF INTEREST XXX 

These visual connections between the node, edge, and time data spaces are themselves part of the data space, because they can be viewed in different ways. Another way to connect the node and the edge data, as discussed in Section~\ref{sec:netvizintro}, is through the network's adjaceny matrix. The time dimension can also be incorporated with side-by-side adjacency matrix visualizations. 

Another way to view the model in the data space is through simulation from the model. Since no single simulation from a model is going to look like the data, it's better to visualize many simulations together. For network data, one way to do this is through the traditional node-link diagram. In Figure~\ref{fig:summnetM1}, we show a summary network created with 1,000 simulations of the second wave of the network from Model 1. Only edges that appear in more than 5\% of the simulations (in at least 51 of the 1000 simulations) are included in the visualization, with edges that appear more frequently emphasized by thicker linewidths and a brighter red color. On either side of the summary network, we show the actual data, wave 1 on the left, and wave 2 on the right. \st{We can see that the structure of the summary network is much more similar to the first network observation than to the second network observation. The simulations, however, are supposed to represent the second wave of data, which is shown on the right in Figure~\ref{fig:summnetM1}. This is an indication that our simple model, $M1$ is doing a very poor job of capturing the change mechanism from the first to the second wave of observation.} % XXX ST - I was looking at the wrong one. I had wave == 2 instead of wave == 1 in the code. This means I was looking at the wrong simulations, so they really do look more like the first data obs than the second obs. XXX Comparing the three visualizations side-by-side shows us that the summary network of simulations of wave 2 looks more like the true wave 2 than wave 1, which is a good indication that the model is capturing the true data structure. It's not ideal, however, because there appear to be too many reciprocated ties in the summary network, far more than there are in the data. This could indicate that we should remove the reciprocity parameter from the model. 

% below chunk contains some repetitive stuff. Haven't figured out what it's purpose is yet. 
<<summnetM1, message=FALSE, fig.cap='On the left, the first wave of observed data that is conditioned on in the model. On the right, the second wave of observed data. In the middle, a summary network from the first model fit to the data. This summary network represents 1,000 simulations of wave 2 using the values from the simple fitted model $M1$.',results='hide', fig.height=3>>=
friendshipData <- array(c(fd2.w1, fd2.w2, fd2.w3), dim = c(16, 16, 3))
drink1 <- s50a[20:35,1]
drink2 <- s50a[20:35,2]
drink3 <- s50a[20:35,3]
friendship <- sienaDependent(friendshipData)
alcohol <- varCovar(cbind(drink1,drink2, drink3))
mydata <- sienaDataCreate(friendship, alcohol)
M1eff <- getEffects(mydata)
M2eff <- includeEffects(M1eff, jumpXTransTrip, interaction1 = "alcohol")
M3eff <- includeEffects(M1eff, nbrDist2twice)
simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")
means <- simu2 %>% group_by(Model, parameter) %>% summarize(
  means = mean(estimate)
)
M1parms <- (means %>% filter(Model == "M1"))$means
M2parms <- (means %>% filter(Model == "M2"))$means
M3parms <- (means %>% filter(Model == "M3"))$means
set.seed(4231352)
M1sims1000 <- netvizinf::saom_simulate(dat = mydata, 
                            struct = M1eff,
                       parms = as.numeric(M1parms), 
                       N = 1000)
M2sims1000 <- netvizinf::saom_simulate(dat = mydata, 
                            struct = M2eff,
                       parms = as.numeric(M2parms), 
                       N = 1000)
M3sims1000 <- netvizinf::saom_simulate(dat = mydata, 
                            struct = M3eff,
                       parms = as.numeric(M3parms), 
                       N = 1000)
M1simsdf <- sims_to_df(M1sims1000)
M2simsdf <- sims_to_df(M2sims1000)
M3simsdf <- sims_to_df(M3sims1000)
M1avgW2 <- M1simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M2avgW2 <- M2simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M3avgW2 <- M3simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))

# make a df of wave 2, wave 1, and the three averages and facet. 
names(actual1)[1:2] <- c("from", "to")
names(actual2)[1:2] <- c("from", "to")
actual1$count <- 1
actual1$weight <- 1
actual2$count <- 1
actual2$weight <- 1
actual1 <- actual1 %>% dplyr::select(from,to, count, weight)
actual2 <- actual2 %>% dplyr::select(from,to, count, weight)
actual1$cat <- "First Observation"
actual2$cat <- "Second Observation"

avgW2M1 <- M1avgW2 %>% ungroup() %>% 
  filter(weight > 50) %>% 
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model1")
add1 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M1$from), as.character(avgW2M1$to)))) 
avgW2M1 %>% add_row(from = add1, to = NA, count = NA, weight = NA, cat = "Model1") -> avgW2M1
avgW2M2 <- M2avgW2 %>% ungroup() %>% 
  filter(weight > 50) %>% 
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model2")
add2 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M2$from), as.character(avgW2M2$to)))) 
avgW2M2 %>% add_row(from = add2, to = NA, count = NA, weight = NA, cat = "Model2") -> avgW2M2
avgW2M3 <- M3avgW2 %>% ungroup() %>% 
  filter(weight > 50) %>% 
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model3")
add3 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M3$from), as.character(avgW2M3$to)))) 
avgW2M3 %>% add_row(from = add3, to = NA, count = NA, weight = NA, cat = "Model3") -> avgW2M3

combinedavgs <- rbind(actual1, actual2, avgW2M1, avgW2M2, avgW2M3)
combinedavgs %>% group_by(cat) %>% 
  mutate(linewidth = weight / max(weight,na.rm = T)) -> combinedavgs

combinedavgs %>% filter(cat == "Model1") %>% mutate(logweight = log(weight)) -> t1 
colors <- tweenr::tween_color(data = c("#969696", "#67000d"), n = 26, ease = 'linear')
t1 %>% arrange(logweight) -> t1
t1$color <- NA
t1$color[1:26] <- colors[[1]]

p1 <- ggplot(data = t1) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth),
           directed = T, curvature = .2, labelon=T, size = 5,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = t1$color,
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 4, arrowgap = .02,
           singletons = T) +
  labs(title = "Summary: M1") + ThemeNet +
  theme(plot.background = element_rect(color = 'black'))

p2 <- ggplot(data = filter(combinedavgs, cat == "First Observation")) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth/3),
           directed = T, curvature = .2, labelon=T, size = 5,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = "#969696",
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 4, arrowgap = .03,
           singletons = T) +
   labs(title = "Wave 1") + ThemeNet + 
    theme(plot.background = element_rect(color = 'black'))
p3 <- ggplot(data = filter(combinedavgs, cat == "Second Observation")) +
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth/3),
           directed = T, curvature = .2, labelon=T, size = 5,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = "#969696",
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 4, arrowgap = .03,
           singletons = T) +
  labs(title = "Wave 2")  +  ThemeNet + 
  theme(plot.background = element_rect(color = 'black')) 
grid.arrange(p2, p1, p3, nrow = 1)
@

\st{Because of the multi-level data structure of a network, viewing the model in the data space can depend on which aspect of the model you are interested in viewing. XXX Ideas: summary statistic dist. on nets simulated from model and show statistic from data; interactive plots showing objective function values on nodes; select node, select other node, show how the function values change interactively (future); goodness of fit should do here. XXX} 



% \st{
% \begin{itemize}
% \item WHat is the model? the network model (SAOM model is just one of many)
% \item What is the data? Edge data, node data, observed time points, unobserved timepoints (CTMC)
% \item Note: specifically dealing with dynamic networks observed at discrete time points. 
% \end{itemize}
% What are some things that could go in here? need more data examples; not just the friends data.
% Harry Potter data??? I want to find my own data, too.}

\subsection{Collections are more informative than singletons}

The second principle of Wickham et al states that ``collections are more informative than singletons" \citep[p.~210]{modelvis}. They describe several different methods for producing such collections, but we focus on only a few here. We look at collections produced ``from exploring the space of all possible models," ``by varying model settings", ``by fitting the same model to different datasets", and ``by treating each iteration as a model" \citep[p.~210-11]{modelvis}. We also fit the same model many times, resulting in distributions of fitted parameters for one data set.

A classical problem with statistical network models generally is the lack of an ``average network" measure. Statisticians frequently rely on averages and expected values, but statistical network models, especially those as complex as SAOMs, lack an intuitive expected value measure. We could talk about expected values of parameters, but there is no way to talk about the expected value of the network based on the model. How then, can we arrive at an ``average" network? We propose to answer this question through visualization.

We first consider the 16 actor subset of the teenage friends and lifestyle data available on the \texttt{RSiena} website \citep{RSiena}. To this data, we fit three different SAOMs. Each SAOM used a simple rate function, $\alpha_m$, and an objective function with two or three parameters. The first model, $M1$, contains the absolute minimum number of parameters in the objective function $f_i(x)$:  $$f_i(x)^{M1} = \beta_1s_{i1} + \beta_2s_{i2}$$, where $s_{i1}$ is the density network statistic and $s_{i2}$ is the reciprocity network statistic for actor $i$. The second and third models, $M2$ and $M3$, contain a third parameter in the objective function that was determined in the\texttt{RSienaTest} software to be significant, with $p$-values less than 0.05 \citep{RSienaTest}. The $M2$ model contains an actor-level covariate parameter, and the $M3$ model contains an additional strutural effect in the objective function.
\begin{align*}
f_i(x)^{M2} & = \beta_1s_{i1} + \beta_2s_{i2} + \beta_3s_{i3} \\
f_i(x)^{M3} & = \beta_1s_{i1} + \beta_2s_{i2} + \beta_4s_{i4}, 
\end{align*} where $s_{i3} = \sum_{j \neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(z_i = z_h \neq z_j)$, and $s_{i4} =  |\{j : x_{ij} = 0, \sum\limits_h x_{ih}x_{hj} \geq 2\}|$. These statistics are known as the number of jumping transitive triplets and the number of doubly achieved distances two effect, respectively. The first statistic emphasizes triad relationships that are formed between actors from different covariate groups, while the other emphasizes indirect ties between actors. These two effects are visually represented in Figure~\ref{fig:jtt} and ~\ref{fig:dad}, respectively. 

\begin{figure}\label{fig:structures}
\begin{subfigure}[t]{.45\textwidth}
\caption{\label{fig:jtt}Realization of a jumping transitive triplet, where $i$ is the focal actor, $j$ is the target actor, and $h$ is the intermediary. The group of the actors is represented by the shape of the node.}
<<jtt, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center',out.width='.49\\linewidth'>>=
jTTe <- data.frame(from = c('i', 'i', 'h'), to = c('h', 'j', 'j'))
jTTn <- data.frame(id = letters[8:10], group = c(1,1,2))

jTT <- merge(jTTe, jTTn, by.x = 'from', by.y = "id", all = T)

set.seed(12345) 
ggplot(data = jTT, aes(from_id = from, to_id = to)) + 
  geom_net(aes(shape = as.factor(group)), directed = T, labelon = T, 
           labelcolour='grey80',vjust = 0.5, hjust =0.5, arrowgap = .15, 
           colour = 'black', size=10, fontsize = 4,
           ecolour = c("red", "grey40", "grey40", "grey40")) + 
  expand_limits(x=c(-0.1,1.1), y=c(-0.1,1.1)) +
  ThemeNet +
  theme(legend.position = "none")
@
\end{subfigure}\hfill
\begin{subfigure}[t]{.45\textwidth}
\caption{\label{fig:dad}Doubly achieved distance between actors $i$ and $k$.}
<<dab, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center', out.width='.49\\linewidth'>>=
dade <- data.frame(from = c('i', 'i', 'h', 'j'), to = c('h', 'j', 'k', 'k'))
dadn <- data.frame(id = letters[8:11], group = c(1,1,1,1))

dad <- merge(dade, dadn, by.x = 'from', by.y = "id", all = T)

set.seed(12345) 
ggplot(data = dad, aes(from_id = from, to_id = to)) + 
  geom_net(aes(shape = as.factor(group)), directed = T, labelon = T, labelcolour='grey80',vjust = 0.5, hjust =0.5, arrowgap = .15, colour = 'black', size=10, fontsize = 4) + 
  expand_limits(x=c(-0.1,1.1), y=c(-0.1,1.1)) +
  ThemeNet +
  theme(legend.position = "none")
@
\end{subfigure}
\caption{Structural newtwork effects. On the left, a jumping transitive triplet (JTT). On the right, a doubly achieved distance between $i$ and $k$.}
\end{figure}
We these three models using \texttt{RSiena} 1,000 times each. We then looked at the distribution of the fitted values, which are shown in Figure~\ref{fig:distests}. We can see from these distributions that the inclusion of the jumping transitive triplet parameter, $\beta_3$ is obviously affecting the distributions of the other four parameters included in all models, $\alpha_1$, $\alpha_2$, $\beta_1$, and $\beta2$. In comparison with models M1 and M3, model M2 typically has higher estimates of the rate parameter, meaning that the inclusion of the covariate statistic in the model leads to higher estimates of the number of times, on average, a node gets to change its ties. This seems problematic: it is not clear that the addition of a parameter to the objective function \textit{should} effect the estimation of the rate parameters. 

To further investigate this relationship between the parameter values, we look at correlations between each of the parameter estimates in each model. In Figure~\ref{fig:corplots}, we examine correlations between each of pair of parameters within each model and overall.\nocite{ggally} The strongest correlation within each model is between $\beta_1$ and $\beta_2$, with absolute value of correlation between those two parameter values greater than 0.90 in all three models. The $\beta_1$ parameter is also highly correlated with the $\beta_3$ parameter within model M2, but it is not as highly correlated with the $\beta_4$ parameter in model M3. 

<<getdata>>=
nulls <- read.csv("data/distribution_null_model.csv")
alt <- read.csv("data/distribution_jumpTT_model.csv")
alt2nd <- read.csv("data/distribution_dblpairs_model.csv")
simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")
means <- simu2 %>% group_by(Model, parameter) %>% summarize(
  means = mean(estimate)
)
null_mod_sv <- as.numeric(nulls[,-c(1,6,7)] %>% summarise_each(funs(mean)))
alt_mod_sv <- as.numeric(alt[,-c(1, 7,8)] %>% summarise_each(funs(mean)))
alt_mod2_sv <- as.numeric(alt2nd[,-c(1,7,8)] %>% summarise_each(funs(mean)))
@

<<distests, fig.cap="Distribution of fitted parameter values for our three SAOMs. The inclusion of $\\beta_3$ or $\\beta_4$ clearly has an effect on the distribution of the rate parameters, $\\alpha_1$ and $\\alpha_2$.", fig.width=8>>=

#simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")

ggplot(data = simu2) + 
  geom_density(aes(x = estimate, fill = Model), alpha = .5) + 
  facet_wrap(~parameter, scales = 'free') + 
  ThemeNoNet
@


<<corplots, fig.cap="A matrix of plots demonstrating the strong correlations between parameter estimate in our SAOMs. The strongest correlation within each model is between $\\beta_1$ and $\\beta_2$.", fig.width=8>>=

#simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")

simu_spread <- simu2 %>% spread(parameter, estimate)
ggpairs(simu_spread, columns = 3:8, ggplot2::aes(colour=Model, alpha = .5)) + ThemeNoNet
@

We also explore simulations from these different models given parameter values. Using the means from the 1,000 fitted parameter values, we simulated 1,000 observations from each of our three models. We condition on the first friendship network observation, and the second and third observations are simulated from the SAOM models estimated previously. From these simulations, we first create a visualization that represents an ``average" network. To do this, we count occurences of each possible edge in the simulations, resulting in a summary network with weighted edges representing the number of times an edge appeared in the simulated wave 2 when simulating from the SAOM 1000 times. In Figure~\ref{fig:averagenet}, we show the first wave, the ``average'' network from the three models we fit, and the second wave. \st{Comparing the three averages to waves 1 and 2, we see that they have very similar structure to wave 1. Model 2, which included the transitive triplet parameter, seems to have created more group structure overall than models 1 and 3. In particular, if we look at the group of nodes $\{10,11,14\}$, we see they are very strongly connected within the three average networks, and they are completely separate from the other nodes in the true wave 2. None of the three average networks show node 16 gaining ties as it does in wave two, nor do they show nodes 4 and 7 becoming isolated. In Model 2, however, the ties to node 7 appear much weaker than in Model 1 or Model 2, suggesting that of the three, Model 2 is the best fit for our data.}  %XXXX ST commented out after I realized I had made the same mistake as figure 7. XX This could be an indication that, despite the highly significant nature of the parameter $\beta_3$ according to the $t$-tests in \texttt{RSiena}, the transitive triplet parameter should not be included because it does not result in a structure similar to the data. 

<<averagenet, results='hide', out.width='99%',fig.cap='The three "average" networks in the middle, with the actual network observations oon either side. There is some difference between the three models, but overall, these three models cannot capture the structure in the true second wave of data.', fig.height=3>>=
#friend.data.w1 <- as.matrix(read.table("s50_data/s50-network1.dat"))
#friend.data.w2 <- as.matrix(read.table("s50_data/s50-network2.dat"))
#friend.data.w3 <- as.matrix(read.table("s50_data/s50-network3.dat"))
#drink <- as.matrix(read.table("s50_data/s50-alcohol.dat"))
friendshipData <- array(c(fd2.w1, fd2.w2, fd2.w3), dim = c(16, 16, 3))
friendship <- sienaDependent(friendshipData)
alcohol <- varCovar(drink[20:35,])
mydata <- sienaDataCreate(friendship, alcohol)
M1eff <- getEffects(mydata)
M2eff <- includeEffects(M1eff, jumpXTransTrip, interaction1 = "alcohol")
M3eff <- includeEffects(M1eff, nbrDist2twice)
data(M1ests_bigfriends)
M1parms <- (means %>% filter(Model == "M1"))$means
M2parms <- (means %>% filter(Model == "M2"))$means
M3parms <- (means %>% filter(Model == "M3"))$means
# M1sims1000 <- saom_simulate(dat = mydata, 
#                             struct = M1eff,
#                        parms = as.numeric(M1parms), 
#                        N = 1000)
# M2sims1000 <- saom_simulate(dat = mydata, 
#                             struct = M2eff,
#                        parms = as.numeric(M2parms), 
#                        N = 1000)
# M3sims1000 <- saom_simulate(dat = mydata, 
#                             struct = M3eff,
#                        parms = as.numeric(M3parms), 
#                        N = 1000)
# M1simsdf <- sims_to_df(M1sims1000)
# M2simsdf <- sims_to_df(M2sims1000)
# M3simsdf <- sims_to_df(M3sims1000)
M1avg <- M1simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M1avgW2 <- M1simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M1avg2 <- M1simsdf %>% filter(!is.na(to)) %>%
 group_by(from, to, wave) %>%
 summarise(count = n()) %>%
 mutate(weight = ifelse(from == to, 0, count))
M2avg <- M2simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M2avgW2 <- M2simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M2avg2 <- M2simsdf %>% filter(!is.na(to)) %>%
 group_by(from, to, wave) %>%
 summarise(count = n()) %>%
 mutate(weight = ifelse(from == to, 0, count))
M3avg <- M3simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M3avgW2 <- M3simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M3avg2 <- M3simsdf %>% filter(!is.na(to)) %>%
 group_by(from, to, wave) %>%
 summarise(count = n()) %>%
 mutate(weight = ifelse(from == to, 0, count))

# M1avg$from <- paste0("V", M1avg$from)
# M1avg$to <- paste0("V", M1avg$to)
# M1avg2$from <- paste0("V", M1avg2$from)
# M1avg2$to <- paste0("V", M1avg2$to)
# M1avgW2$from <- paste0("V", M1avgW2$from)
# M1avgW2$to <- paste0("V", M1avgW2$to)
# M2avg$from <- paste0("V", M2avg$from)
# M2avg$to <- paste0("V", M2avg$to)
# M2avgW2$from <- paste0("V", M2avgW2$from)
# M2avgW2$to <- paste0("V", M2avgW2$to)
# M2avg2$from <- paste0("V", M2avg2$from)
# M2avg2$to <- paste0("V", M2avg2$to)
# M3avg$from <- paste0("V", M3avg$from)
# M3avg$to <- paste0("V", M3avg$to)
# M3avgW2$from <- paste0("V", M3avgW2$from)
# M3avgW2$to <- paste0("V", M3avgW2$to)
# M3avg2$from <- paste0("V", M3avg2$from)
# M3avg2$to <- paste0("V", M3avg2$to)
combinedavgs$linewidth[combinedavgs$cat %in% c("First Observation", "Second Observation")] <- .5
ggplot(data = combinedavgs) + 
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth),
           directed = T, curvature = .2, labelon=T, size = 5,
           arrow = arrow(angle = 20, type = "open", length = unit(.2, "cm")), ecolour = "#969696", fiteach = T, 
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 4, arrowgap = .02,
           singletons = T) + 
  ThemeNet + theme(panel.background = element_rect(color = 'black')) + 
  facet_wrap(~cat, nrow = 1)

# wave2drink <- drink2[,2]
# wave2drink <- as.data.frame(wave2drink)
# wave2drink$id <- paste0("V", 1:16)
# names(wave2drink)[1] <- "drink"
# wave3drink <- drink2[,3]
# wave3drink <- as.data.frame(wave3drink)
# wave3drink$id <- paste0("V", 1:16)
# names(wave3drink)[1] <- "drink"
# M1avgcov <- merge(M1avg, wave2drink, by.x = "from", by.y = "id", all = T)
# M2avgcov <- merge(M2avg, wave2drink, by.x = "from", by.y = "id", all = T)
# M3avgcov <- merge(M3avg, wave2drink, by.x = "from", by.y = "id", all = T)
# M1avgW2cov <- merge(M1avgW2, wave3drink, by.x = "from", by.y = "id", all = T)
# M2avgW2cov <- merge(M2avgW2, wave3drink, by.x = "from", by.y = "id", all = T)
# M3avgW2cov <- merge(M3avgW2, wave3drink, by.x = "from", by.y = "id", all = T)
# 
# M1avg[,-3] %>% spread(to, weight) %>% data.frame-> distmatM1
# rownames(distmatM1) <- distmatM1$from
# distmatM1 <- distmatM1[,-1]
# distmatM1 <- as.matrix(sqrt(1/distmatM1))
# distmatM1[is.na(distmatM1)] <- 0
# M1avgW2[,-3] %>% spread(to, weight) %>% data.frame-> distmatM1W2
# rownames(distmatM1W2) <- distmatM1W2$from
# distmatM1W2 <- distmatM1W2[,-1]
# distmatM1W2 <- as.matrix(sqrt(1/distmatM1W2))
# distmatM1W2[is.na(distmatM1W2)] <- 0
# M2avg[,-3] %>% spread(to, weight) %>% data.frame-> distmatM2
# rownames(distmatM2) <- distmatM2$from
# distmatM2 <- distmatM2[,-1]
# distmatM2 <- as.matrix(sqrt(1/distmatM2))
# distmatM2[is.na(distmatM2)] <- 0
# M2avgW2[,-3] %>% spread(to, weight) %>% data.frame-> distmatM2W2
# rownames(distmatM2W2) <- distmatM2W2$from
# distmatM2W2 <- distmatM2W2[,-1]
# distmatM2W2 <- as.matrix(sqrt(1/distmatM2W2))
# distmatM2W2[is.na(distmatM2W2)] <- 0
# M3avg[,-3] %>% spread(to, weight) %>% data.frame-> distmatM3
# rownames(distmatM3) <- distmatM3$from
# distmatM3 <- distmatM3[,-1]
# distmatM3 <- as.matrix(sqrt(1/distmatM3))
# distmatM3[is.na(distmatM3)] <- 0
# M3avgW2[,-3] %>% spread(to, weight) %>% data.frame-> distmatM3W2
# rownames(distmatM3W2) <- distmatM3W2$from
# distmatM3W2 <- distmatM3W2[,-1]
# distmatM3W2 <- as.matrix(sqrt(1/distmatM3W2))
# distmatM3W2[is.na(distmatM3W2)] <- 0

# cols <- brewer.pal(n = 4, "YlOrRd")
# 
# plotM1W2 <- ggplot(data = M1avgcov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/251, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM1)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 2, M1") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
#   labs(title = "Model 1 Simulated Wave 2")
# plotM1W3 <- ggplot(data = M1avgW2cov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/388.5, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM1W2)
#            ) + 
#   scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 3, M1") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# plotM2W2 <- ggplot(data = M2avgcov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/325, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM2)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 2, M2") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
#   labs(title = "Model 2 Simulated Wave 2")
# plotM2W3 <- ggplot(data = M2avgW2cov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/357, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM2W2)
#            ) + 
#   scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 3, M2") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# plotM3W2 <- ggplot(data = M3avgcov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/258, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM3)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 2, M3") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
#   labs(title = "Model 3 Simulated Wave 2")
# plotM3W3 <- ggplot(data = M3avgW2cov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/372.5, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM3W2)
#            ) + 
#   scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 3, M3") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# 
# origW1 <- ggplot(data = alldat %>% filter(Wave==1), 
#        aes(from_id = X1, to_id = X2, color = as.ordered(drink))) +
#   geom_net(fiteach = FALSE, directed=TRUE, layout.alg = "kamadakawai", size = 3, arrowsize = .5, curvature = .2,
#            arrowgap = .03) +
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   theme_net() +
#   theme(legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# origW2 <- ggplot(data = alldat %>% filter(Wave==2), 
#        aes(from_id = X1, to_id = X2, color = as.ordered(drink))) +
#   geom_net(fiteach = FALSE, directed=TRUE, layout.alg = "kamadakawai", size = 3, arrowsize = .5, curvature = .2,
#            arrowgap = .03) +
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   theme_net() +
#   theme(legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
#   labs(title = "Observed Wave 2")
# origW3 <- ggplot(data = alldat %>% filter(Wave==3), 
#        aes(from_id = X1, to_id = X2, color = as.ordered(drink))) +
#   geom_net(fiteach = FALSE, directed=TRUE, layout.alg = "kamadakawai", size = 3, arrowsize = .5, curvature = .2,
#            arrowgap = .03) +
#   scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
#   theme_net() +
#   theme(legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# 
# grid.arrange(origW2, plotM1W2,
#               plotM2W2,plotM3W2, 
#               ncol=2, clip = T)
# alldataviz2 <- alldataviz + theme(legend.position = 'none')
# ggdraw() +
#   draw_plot(alldataviz2, x=0, y=.75, width=1, height=.25) + 
#   draw_plot(plotM1W2, .1, .5, .5, .25) +
#   draw_plot(plotM1W3, .3, .5, .5, .25) +
#   draw_plot(plotM2W2, .1, .25, .5, .25) +
#   draw_plot(plotM2W3, .3, .25, .5, .25) +
#   draw_plot(plotM3W2, .1, 0, .5, .25) +
#   draw_plot(plotM3W3, .3, 0, .5, .25) + 
#   draw_plot_label(label = c("Original Data", "M1: Wave 2", "M1: Wave 3", "M2: Wave 2", "M2: Wave 3", "M3: Wave 2", "M3: Wave 3"))
# 
# M1avg2$Model <- "M1"
# M2avg2$Model <- "M2"
# M3avg2$Model <- "M3"
# allavg <- rbind(as.data.frame(M1avg2), 
#                 as.data.frame(M2avg2),
#                 as.data.frame(M3avg2))
# 
# 
# ggplot(data = allavg) + 
#   geom_histogram(aes(x = weight), binwidth = 25) + 
#   facet_grid(wave~Model, labeller = "label_both") + 
#   labs(x = "Number of times an edge occurs in 1,000 simulations") +
#   theme_bw()

# ggplot(data = M3avgcov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/251, color = as.factor(drink)), 
#            directed = TRUE, curvature = .2,
#            ealpha = .75,
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM2)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net()
# M1avg2$wave <- M1avg2$wave + 1
# alldrink <- data.frame(drink2[,2:3])
# alldrink <- gather(alldrink, wave,drink) %>% mutate(wave = parse_number(wave), id = rep(paste0("V",1:16),2))

# M1avg2cov <- merge(M1avg2, alldrink, by.x = c("from", "wave"), by.y = c("id", "wave"), all = T)
# ggplot(data = M1avg2cov) + # edge appears > 50% of the time. (is this actually what that is????) 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/388, color = as.factor(drink)), 
#            directed = TRUE, curvature = .2, labelon = F,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai", fiteach = T,
#            layout.par = list(elen = distmat)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   facet_wrap(~wave, labeller = "label_both") + theme_net() + labs(title = "Average Wave 2 and Wave 3 Networks from Model M1")
# ggplot(data = M1avg %>% filter(weight >= 51)) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/500), 
#            #directed = TRUE,
#            layout.alg = "mds", 
#            layout.par = list(var = 'user', dist = 'none', exp = 3,vm = distmat))
@


\st{What are some things that could go in here?:
\begin{itemize}
\item "average" networks -- you've already done this! Write it up, make several examples. How does the procedure for averaging generalize? What is it good for? 
\item distributions of fitted parameters. again, you've already done this!!! just write it up and put it in here. 
\item view correlations between model parameters under different model sets. eg 2 betas, 3 betas, 4 betas, etc.
\end{itemize}
 }



\section{Discussion} \label{sec:discussion}

\bibliographystyle{asa}
\bibliography{references}

\end{document}
