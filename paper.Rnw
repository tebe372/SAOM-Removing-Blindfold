\documentclass[12pt]{article}
% \usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
% \geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\usepackage{graphicx}
\usepackage[font=small,skip=5pt]{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
% \usepackage{color}
\usepackage{multirow}
\usepackage{rotating}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\usepackage{hyperref}
\graphicspath{{figure/}}
% \usepackage{endfloat} % Figures to the end of the document

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%---------------------------------------------------
%                 Editing Commands
\newcommand{\hh}[1]{{\color{magenta} #1}}
\newcommand{\st}[1]{{\color{orange} #1}}

%---------------------------------------------------
%                 Placing Figures


%---------------------------------------------------
% Define new environment
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}
%---------------------------------------------------

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Model Visualization Techniques for a Social Network Model}
  \author{Samantha Tyner\thanks{
    The authors gratefully acknowledge funding from the National Science Foundation Grant \# DMS 1007697. All data collection has been conducted with approval from the Institutional Review Board IRB 10-347}\hspace{.2cm}\\
    Department of Statistics and Statistical Laboratory, Iowa State University\\
    and \\
    Heike Hofmann\\
    Department of Statistics and Statistical Laboratory, Iowa State University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Model Visualization Techniques for a Social Network Model}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
 \hh{XXX just a placeholder}
\end{abstract}

\noindent%
{\it Keywords:} social network analysis, model visualization, dynamic networks, network visualization, network mapping, animation
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\tableofcontents
\newpage

<<setup, echo = FALSE, message = FALSE, warning = FALSE>>=
options(replace.assign=TRUE,width=70, digits=3)
require(knitr)
opts_chunk$set(fig.path='figure/', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, par=TRUE, cache=TRUE, concordance=TRUE, autodep=TRUE, message=F, warning=F, echo = FALSE, root.dir = "~/Desktop/Dissertation - LETS GO/SAOM-removing-blindfold/")
@

<<pkgs>>=
library(tidyverse)
library(RSiena)
library(network)
library(sna)
library(geomnet)
library(GGally)
library(netvizinf)
library(RColorBrewer)
library(gridExtra)
library(cowplot)
@

\section{Introduction} 
% \st{
% \begin{itemize}
% %\item What? - model visualization and SAOMs. briefly dsecribe what these things are
% %\item Why? - bring light to the RSiena blackbox. Need ways to visualize network models, collections of networks. 
% \item How?
% \end{itemize}
% %also talk about other dynamic network models briefly to put SAOMs in context.
% }
%\hh{XXX - talk a bit about social networks first, why we want to model them, and that usually the models are not dynamic.}

Social networks have been studied for decades, beginning with a few foundational works, the most well known of which is the 1967 study, "The Small World Problem" by Stanley Milgram \citep{goldenberg09}. Examples of social networks include collaboration networks between academic researchers, friendship networks in a school or university, and trade networks between nations. In recent years, the study of social networks has grown in popularity due to an increase in the availability and access to social network data. There are many kinds of social networks, but there are not as many statistical models for social network data. Some network models that have been applied to social networks include the exponential random graph model and latent space models. These models, however, are only for single instance networks. If we only care about one network observation, that is not a problem, but if we have many observations of social network over time, these models may not be appropriate. When studying a network over time, which is referred to as a \textit{dynamic network}, we need a model that can take time into account. 
Models for dynamic social networks are extremely important because of how realistic they are. Social networks do not form spontaneously; they evolve over time. Ties are added and removed, and new actors join the network. Modeling the underlying mechanisms that create network changes over time is very complex but also has the potential to uncover hidden truths. 

\par In this paper, we concentrate on one type of model for dynamic social networks: the stochastic actor-oriented models (SAOMs), introduced by \citet{saompaper}. These models are fundamentally different from other social network models because they allow us to incorporate network \textit{and} actor statistics, when other models only rely on  the network statistics to determine change. Letting the actor-level statistics change the structure of the network leads to a more practical and relevant approach to model change in a social network: in the ``real" world we expect people with common interests to be more likely to form relationships. SAOMs allow us to incorporate this intuition in the modeling process. 

\st{
%\par In order to estimate the parameters of  SAOMs the software SIENA, and its \texttt{R} implementation \texttt{RSiena}, was developed \cite{RSiena}. This software marks a huge contribution to the field of social network analysis., and when we began working with it we were eager to understand how it works. HH: we are still interested in this.
\hh{The next sentence is very vague - make it more specific: what aspect of SAOMs is not well understood? I think what you want is something like ``SAOMs are not very tractable analytically with traditional methods, e.g. likelihood functions turn into very complex objects because of the sheer number of parameters involved. Therefore, computationally more tractable solutions are used to fit estimators, in particular, SAOMs are fit using ... }
Unlike other network models, SAOMs are not very well understood. Traditional modeling steps, like determining the likelihood function, are very complex. Because of this, SAOMs are fit to data using a series of MCMC phases for finding method of moments estimators. Rather than treat SAOM fits and the fitting process as black boxes, we combine the principals of network visualization with those of model visualization as discussed in \citet{modelvis}. %Here, we explore RSiena and SAOMs with innovative network visualizations, both static and dynamic.
By bringing some light to the underlying methods and structures ``behind the scenes" of SAOMs, we help researchers working with the models better understand the implications and analysis of these models.
}


\par The guiding principles of model visualization introduced in \citet{modelvis} are (1) display the model in the data space, (2) collections of models are more informative than singletons, and (3) explore the fitting process rather than just looking at the final result. Stochastic actor-oriented models are a prime example of a set of models that  benefit greatly from application of model visualization. First, the models themselves include a continuous-time Markov chain (CTMC) that is completely hidden in the model fitting process. Bringing the CTMC out of the black box and into the light of visualization  provides researchers with insights into the underlying features of the model. Secondly, SAOMs allow for a great deal of parameters that can be added to the objective function, each of which is attached to a network statistic. These statistics are often somewhat, if not highly, correlated, and so are the associated parameters. By visualizing collections of SAOMs, we gain a better understanding of these correlations and find ways to deal with them and rectify their effects. Finally, estimation of the parameters in a SAOM relies on a Robbins-Monro algorithm, and convergence checks for these estimates rely on simulations from the fitted model. Again, these steps are largely kept in the background of the estimation process. With the help of static and dynamic visualizations we bring these elements and others into the foreground, leading to a better understanding and higher accessibility of stochastic actor-oriented models  for social network analysts. 

In Section~\ref{sec:netintro}, we introduce the concepts of networks and network visualizations. In Section~\ref{sec:saoms}, we present the SAOM for social network analysis. In Section~\ref{sec:ModelVis}, we combine concepts from Sections~\ref{sec:netintro} and \ref{sec:saoms} in an application of the model-vis paradigm, and conclude with a discussion in Section~\ref{sec:discussion}.

\section{Networks and their Visualizations} \label{sec:netintro}

\subsection{Introduction to Network Structures}

Network data is of frequent interest to researchers in a wide array of fields. There are technological networks, like power grids or the internet, information networks, such as citation networks or the World Wide Web, biological networks, e.g. neural networks, and social networks, just to name a few \citep{newman}. Each of these examples have one thing in common: their data structure. There are always units of observation, which we refer to throughout this paper as \textit{nodes}, and connections of some kind between those units, which we will call \textit{edges}. Networks might change over time, for example when new websites and hyperlinks are added on the World Wide Web, or when there are new people and relationships in a friendship network. Nodes and edges themselves also have inherent variables of interest, e.g.\ the affiliation of authors in a co-authorship network, or the number of times two authors have collaborated. 

The multiple layers of network data structures create unique struggles to network analysts. Some questions that researchers aim to answer are: How does the strength of a connection between two nodes affect the overall structure of the network? Do node-level differences affect the formation or destruction of edges? Which views of the data are most informative for communicating effects and results of statistical analyses? These are, of course, just a few broad questions. In this paper we want to focus on the visual exploration of network data and models.

\subsection{Visualizing Network Data} \label{sec:netvizintro}

Network visualization, also called network mapping, is a prominent subfield of network analysis. Visualizing network data is uniquely difficult because of the structure of the data itself.  Most, if not all, data visualizations rely on well-defined axes inherited from the data. If variables are numerical, histograms, scatterplots, or time series plots are straightforward to construct. If the variables are categorical, bar charts and mosaic plots are available to to the researcher. If the data are spatial, there is a well-defined region in which to view information. Network data, however, are much less cut-and-dried.

There are two primary methods used to visualize networks: node-link diagrams and matrix visualization \citep{knuth2000, adjmatpro}. As a toy example, let us assume that we have five nodes, $\{1,2,3,4,5\}$, connected by five edges:  $\{2 \mapto 4, 3 \mapto 4, 1 \mapto 5, 3 \mapto 5, 5 \mapto 4\}$ We use this toy data set to demonstrate the two visualization methods in Figure~\ref{fig:toyplots}.

<<toyex, echo = FALSE>>=
set.seed(81393)
nodes <- data.frame(id = 1:5, group = sample(LETTERS[1:2], size = 5, replace = T), stringsAsFactors = F)
g <- igraph::as_data_frame(igraph::random.graph.game(n = 5, p = .4))
toynet <- list(nodes = nodes, edges = rbind(g, c(5,4)))
@

The first method, the node-link diagram, represents nodes with points in 2D Euclidean space and then represents edges by connecting the points with lines when there is an edge between the two nodes. But because there is no natural placement of the points, a random placement is used, then adjusted iteratively via a layout algorithm, of which there are many. The node-link diagram for our toy network is shown in Figure~\ref{fig:toyplots}.

Some layout algorithms were designed to mimic physical systems, drawing the graphs based on the ``forces" connecting them. The network's edges act as springs pushing and pulling the nodes in a lower dimensional (usually two-dimensional) space. %Other algorithms use properties of the network's adjaceny matrix in order to place the nodes. \hh{XXX how does the adjacency matrix feature here? The adjacency matrix is  a different mathematical representation of a network, but describes the same network as a node-link diagram.} 
Some commonly used layout algorithms, such as  the Kamada-Kawai layout \citep{kamadakawai} and the Fruchterman-Reingold layout \citep{fruchterman-reingold}, treat networks like physical systems. Other algorithms are based on multi-dimensional scaling, which relies on a defined distance matrix, or an eigen value, which depends on the eigenvalues of the network's adjacency matrix. 

The second primary method for network visualization uses the adjacency matrix of the network. \hh{XXX what is an adjacency matrix? - explain at this point in mathematical terms} An example of this type of visualization is also shown in Figure~\ref{fig:toyplots} for our toy example. This method is particularly useful when the network is very complex, dense, or large. Experimental studies have shown this to be true. For example, with seven basic perceptual tasks on networks, including things like node and edge count, adjacency matrix visualizations outperform node-link diagrams as the size and density of the network increases \citep{adjmatviz}. One drawback of the adjacency matrix visualization that \citeauthor{adjmatviz} found was that edges are overrepresented for undirected graphs. \hh{XXX What is the exact logic here that Ghoniem used to come to the conclusion that edges in undirected graphs are over-represented? Why would directed graphs make that better? wouldn't that just add to the number of edges? } This, however, may actually be an advantage for \emph{directed} graphs, where exactly the correct number of edges is represented in a matrix visualization, but a node-link diagram may underrepresent the edge count with overlapping edges.  

<<toyplots, fig.show='hold', fig.cap="On the left, a node-link diagram of our undirected toy network, with nodes placed using the Kamada-Kawai algorithm. On the right, the adjacency matrix visualization for that same network.", out.width='.49\\linewidth'>>=
toynetdat <- fortify(as.edgedf(toynet$edges), toynet$nodes)
ggplot(data = toynetdat) + 
  geom_net(aes(from_id=from_id, to_id=to_id, shape =group, color = group), labelon = T, size = 10, labelcolour = 'white', ecolour = 'black', vjust = .5, hjust = .5, arrowgap = .02) +
  scale_color_manual(values = c("red", "blue")) + 
  theme_net() + 
  labs(title = "Node-Link Diagram")

ggplot(data = toynet$edges) +
  geom_tile(aes(x = from, y = to), color = 'grey20', fill = 'black') +
  geom_tile(aes(x = to, y = from), color = 'grey20', fill = 'black') +
  geom_segment(aes(x = x, xend = xend), y = 0.5, yend=5.5, colour="grey30", data = data.frame(x = 0:5+.5, xend= 0:5+.5)) + 
  geom_segment(aes(y=y, yend=yend), x = 0.5, xend=5.5, colour="grey30", data = data.frame(y = 0:5+.5, yend= 0:5+.5)) + 
  scale_x_continuous(breaks = 1:5, labels = 1:5, limits = c(0.5,5.5)) +
  scale_y_continuous(breaks = 1:5, labels = 1:5, limits = c(0.5,5.5)) +
  theme_bw() + 
  theme(panel.grid = element_blank()) +
  labs(title = "Adjacency Matrix Visualization") + 
  theme(aspect.ratio = 1) 
@
 \hh{XXX Explain each of these visualizations and either comment on the differences in the visualizations (colors) or don't show them. }

\section{Stochastic Actor-Oriented Models for Longitudinal Social Networks} \label{sec:saoms}

% \st{What do I need to communicate in this section? 
% \begin{itemize}
% \item What the models are for: dynamic social networks with actor-level covariates
% \item What are the model components? rate function, objective function (network statistics, actor covariates, interactions between covariates and network statistics), time points ($t_m = 1, \dots, M$)
% \item What is RSiena? What methods of estimation, convergence checks does it use? What are the algorithms? What kinds of models can it fit? 
% \end{itemize}}


%The stochastic actor-oriented models are very aptly named: they model dynamic networks that can account for the individual node characteristics when making the edge changes. The object of analysis is a social network that has been observed at several discrete time points. The nodes in the network can have associated covariate variables that may be incorporated into modeling the tie changes in the network. The model itself has two primary pieces: the rate of change of ties, and the objective function of the nodes, which determines the exact change that gets made. 

%We start with a set of network observations at $M$ discrete time points: $x(t_1), x(t_2) \dots, x(t_M)$. The first observation, $x(t_1)$ is always conditioned on. The number of nodes / actors / vertices is constant in all time points and is denoted $n$.  The tie variables are directed and  binary, and are denoted at $x_{ij}(t_m)$ for time point $m$, and this is sometimes abbreviated to $x_{ij}$ in notation. If there is a tie from node $i$ to node $j$, then $x_{ij}=1$, and $x_{ij}=0$ otherwise. 

%\hh{mostly lit review plus a detailed description of the model and fitting process.}

A Stochastic Actor-Oriented Model (SAOM) is a model that is changing in time in order to accomodate for observations from the same network made at different points in time and that allows for changes in network structure due to actor-level covariates. This model was first introduced by Snijders in 1996 \citep{saompaper}. The two main properties of SAOMs, modelling the changes in time and using actor covariates, are crucial to understanding networks as they exist naturally. Most social networks, even holding constant the set of actors over time, are ever-changing as relationships decay or grow, and most actors (or nodes) in social networks have inherent properties that could affect how they change their place within the network. 

\subsection{Definitions, Terminology, and Notation}

In this paper, the term \textit{dynamic network} refers to a network consisting of the same set of $n$ nodes that is changing over time, and is observed at $M$ discrete time points, $t_1, \dots, t_M$. We denote these network observations $x(t_1), \dots, x(t_M)$, and in modelling we condition on the first observation, $x(t_1)$. The SAOM assumes that this longitudinal network is embedded within a continuous time Markov chain (CTMC), call it $X(T)$. This process is almost entirely unobserved: assume that the beginning of the process, $X(0)$ is equivalent to the first network observation $x(t_1)$, while the end of the process $X(\infty)$ is equivalent to the last observation $x(t_M)$. All other parts of the process are unseen. The process $X(T)$ is a series of single tie (or edge) changes, in which one actor at a time is given the opportunity to add or remove one outgoing tie. Once an actor is given the chance to change a tie, it tries to maximize a sort of utility function based on the current and potential future states of the network. 

\subsubsection{The Rate Function}

For the network $x$ and each actor $i$ in the network, the rate function dictates how often the actor $i$ gets to change its ties, $x_{ij}$, to other nodes $j \neq i$ in the network. For this paper, we assume a simple rate function, $\alpha_m$ that is constant for all actors between observations at time $t_m$ to $t_{m+1}$. In other modelling scenarios, this rate can be a function that depends on the time period of observation, some actor-level covariates or some actor-level network statistics. The rate parameter determines how quickly actor $i$ gets an opportunity to change one of its ties, $x_{ij}$ in the time period $t_{m} \leq T < t_{m+1}$. We assume that the actors $i$ are conditionally independent given their current ties, $x_{i1}, \dots, x_{in}$. This assumption leads to the rate for the whole network, $n\alpha$. In order to achieve the memorylessness property of a Markov process, for any time point, $T$, where $t_{m} \leq T < t_{m+1}$, the waiting time to the next change opportunity by actor $i$ is exponentially distributed with expected value $\alpha_m^{-1}$. Thus, the waiting time to the next change opportunity by \textit{any} actor in the network is also exponentially distributed with expected value $(n\alpha_m)^{-1}$

\hh{XXX we tried to estimate the rate based on the simulations - you could include a histogram of the number of changes for the 1000 simulations in the viz part XXX}

 
\subsubsection{The Objective Function} 

Thanks to the conditional dependence assumptions in the model, we can consider the objective function for each node separately, since only one tie from one node is changing at a time. The objective function is written as $$f_i(\boldsymbol{\beta}, x) = \sum_k \beta_k s_{ik}(x, \mathbf{Z}),$$
for $x \in \mathcal{X}$, the space of all possible directed networks with the $n$ nodes, and $\mathbf{Z}$ matrix of covariates. The vector $\boldsymbol{\beta}$ are the parameters of the model with corresponding network and covariate statistics, $s_{ik}(x, \mathbf{Z})$, for $k = 1,\dots, K$. Given the ego node, $i$, there are $n$ possible steps for the actor $i$ to take: either one of all current ties $x_{ij} = 1$ will be destroyed, a new tie will be created, or no change will occur. 

The parameters, $\boldsymbol{\beta}$, are attached to various actor-level network statistics, $s_{ik}(x)$. There are always at least two parameters, $\beta_1$ for the outdegree of a node, and $\beta_2$ for the number of reciprocal ties held by a node \citep[p. 371]{snijders01}. There are many possible parameters $\beta_i$ to add to the model. They can be split up into two groups: first, the structural effects, which only depend on the structure of the network. The inclusion of these effects has origin in the classical exponential random graph model (ERGM) for networks. These effects are written in terms of the edge variables $x_{ij}$, for $i \neq j$. The second set of effects are the actor-level or covariate effects. These effects also depend on the structure of the network. They are written in terms of $x_{ij}$ but also in terms of the covariates, $\mathbf{Z}$. A table of some possible structural and covariate effects is given in Table~\ref{tab:effects}. For a complete list of the network and covariate statistics that can be included in the objective function, see \citet{RSiena}.

\begin{table}
\centering
\begin{tabular}{m{4.5cm}m{4.5cm}m{3cm}}
\multicolumn{3}{l}{\textbf{Structural Effects}} \\ \hline\hline
outdegree  & $s_{i1}(x) = \sum_j x_{ij}$ & \includegraphics[width=2cm]{img/outdegree}\\ \hline
reciprocity  & $s_{i2}(x) = \sum_j x_{ij}x_{ji}$ & \includegraphics[width=2cm]{img/reciprocity}\\ \hline
transitive triplets  & $s_{i3}(x) = \sum_{j,h} x_{ij}x_{jh}x_{ih}$ & \includegraphics[width=1.5cm]{img/transtrip}  \\
\\
\multicolumn{3}{l}{\textbf{Covariate Effects}}\\  \hline \hline
covariate-alter  & $s_{i4}(x) = \sum_j x_{ij}z_j$ & \includegraphics[width=3cm]{img/covaralter} \\  \hline
covariate-ego  & $s_{i5}(x) = z_i\sum_j x_{ij}$ & \includegraphics[width=3cm]{img/covarego} \\  \hline
same covariate & $s_{i6}(x) = \sum_j x_{ij} \mathbb{I}(z_i = z_j)$ & \includegraphics[width=3cm]{img/covarsim}%\\
%jumping transitive triplets  & $s_{i7}(x) = \sum_{j \neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(z_i = z_h \neq z_j)$
\end{tabular}
\caption{\label{tab:effects} Some of the possible effects to be included in the stochastic actor-oriented models in RSiena. There are many more possible effects, but we only consider a select few here. For a complete list, see the RSiena manual \citep{RSiena}.}
\end{table}

When node $i$ is given the chance to change a tie, we assume that they wish to maximize the value of their objective function $f_i(\boldsymbol{\beta}, x)$ plus a random element, $U_i(x)$, where the $U_i(x)$ are from "the type 1 extreme value distribution (or Gumbel distribution) with mean 0 and scale parameter 1" \citep[p. 368]{snijders01}. This distribution, which is also known as the log-Weibull distribution, has probability distribution function, using $\mu$ for the mean parameter and $\sigma$ for the scale parameter, of

$$f(u|\mu, \sigma) = \frac{1}{\sigma}\exp\left\{-\left(\frac{u-\mu}{\sigma} + e^{-\frac{u-\mu}{\sigma}}\right)\right\}.$$

Using this distribution is convenient because it allows the probablity the actor $i$ chooses to change its tie to actor $j$ in terms of the objective function alone. Let $p_{ij}(\boldsymbol{\beta}, x)$ be this probability. Next, write the network $x$ in its potential future state, where the tie $x_{ij}$ has changed to $1-x_{ij}$, as $x(i \leadsto j)$. Then, the probility that the tie $x_{ij}$ changes is 

$$p_{ij}(\boldsymbol{\beta}, x) = \dfrac{\exp\left\{f_i(\boldsymbol{\beta}, x(i\leadsto j))\right\}}{\sum_{h \neq i} \exp\left\{f_i(\boldsymbol{\beta}, x(i \leadsto h))\right\}}$$

%### A SAOM as a CTMC {#saomctmc}

%In order to fit this model definition back into the original context of the CTMC described in Section \@ref(dynamicnets), it must be written in terms of its intensity matrix, $\mathbf{Q}$.  This matrix describes the rate of change between states of the process. For networks, there are a very large number of possible states, $2^{n(n-1)}$, so the intensity matrix is a square matrix of that dimension. But, thanks to the property of SAOMs that the states are allowed to change only one tie at a time, there are only $n$ possible states given the current state, $n-1$ of which are uniquely determined by the node $i$ that is given the opportunity to change. Thus, the intensity matrix $\mathbf{Q}$ is very sparse, with only $n(n-1) + 1$ non-zero entries in each row. Note that $n(n-1)$ of these represent the possible states that are one edge different from a given state, and the additional non-zero entry is for the state to remain the same. All other entries in a row are zero because those column states cannot be reached from the row state by just one change as dictated by the SAOM. The entries of $\mathbf{Q}$ are defined as follows: let $b \neq c \in \{1, 2, \dots, 2^{n(n-1)} \}$ be indices of two different possible states of the network, $x^b, x^c \in \mathcal{X}$.  Then the $bc^{th}$ entry of $Q$ is:
% \[ q(x^b, x^c) = \begin{cases} 
%       q_{ij}(\alpha, \rho, \boldsymbol{\beta}, x^b) = \lambda_i(\alpha, \rho, x^b, m)p_{ij}(\boldsymbol{\beta}, x^b) & \text{if } x^c \in \{x^b(i \leadsto j) | \text{ any } i \neq j \in \mathcal{N}\} \\
%       0 & \text{if } x^c \text{ differs from } x^b \text{ by more than 1 tie} \\
%       -\sum_{i\neq j} q_{ij}(\alpha, \rho, \boldsymbol{\beta}, x^b)  & \text{if } x^b = x^c 
%    \end{cases}
% \]
% 
% Thus, the rate of change between any two states that differ by only one tie, $x_{ij}$, is the product of the rate at which actor $i$ gets to change a tie and the probability that the tie that will change is the tie to node $j$.^[Just to be clear, the change is from $x^b_{ij}$ to $x^c_{ij} = 1 - x^b_{ij}$.] Furthermore, the theory of continuous time Markov chains gives that the matrix of transition probabilities between observation times $t_{m-1}$ and $t_{m}$ is dependent only on the difference between timepoints, $t_m - t_{m-1}$. Following the same definition for transition probabilities in Section \@ref(dynamicnets), the matrix of transition probabilities is 
% $$e^{(t_m - t_{m-1})\mathbf{Q}},$$ 
% where $\mathbf{Q}$ is the matrix defined above and $e^X$ for a real or complex square matrix $X$ is equal to $\sum_{k=0}^{\infty} \frac{1}{k!} X^k$.

\subsection{Fitting Models to Data}

To fit a SAOM to longitudinal network data, we use the package \texttt{RSiena} \citep{RSiena}. This package uses simulation methods to estimate parameter values using either the method of moments or maximum likelihood estimation. In this paper, we focus of the method of moments estimation because the theory behind it was established in \citet{saompaper}, while the maximum likelihood estimation methods were not fully established until \citet{saomsml2010}. We also focus on the score function method for estimating the derivatives of the expected values, as opposed to the finite differences method, both of which are outlined in detail in \citet{SienaAlgs}.

\par For the score function method, the SIENA software uses a Robbins-Monro algorithm (see \citet{robbinsmonro}) to estimate the solution of the moment equation
$$ E_{\theta}S = s_{obs}$$
where $\theta$ is the vector of rate and objective function parameters, and $s_{obs}$ is the observed vector of model statistics, $S$. The entire algorithm is provided in \citet{SienaAlgs}. \par There are three phases in the the SIENA algorithm, as described in \citet{RSienamanual, SienaAlgs}. The first phase performs initial estimation of the score functions used in the Robbins-Monro procedure. The second phase carries out the Robbins-Monro algorithm and obtains iterative estimates of the parameter values. The third phase uses the parameter vector estimated in phase two to estimate the score functions and the covariance matrix of the parameters, as well as performs convergence checks. In each of the the first two phases, the estimation procedure also relies on ``microsteps" that simulate from the model as it exists in its current state in order to update either the score functions or the parameter estimates. These microsteps become observed instances of the continuous-time Markov chain that is the backbone of the stochastic actor-oriented model. In Section~\ref{sec:ModelVis}, we visualize the parts of the SIENA method-of-moments algorithm, bringing them out of the ``black-box" and into the light. 

\subsection{Model Goodness-of-Fit}

The \texttt{RSiena} software that fits the models to data also comes equipped with the \texttt{sienaGOF()} function for examining model fit. This function ``assess[es] the fit of the model with respect to auxiliary statistics of networks" \citep[p.~53]{RSienamanual}. Examples of the auxiliary statistics are the out- or indegree distribution on the nodes, with the option for users to input their own statistics to examine. The fit is tested using Mahalanobis distance to determine a $p$-value when comparing the statistics calculated from the observed data (excluding the first observation that is conditioned on) to the distribution of the statistics when calculated on the simulated waves. So, good fit is acheived when $p$ is large. XX more detail about mahalanobis distance and calculating p-value XX

The \texttt{plot.sienaGOF()} function allows us to visualize this fit. Below is an example of what the goodness-of-fit plot output looks like for indegree and outdegree statistics for a small model. Box plots and violin plots are drawn on top of each other in Figure~\ref{fig:sienagof} in order to show the distribution of the simulated network observations compared to the true data shown in red points connected by red lines. If the red points lie well within the simulated values, the model is a good fit to the data. The model examined in Figure~\ref{fig:sienagof} appears to do a better job of capturing the outdegree distribution than the indegree distribution of the data. 

<<sienagof, fig.cap="Goodness of fit measures for two network statistics: indegree and outdegree cumulative distributions. On the x-axis is the degree value, and on the y-axis is the number of times that degree value occurs in the data.", cache = TRUE, message = FALSE, warning = FALSE>>=
load("data/ansnullpaper.rda")
library(lattice)
gof1 <- sienaGOF(ansnull, OutdegreeDistribution, varName ="friendship")
gof2 <- sienaGOF(ansnull, IndegreeDistribution, varName ="friendship")
p1 <- plot(gof1)
p2 <- plot(gof2)
gridExtra::grid.arrange(p1, p2)
@

\subsection{Example Data}

To guide our visual exploration of stochastic actor-oriented models, we used a subset of the 50 actor dataset from the ``Teenage Friends and Lifestyle Study" that is provided on the \texttt{RSiena} webpage. These data come from \citet{friendsdata}, and we chose to only work with a subset of the data to make network visualizations less busy, with changes that are more noticeable. The subset contained actors 20 through 35 and the ties between them, as well as the drinking behavior of each actor at each of the three waves. This specific subset was chosen because it showed somewhat higher connectivity than other subsets, as we've emphasized in the visualizations of the three network adjacency matrices in Figure~\ref{fig:adjmatviz}. For model fitting, we condition on wave 1 and estimate the parameters of the models from the second and third waves. We will also be working with one actor level categorical covariate, drinking behavior. This variable has five values in the original data: (1) does not drink, (2) drinks once or twice a year, (3) drinks once a month, (4) drinks once a week, and (5) drinks more than once a week. The actor covariates and networks are visualized in Figures~\ref{fig:adjmatviz} and~\ref{fig:alldataviz}. 
<<adjmatviz, fig.align='center', fig.cap="A visualization of the adjacency matrices of the three waves of network observations in the ``Teenage Friends and Lifestyle Study'' data. The subset we will be using is outlined in red.", fig.width=8>>=
source("code/00e_small_friends.R")
view_rs2$wave <- factor(view_rs2$wave)
levels(view_rs2$wave) <- c("Wave 1", "Wave 2", "Wave 3")
ggplot(data= view_rs2, aes(x = x, y=y)) + 
  geom_tile(aes(fill = as.factor(value))) + 
  scale_fill_manual("i likes j", labels=c("no", "yes"), 
                    values = c('white', 'black')) +
  geom_rect(data= NULL, inherit.aes = FALSE, color = 'red',
            aes(xmin = 19.5, xmax = 35.5, ymin = 19.5, ymax = 35.5, fill =NA)) + 
  facet_wrap(~wave) + theme_bw() +
  labs(x = "i", y = "j") + 
  theme(aspect.ratio=1, axis.text = element_blank(), axis.ticks = element_blank()) 
@

<<alldataviz, fig.cap="The smaller friendship network data we will be modelling throughout the paper.", fig.width=8>>=
actual1 <- merge(data.frame(as.edgelist(as.network(fd2.w1))), 
                 data.frame(id = 1:16, drink = drink2[,1]),
                 by.x = "X1", by.y = "id", all = T)
actual1$Wave <- 1 
actual2 <- merge(data.frame(as.edgelist(as.network(fd2.w2))),
                 data.frame(id = 1:16,drink = drink2[,2]),
                 by.x = "X1", by.y = "id", all = T)
actual2$Wave <- 2
actual3 <- merge(data.frame(as.edgelist(as.network(fd2.w3))),
                 data.frame(id = 1:16,drink = drink2[,3]),
                 by.x = "X1", by.y = "id", all = T)
actual3$Wave <- 3
alldat <- rbind(actual1, actual2, actual3)
#alldat$X1 <- paste0("V", alldat$X1)
alldat$X1 <- as.factor(alldat$X1)
#alldat$X2 <- ifelse(!is.na(alldat$X2), paste0("V", alldat$X2), NA)
alldat$X2 <- as.factor(alldat$X2)
#write_csv(alldat, "smallfriends4Geomnet.csv")
alldataviz <- ggplot(data = alldat, 
       aes(from_id = X1, to_id = X2, color = as.ordered(drink), group = as.ordered(drink))) +
  geom_net(fiteach = FALSE, directed=TRUE, layout.alg = 'fruchtermanreingold', size = 4, arrowsize = .25, arrowgap = .03, ecolour = 'grey40', labelon=T, labelcolour = 'black', fontsize = 3, vjust = .5) +
  scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
  theme_net() +
  facet_wrap(~Wave, labeller = "label_both") + 
  theme(legend.position = 'bottom', panel.background = element_rect(fill = NA, color = "grey30"))
alldataviz
@

\section{Model Visualizations} \label{sec:ModelVis}

The concept of model visualization was developed to complement traditional model diagnostic tools. Typically, numerical summaries such as $R^2$ are used to assess model fit, and visualizations, like residual plots, are restricted to the data. Every good data analysis includes both numerical and visual summaries of the data, so why should model description be restricted to numerical summaries? \citeauthor{modelvis} outline the three different parts of a model: the model family, the model form, and the fitted model. The latter is primarily what one thinks of first when considering a model in a data analysis, where a model is fit to data, and parameter estimates are reported. In the context of SAOMs, the fitted model contains the estimated rate parameters and the estimated objective function parameters. The model form describes the the model before the fitting process and how variables describe the data in the model. In SAOMs, the model form includes description of the rate and objective functions and the variables therein that describe how the network evolves over time. Finally, the model family is the most broad description of the model. This is the type of model that you wish to fit to the data, and is chosen based on the problem and knowledge at had. For example, we chose to use a SAOM to model network data over an exponential random graph model (ERGM) because we believe that actor-level variables effect network dynamics. These three parts of a "model" are should be visualized according to the three principals of model visualization: display the model in the data space, look at a collection of models, and explore the process of fitting the model, not just the end result. 

\subsection{Explore algorithms, not just end result}

The model fitting process in \texttt{RSiena} involves several simulation steps that are hidden from the user. This is practical and efficient if a researcher is primarily interested in fitting one model to a set of longitudinal network data and obtaining parameter estimates. We are more interested in how the models are fit, so we extracted and explored the different steps of that process. 

A key component of each step of the SIENA method of moments algorithm is the ``microstep." This step simulates from the model in its current state, $x(t_m)$ with current parameter values $\theta_{0}$, to the next state, $x(t_{m+1})$. This microstep process stops when the simulated network has acheived the same number of differences, $C$, from $x(t_m)$ as $x(t_{m+1})$, where 
$$C = \sum_{i \neq j} |x_{ij}(t_{m+1}) - x_{ij}(t_{m})|.$$ 
This simulation process follows the steps of the continuous-time Markov chain. First, an ``ego node'' is selected to make a change, and then it randomly makes one change in its ties according to the probability, $p_{ij}$, determined by its objective function. The options for change are (1) removing a current tie, (2) adding a new tie, or (3) making no change at all. Saving all of these steps is not efficient if you're only interested in estimating parameter values, but they can be saved and extracted using options in \texttt{RSiena}, which is exactly what we did to create our visualizations. Between two network observations $x(t_m)$ and $x(t_{m+1})$, there can be dozens, hundreds, or even thousands of microsteps, depending on the size of the network. We wanted to see these in-between steps in order to better understand the behavior of the underlying continuous-time Markov chain. 

The first vizualization we present here is an animation of microsteps simulated to emulate the transition steps from wave 1 to wave 2 the subset of the friends data shown in Figure ~\ref{fig:alldataviz}. Movies similar to our animation were used to visualize the changes of dynamic networks in \citet{dynnetviz}. When each ego node is selected in a microstep, it is emphasized in the animation, then the associated edge either appears or disappears. If there are no changes at a particular microstep, no changes are seen. Some frames of the animations are shown in Figure~\ref{fig:makinggif}, and the full movie can be viewed at INSERT LINK TO GIF. 

<<makinggif, fig.cap='A selection of images in the microstep animation. The selected edges and nodes are emphasized by changing the size and the color, then when edges are removed, they fade out, shrinking in size, while the nodes change color and shrink to blend in with the other nodes.', fig.width=8, cache=TRUE>>=
friend.data.w1 <- as.matrix(read.table("data/s50_data/s50-network1.dat"))
friend.data.w2 <- as.matrix(read.table("data/s50_data/s50-network2.dat"))
friend.data.w3 <- as.matrix(read.table("data/s50_data/s50-network3.dat"))
drink <- as.matrix(read.table("data/s50_data/s50-alcohol.dat"))
fd2.w1 <- friend.data.w1[20:35,20:35]
fd2.w2 <- friend.data.w2[20:35,20:35]
fd2.w3 <- friend.data.w3[20:35,20:35]
friendshipData <- array(c(fd2.w1, fd2.w2,fd2.w3), dim = c(16, 16, 3))

# running Siena
#friendship <- sienaDependent(friendshipData)
#alcohol <- varCovar(drink[20:35,])
#mydata <- sienaDataCreate(friendship, alcohol)
#myeffnull <- getEffects(mydata)
#myalgorithm <- sienaAlgorithmCreate(projname = 's16_3')
set.seed(823746)
ansnullchains <- get_chain_info(ansnull)
ansnullchains %>%
  filter(period == 1) %>%  #only look at chains from wave 1 to wave 2
  group_by(rep) %>%
  dplyr::select(rep, from = ego, to = alter) %>%
  mutate(val = as.numeric(!from == to),
         from = paste0("V", parse_number(from)+1), # make the chains
         to = paste0("V", parse_number(to)+1)) -> ansnullchainsw1w2
colnames(fd2.w1) <- paste0("V", 1:16)
rownames(fd2.w1) <- paste0("V", 1:16)
wave1friends <- fortify(as.adjmat(fd2.w1))
ms1 <- listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 1))
microsteps <- ms1[1:20]

microsteps_df <- getMicrostepsDF(microsteps)
pte <- pretween_edges(microsteps = microsteps)
ptv <- pretween_vertices(pte = pte, layoutparams = list(n = 16))
# step 1 is remove, step 9 is add. 
# get tweened data for all microsteps
pall <- tween_microsteps(ptv, pte, microsteps_df)

static_pall <- filter(pall, .frame %in% c(15,20,22,24,26:28, 71:73, 75, 79, 81, 84))

static_pall$type <- NA
static_pall$type[which(static_pall$.frame %in% c(15,20,22,24,26:28))] <- "Remove Edge"
static_pall$type[which(static_pall$.frame %in% c(71:73, 75, 79, 81, 84))] <- "Add Edge"

set.seed(34569234)
create_net_animate(static_pall) + 
  facet_wrap(~.frame, nrow=2, labeller = 'label_both') + 
  theme(panel.background = element_rect(color = 'black'))
@

The top row of Figure~\ref{makinggif} shows an edge being removed, and the bottom row shows one being added. In both cases, the ego nodes chosen to act change color from black to red, and they also increase greatly in size. In the case of an edge being removed, the edge that currently exists is emphasized with the color and size change that the node also gets, and as the animation proceeds the edge shrinks to disappearing, while the node shrinks and changes color back to the original black. If the edge is being added, it appears colored red from nothing and grows to a large size, then changes color to match the rest of the edges, while the node shrinks and changes color to match the other nodes.  

We also use animation to view the changing structure of the adjacency matrix the microsteps. The adjacency matrices for the three waves of friendship data as shown in Figure~\ref{fig:alldataviz} are ordered by node id. There are 16 nodes in the data, numbered 1-16, and that order is used on the $x$ and $y$ axes for the matrix visualization. Viewing the adjacency matrices with this arbitrary ordering does not provide much information to the viewer about the underlying structure of the network. This lack of perceived structure would be exacerbated in an animation. So, we adjust the ordering so that the viewer can perceive more of the structure of the graph. This process is known as matrix seriation \citep{seriation}. 

To reorder the vertices for the matrix visualization, we first constructed a cumulative adjaceny matrix, $\mathbf{A}^{cum}$, for the series of microsteps simulating the network from $x(t_m)$ to $x(t_{m+1})$. A single entry in the cumulative adjacency matrix, $\mathbf{A}^{cum}_{ij}$, is the total number of times the edge from node $i$ to node $j$ appears in the network from the initial observation, $x(t_m) \equiv X(0)$ to the final reult of the last microstep, $X(C)$: 
$$ \mathbf{A}^{cum}_{ij} = \sum_{c = 0}^C X_{ij}(c). $$
We then performed a principal component analysis on $\mathbf{A}^{cum}$, and used the values of the first principal component to order the vertices on the $x$ and $y$ axes for the adjacency matrix animation. For one such series of microsteps simulated by \texttt{RSiena}, we present the adjacency matrix ordered by the (arbitrary) vertex id alongside the seriated adjacency microsteps using the first principal component loading on the cumulative adjacency matrix, $\mathbf{A}^{cum}$, in Figure~\ref{adjmatorder}. 

<<adjmatorder, fig.cap="On the left, the starting friendship network represented in adjacency matrix form, ordered by vertex id. On the right, the same adjacency matrix is presented after ordering the vertices by one repitition of the microstep simulation process from wave one to wave two.", fig.width=8>>=
# first present the original wave 1 adj mat, then present the adj mat after being ordered by the 1st pc of the big adjmat A. 
numordersf <- data.frame(fd2.w1) %>% 
  mutate(from = rownames(fd2.w1)) %>% 
  gather(key = to ,value = value, -from)
pcaordersf <- numordersf

numordersf$from <- factor(numordersf$from, levels = paste0("V", 1:16))
numordersf$to <- factor(numordersf$to, levels = paste0("V", 1:16))

p1 <- ggplot(data = numordersf, aes(x = from, y = to)) + geom_tile(color = 'black', aes(fill = as.factor(value))) + 
  scale_fill_manual(values = c('white', 'black')) +
  theme(legend.position = 'none', axis.text = element_text(size = 8)) + 
  coord_fixed()

sfmsall <- read_csv("data/smallfriendsmicrosteps.csv")

sfmsall %>% 
  group_by(from, to) %>% 
  filter(rep == 1) %>% 
  summarise(count = n()) -> sfmsall_summ
sfmsall_summ$from <- factor(sfmsall_summ$from, levels = paste0('V', 1:16))
sfmsall_summ$to <- factor(sfmsall_summ$to, levels = paste0('V', 1:16))
sfmsall_summ %>% 
  spread(to, count, fill = 0) %>% 
  data.frame -> allmsrep1_adjmat
allmsrep1_adjmat$V16 <- 0
allmsrep1_adjmat <- allmsrep1_adjmat[,-1]
allmsrep1_adjmat <- as.matrix(allmsrep1_adjmat)
rownames(allmsrep1_adjmat) <- paste0("V", 1:16)

pca1 <- prcomp(allmsrep1_adjmat)
order1 <- names(sort(pca1$rotation[,1], decreasing = T))
order2 <- names(sort(abs(pca1$rotation[,1]), decreasing = T))

pcaordersf$from <- factor(pcaordersf$from, levels = order1)
pcaordersf$to <- factor(pcaordersf$to, levels = order1)

p2 <- ggplot(data = pcaordersf, aes(x = from, y = to)) + geom_tile(color = 'black', aes(fill = as.factor(value))) + 
  scale_fill_manual(values = c('white', 'black')) +
  theme(legend.position = 'none', axis.text = element_text(size = 8)) + 
  coord_fixed()

grid.arrange(p1,p2, nrow = 1)
@

Using the principal component analysis on $\mathbf{A}^{cum}$ to order the rows and columns of the adjacency matrix visualization clearly shows the two distinct connected components in the first wave of the network, which are difficult to find in the arbitrarily ordered visualization. 

We also attempt to better understand the microstep process by visualizing all transition probabilites for the first microstep in the process. We only do the first step of many for now because the \texttt{RSiena} transition probabilities after the first step are only comparable for identical steps due to the conditioning on the current network state in the model. Thus we have one step for each of the 1,000 repititions to visualize. In Figure ~\ref{histogramalters}, we present each acting node, (ego node) and the resulting probabilities of change. The probability shown by the bars is the theoretical probability according to the objective function of the ego node changing the tie to the alter node. The probability shown by the points is the empirical probability of that change being made, calculated by counting all instances of the ego node first, then computing the proprotion of each change. In most cases, they are almost identical, showing that the algorithm is performing as expected. 

<<histogramalters, fig.width = 8, fig.height=11, fig.cap="Each panel shows the theoretical (as bars) and empirical (as points) probabilities of the chosen ego node changing its tie to each of the other nodes.">>=
ansnullchains %>% group_by(period, rep) %>% 
  mutate(ms = row_number()) %>% 
  filter(ms == 1, period == 1) %>% 
  mutate(alterProb = exp(logAlterProb),
         ego = as.numeric(as.character(ego)),
         alter = as.numeric(as.character(alter))) %>% 
  ungroup() %>% 
  group_by(ego, alter) %>% 
  summarize(count = n(), alterProb = unique(alterProb)) %>% ungroup() %>% 
  group_by(ego) %>% 
  mutate(csum = cumsum(alterProb)) %>% 
  arrange(ego, csum) %>% 
  mutate(totego = sum(count), 
         empprob = count / totego) -> cumprobs 
cumprobs$ego <- factor(paste0("V",cumprobs$ego+1), levels = paste0("V", 1:16))
cumprobs$alter <- factor(paste0("V",cumprobs$alter+1), levels = paste0("V", 1:16))

# is the tie that's added new, removed, or does the network stay the same 

changes <- cumprobs %>% dplyr::select(ego, alter) %>% unique 

wave1friends2 <- wave1friends[,-3]
wave1friends2$type <- "remove"
changes2 <- left_join(changes, wave1friends2, c("ego" = "from", "alter" = "to"))

changes2$type[is.na(changes2$type)] <- ifelse(changes2$ego[is.na(changes2$type)] == changes2$alter[is.na(changes2$type)], "noChange", "add")

cumprobs %>% left_join(changes2) %>% ggplot() + 
  geom_histogram(
    aes(x = alter, weight = alterProb, fill = type), 
    color = 'black', fill = 'white', binwidth = 1) + 
  geom_point(aes(x = alter, y = empprob), color = 'red') + 
  scale_x_continuous(labels = paste0("V",1:16), breaks = 0:15) + coord_flip() + facet_wrap(~ego, labeller = "label_both") + theme_gray()
# 
# cumprobs %>% ggplot() + geom_histogram(aes(x = alter, weight = count), color = 'black', fill = 'white', binwidth = 1) + scale_x_continuous(labels = paste0("V",1:16), breaks = 0:15) + coord_flip() + facet_wrap(~ego, labeller = "label_both") + theme_gray()

# cprobslist <- split(cumprobs, cumprobs$ego)
# plots <- list()
# for(i in 1:length(cprobslist)){
#   cprobslist[[i]] %>%
#   ggplot() + 
#   geom_histogram(aes(x = as.numeric(as.character(alter)),
#                      y = alterProb), stat = 'identity', fill = 'white', color = 'black') + 
#   geom_line(aes(x = as.numeric(as.character(alter)), 
#                 y = csum)) + 
#   geom_point(aes(x = as.numeric(as.character(alter)), 
#                 y = csum)) +
#   scale_x_continuous(limits = c(-.5, 15), breaks = 0:15, labels = 0:15) +   
#   geom_hline(yintercept = 1, color = 'red', linetype = 'dashed') + 
#   labs(x = "alter", y = expression(p[ij]), title = paste("Ego node:", names(cprobslist)[i])) + # ylab(expression(p[ij])) + 
#   theme_gray() + 
#     theme(plot.title = element_text(size = 10), 
#           axis.title = element_text(size = 8), 
#           plot.margin = unit(rep(0,4),"pt")) -> p
#   plots[[i]] <- p
# }
# grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]],
#              plots[[5]], plots[[6]], plots[[7]], plots[[8]],
#              #plots[[9]], plots[[10]], plots[[11]], plots[[12]],
#              #plots[[13]], plots[[14]], plots[[15]], plots[[16]], 
#              ncol = 2, padding = unit(0, 'line'))
@

Another way to view the transition probabilities is through the adjacency matrix visualization. In Figure~\ref{fig:heatmap}, we build on the concept of the ordered adjacency matrix of Figure~\ref{fig:adjmatorder}. This heatmap shows the transition probabilities of all ties that are changed in the first microstep of the 1,000 simulations. The heatmap is noticeably sparse, showing the lack of coverage in the model.  Of the 256 possible steps, only 81, or about 32\%, are taken in the 1,000 simulated chains. XXX Check this again.... XXX
<<heatmap>>=
ansnullchains %>% 
  filter(rep == 1, period == 1) %>% 
  mutate(ego = paste0("V",as.numeric(as.character(ego))+1),
         alter = paste0("V",as.numeric(as.character(alter))+1), 
         prob = exp(logAlterProb)) %>% 
  ggplot(aes(x = factor(ego, levels = order1), 
             y = factor(alter, levels = rev(order1)), 
             fill = prob)) + 
  geom_tile(color = 'black') + 
  scale_fill_gradient(low = 'white', high = 'blue', limits = c(0,1)) +
  theme(panel.background = element_rect(fill = 'white')) + 
  labs(x = "ego", y = "alter")+
  coord_fixed()
@

We also wanted to view the complete microstep process from the first wave, which we condition on, to the second wave. The number of steps taken from wave 1 to wave 2 varies. In the simulations from Model 1, the smallest number of steps taken was 58, and the longest was 248, with a mean of 106 and a median of 103. The standard deviation of the 1000 simulations' number of microsteps was 22.8. In Figure~\ref{fig:allsteps}, we see three simulations of the process from wave 1 to wave 2, with wave 1 shown on the left, and wave 2 shown on the right. In each of the three plots, the $y$-axis contains the edges sorted by how often they appear in the networks along the way. We can see that some edges are there in the beginning, but disappear and never come back, while others appear a few steps in, only to dispappear again. There are also some edges that exist in wave 2 that don't appear at all in the microstep process in a given repitition. 

<<allsteps, fig.cap="Three repetitions of the microstep process from wave 1 to wave 2. The $x$ axis is the microstep number, with step 0 representing the first wave of data and the final step representing the second wave of data.">>=
smallfriends <- read_csv("data/smallfriends4Geomnet.csv")
names(smallfriends)[1:2] <- c("from", "to")
msall1 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 1))
msall1_df <- plyr::rbind.fill(lapply(X = seq_along(msall1), FUN = function(i, x) {
        x[[i]]$ms <- i - 1
        return(x[[i]])
    }, msall1))
msall1_df <- msall1_df %>% dplyr::select(from, to, ms)
wave2 <- smallfriends %>% filter(Wave == 2) %>% na.omit %>% data.frame()
msall1_df <- msall1_df %>% add_row(from = paste0("V",wave2$from),
                      to = paste0("V",wave2$to), 
                      ms = max(msall1_df$ms)+1)
edges <- msall1_df %>% 
  group_by(from, to) %>%
  summarize(edgeappears = n()) %>% 
  arrange(desc(edgeappears)) %>% ungroup() %>%
  mutate(eid = row_number())

msall1_df2 <- left_join(msall1_df, edges, by = c("from" = "from", "to" = "to"))

p1 <- ggplot(data = msall1_df2) + 
  geom_tile(aes(x = ms, y = -eid, fill = ms)) + 
  geom_vline(xintercept = c(0.5,81.5)) + 
  scale_fill_continuous(low = "red", high = "blue") + 
  theme(legend.position = 'none', axis.ticks.y = element_blank(),
        axis.text.y = element_blank(), axis.title = element_blank()) +
  labs(x = "Microstep")


# rep 2 
msall2 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 2))
msall2_df <- plyr::rbind.fill(lapply(X = seq_along(msall2), FUN = function(i, x) {
        x[[i]]$ms <- i - 1
        return(x[[i]])
    }, msall2))
msall2_df <- msall2_df %>% dplyr::select(from, to, ms)
msall2_df <- msall2_df %>% add_row(from = paste0("V",wave2$from),
                      to = paste0("V",wave2$to), 
                      ms = max(msall2_df$ms)+1)

edges2 <- msall2_df %>% 
  group_by(from, to) %>%
  summarize(edgeappears = n()) %>% 
  arrange(desc(edgeappears)) %>% ungroup() %>%
  mutate(eid = row_number())

msall2_df2 <- left_join(msall2_df, edges2, by = c("from" = "from", "to" = "to"))

p2 <- ggplot(data = msall2_df2) + 
  geom_tile(aes(x = ms, y = -eid, fill = ms)) + 
  geom_vline(xintercept = c(0.5,49.5)) + 
  scale_fill_continuous(low = "red", high = "blue") +
  theme(legend.position = 'none', axis.ticks.y = element_blank(),
        axis.text.y = element_blank(), axis.title = element_blank()) #+
  #labs(x = "Microstep", y = "Edges (most frequent at top)")

# rep 3 
msall3 <- netvizinf::listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 3))
msall3_df <- plyr::rbind.fill(lapply(X = seq_along(msall3), FUN = function(i, x) {
        x[[i]]$ms <- i - 1
        return(x[[i]])
    }, msall3))
msall3_df <- msall3_df %>% dplyr::select(from, to, ms)
msall3_df <- msall3_df %>% add_row(from = paste0("V",wave2$from),
                      to = paste0("V",wave2$to), 
                      ms = max(msall3_df$ms)+1)

edges3 <- msall3_df %>% 
  group_by(from, to) %>%
  summarize(edgeappears = n()) %>% 
  arrange(desc(edgeappears)) %>% ungroup() %>%
  mutate(eid = row_number())

msall3_df2 <- left_join(msall3_df, edges3, by = c("from" = "from", "to" = "to"))

p3 <- ggplot(data = msall3_df2) + 
  geom_tile(aes(x = ms, y = -eid, fill = ms)) + 
  geom_vline(xintercept = c(0.5,106.5)) + 
  scale_fill_continuous(low = "red", high = "blue") +
  theme(legend.position = 'none', axis.ticks.y = element_blank(),
        axis.text.y = element_blank(), axis.title = element_blank()) #+
  #labs(x = "Microstep", y = "Edges (most frequent at top)")

grid.arrange(p1, p2, p3, nrow = 3)
@

\st{The algorithms here are many!
\begin{enumerate}
%\item First, the microsteps / CTMC between each time point (gifs, other animations)
\item Phases of RSiena: phase 1 = initial parameter estimate
\item phase 2 = iterative update of parameters - trace plots
\item phase 3 = simulations for convergence diagnostics
\end{enumerate}
 } 

\hh{The visualizations should address most elements of structure described before, i.e. follow along in the setup (for descriptive and teaching purposes), the fitting (understanding the model and interpreting the results) and then diagnostics (how well does the model fit the data - where are the most differences to the actual data/what are the sensitive parameters?) }

\hh{Some of the visualizations we talked about
\begin{enumerate}
\item visualization of the MCMC process:  movie of individual actors' step-by-step decisions; could be done with the ndtv package, as long as we can get individual steps out of RSiena
\item global picture: 1000 simulated networks + alpha blending should give a pretty good idea of the most important edges
%\item global structure + principal components (tensor pca)
\end{enumerate}}

\subsection{View the model in the data space}

In \citeauthor{modelvis}, they define the \textit{data space} as ``the region over which we can reliably make inferences, usually a hypercube containing the data" \citep[p.~206]{modelvis}. What does this definition mean for network data? For dynamic social networks, there are a few different data ``spaces." First, we have the actors and their corresponding covariates. Second, we have the dyad variables that describe the relationships between the variables. Finally, we have the time over which the network is allowed to evolve. These three data pieces can be visualized together in various ways. The traditional node-link visualization uses one of many algorithms to layout the actors as points in 2D space, then draws segments connecting the points in 2D if the dyad between two nodes has value greater than zero, and draws nothing otherwise. The time aspect can be visualized by drawing each network observation in time and placing the observed timepoints side-by-side. 

One tool that can bring these three data spaces together in this way is the \texttt{R} package \texttt{geomnet} \citep{geomnet}. Different aesthesic aspects of each piece of the node-link diagram can be tied to the underlying node or edge data. The color, size, and shape of the points can be used to represent variables in the node data, and the color, size, and linetype of the links between points can be used to represent the edge variables. In a social network, node data might be age, gender, and occupation of the person in the network, and node data might be length of connection between two people, how the people first met (school, work, church, etc.), and how often they interact. Pulling all of this information together with \texttt{geomnet} allows the entire data space to be viewed at once. 

XXX INSERT AN EXAMPLE OF DATA WITH LOTS OF EDGE, NODE VARS OF INTEREST XXX 

These visual connections between the node, edge, and time data spaces are themselves part of the data space, because they can be viewed in different ways. Another way to connect the node and the edge data, as discussed in Section~\ref{sec:netvizintro}, is through the network's adjaceny matrix. The time dimension can also be incorporated with side-by-side adjacency matrix visualizations. 

Another way to view the model in the data space is through simulation from the model. Since no single simulation from a model is going to look like the data, it's better to visualize many simulations together. For network data, one way to do this is through the traditional node-link diagram. In Figure~\ref{fig:summnetM1}, we show a summary network created with 1000 simulations from Model 1. Only edges that appear in at least 51 of the 1000 simulations are included in the visualization, with edges that appear more frequently emphasized by thicker linewidths and a bright red color. On either side of the summary network, we show the actual data, wave 1 on the left, and wave 2 on the right. Comparing the three visualizations side-by-side shows us that the summary network of simulations of wave 2 looks more like the true wave 2 than wave 1, which is a good indication that the model is capturing the true data structure. It's not ideal, however, because there appear to be too many reciprocated ties in the summary network, far more than there are in the data. This could indicate that we should remove the reciprocity parameter from the model. 

<<summnetM1>>=
friendshipData <- array(c(fd2.w1, fd2.w2, fd2.w3), dim = c(16, 16, 3))
drink1 <- s50a[20:35,1]
drink2 <- s50a[20:35,2]
drink3 <- s50a[20:35,3]
friendship <- sienaDependent(friendshipData)
alcohol <- varCovar(cbind(drink1,drink2, drink3))
mydata <- sienaDataCreate(friendship, alcohol)
M1eff <- getEffects(mydata)
M2eff <- includeEffects(M1eff, jumpXTransTrip, interaction1 = "alcohol")
M3eff <- includeEffects(M1eff, nbrDist2twice)
means <- simu2 %>% group_by(Model, parameter) %>% summarize(
  means = mean(estimate)
)
M1parms <- (means %>% filter(Model == "M1"))$means
M2parms <- (means %>% filter(Model == "M2"))$means
M3parms <- (means %>% filter(Model == "M3"))$means
set.seed(4231352)
M1sims1000 <- saom_simulate(dat = mydata, 
                            struct = M1eff,
                       parms = as.numeric(M1parms), 
                       N = 1000)
M2sims1000 <- saom_simulate(dat = mydata, 
                            struct = M2eff,
                       parms = as.numeric(M2parms), 
                       N = 1000)
M3sims1000 <- saom_simulate(dat = mydata, 
                            struct = M3eff,
                       parms = as.numeric(M3parms), 
                       N = 1000)
M1simsdf <- sims_to_df(M1sims1000)
M2simsdf <- sims_to_df(M2sims1000)
M3simsdf <- sims_to_df(M3sims1000)
M1avgW2 <- M1simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M2avgW2 <- M2simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M3avgW2 <- M3simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))

# make a df of wave 2, wave 1, and the three averages and facet. 
names(actual1)[1:2] <- c("from", "to")
names(actual2)[1:2] <- c("from", "to")
actual1$count <- 1
actual1$weight <- 1
actual2$count <- 1
actual2$weight <- 1
actual1 <- actual1 %>% dplyr::select(from,to, count, weight)
actual2 <- actual2 %>% dplyr::select(from,to, count, weight)
actual1$cat <- "First Observation"
actual2$cat <- "Second Observation"

avgW2M1 <- M1avgW2 %>% ungroup() %>% 
  filter(weight > 50) %>% 
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model1")
add1 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M1$from), as.character(avgW2M1$to)))) 
avgW2M1 %>% add_row(from = add1, to = NA, count = NA, weight = NA, cat = "Model1") -> avgW2M1
avgW2M2 <- M2avgW2 %>% ungroup() %>% 
  filter(weight > 50) %>% 
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model2")
add2 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M2$from), as.character(avgW2M2$to)))) 
avgW2M2 %>% add_row(from = add2, to = NA, count = NA, weight = NA, cat = "Model2") -> avgW2M2
avgW2M3 <- M3avgW2 %>% ungroup() %>% 
  filter(weight > 50) %>% 
  mutate(from = as.factor(from), to = as.factor(to),
         cat = "Model3")
add3 <- setdiff(as.character(1:16), unique(c(as.character(avgW2M3$from), as.character(avgW2M3$to)))) 
avgW2M3 %>% add_row(from = add3, to = NA, count = NA, weight = NA, cat = "Model3") -> avgW2M3

combinedavgs <- rbind(actual1, actual2, avgW2M1, avgW2M2, avgW2M3)
combinedavgs %>% group_by(cat) %>% 
  mutate(linewidth = weight / max(weight,na.rm = T)) -> combinedavgs

combinedavgs %>% filter(cat == "Model1") %>% mutate(logweight = log(weight)) -> t1 
colors <- tweenr::tween_color(data = c("#969696", "#67000d"), n = 26, ease = 'linear')
t1 %>% arrange(logweight) -> t1
t1$color <- NA
t1$color[1:26] <- colors[[1]]

p1 <- ggplot(data = t1) + 
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth),
           directed = T, curvature = .2, labelon=T, size = 5,
           arrow = arrow(angle = 20, type = "open", length = unit(.1, "inches")), ecolour = t1$color,
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 3, arrowgap = .02,
           singletons = T) +
  theme_net() + labs(title = "Summary Network: M1") + theme(plot.background = element_rect(color = 'black'))
p2 <- ggplot(data = filter(combinedavgs, cat == "First Observation")) + 
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth),
           directed = T, curvature = .2, labelon=T, size = 5,
           arrow = arrow(angle = 20, type = "open", length = unit(.1, "inches")), ecolour = "#969696",
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 3, arrowgap = .02,
           singletons = T) +
  theme_net() + labs(title = "Wave 1") + theme(plot.background = element_rect(color = 'black'))
p3 <- ggplot(data = filter(combinedavgs, cat == "Second Observation")) + 
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth),
           directed = T, curvature = .2, labelon=T, size = 5,
           arrow = arrow(angle = 20, type = "open", length = unit(.1, "inches")), ecolour = "#969696",
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 3, arrowgap = .02,
           singletons = T) +
  theme_net() + labs(title = "Wave 2")  + theme(plot.background = element_rect(color = 'black'))
grid.arrange(p2, p1, p3, nrow = 1)
@

\st{Because of the multi-level data structure of a network, viewing the model in the data space can depend on which aspect of the model you are interested in viewing. XXX Ideas: summary statistic dist. on nets simulated from model and show statistic from data; interactive plots showing objective function values on nodes; select node, select other node, show how the function values change interactively (future); goodness of fit should do here. XXX} 



% \st{
% \begin{itemize}
% \item WHat is the model? the network model (SAOM model is just one of many)
% \item What is the data? Edge data, node data, observed time points, unobserved timepoints (CTMC)
% \item Note: specifically dealing with dynamic networks observed at discrete time points. 
% \end{itemize}
% What are some things that could go in here? need more data examples; not just the friends data.
% Harry Potter data??? I want to find my own data, too.}

\subsection{Collections are more informative than singletons}

The second principle of Wickham et al states that ``collections are more informative than singletons" \citep[p.~210]{modelvis}. They describe several different methods for producing such collections, but we focus on only a few here. We look at collections produced ``from exploring the space of all possible models," ``by varying model settings", ``by fitting the same model to different datasets", and ``by treating each iteration as a model" \citep[p.~210-11]{modelvis}. We also fit the same model many times, resulting in distributions of fitted parameters for one data set.

A classical problem with statistical network models generally is the lack of an ``average network" measure. Statisticians frequently rely on averages and expected values, but statistical network models, especially those as complex as SAOMs, lack an intuitive expected value measure. We could talk about expected values of parameters, but there is no way to talk about the expected value of the network based on the model. How then, can we arrive at an ``average" network? We propose to answer this question through visualization.

We first consider the 16 actor subset of the teenage friends and lifestyle data available on the \texttt{RSiena} website \citep{RSiena}. To this data, we fit three different SAOMs. Each SAOM used a simple rate function, $\alpha_m$, and an objective function with two or three parameters. The first model, $M1$, contains the absolute minimum number of parameters in the objective function $f_i(x)$:  $$f_i(x)^{M1} = \beta_1s_{i1} + \beta_2s_{i2}$$, where $s_{i1}$ is the density network statistic and $s_{i2}$ is the reciprocity network statistic for actor $i$. The second and third models, $M2$ and $M3$, contain a third parameter in the objective function that was determined in the\texttt{RSienaTest} software to be significant, with $p$-values less than 0.05 \citep{RSienaTest}. The $M2$ model contains an actor-level covariate parameter, and the $M3$ model contains an additional strutural effect in the objective function.
\begin{align*}
f_i(x)^{M2} & = \beta_1s_{i1} + \beta_2s_{i2} + \beta_3s_{i3} \\
f_i(x)^{M3} & = \beta_1s_{i1} + \beta_2s_{i2} + \beta_4s_{i4}, 
\end{align*} where $s_{i3} = \sum_{j \neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(z_i = z_h \neq z_j)$, and $s_{i4} =  |\{j : x_{ij} = 0, \sum\limits_h x_{ih}x_{hj} \geq 2\}|$. These statistics are known as the number of jumping transitive triplets and the number of doubly achieved distances two effect, respectively. The first statistic emphasizes triad relationships that are formed between actors from different covariate groups, while the other emphasizes indirect ties between actors. These two effects are visually represented in Figure~\ref{fig:jtt} and ~\ref{fig:dad}, respectively. 

\begin{figure}\label{fig:structures}
\begin{subfigure}[t]{.45\textwidth}
\caption{\label{fig:jtt}Realization of a jumping transitive triplet, where $i$ is the focal actor, $j$ is the target actor, and $h$ is the intermediary. The group of the actors is represented by the shape of the node.}
<<jtt, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center'>>=
jTTe <- data.frame(from = c('i', 'i', 'h'), to = c('h', 'j', 'j'))
jTTn <- data.frame(id = letters[8:10], group = c(1,1,2))

jTT <- merge(jTTe, jTTn, by.x = 'from', by.y = "id", all = T)

set.seed(12345) 
ggplot(data = jTT, aes(from_id = from, to_id = to)) + 
  geom_net(aes(shape = as.factor(group)), directed = T, labelon = T, 
           labelcolour='grey80',vjust = 0.5, hjust =0.5, arrowgap = .15, 
           colour = 'black', size=10, 
           ecolour = c("red", "grey40", "grey40", "grey40")) + 
  expand_limits(x=c(-0.1,1.1), y=c(-0.1,1.1)) +
  theme_net() +
  theme(legend.position = "none")
@
\end{subfigure}\hfill
\begin{subfigure}[t]{.45\textwidth}
\caption{\label{fig:dad}Doubly achieved distance between actors $i$ and $k$.}
<<dab, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center'>>=
dade <- data.frame(from = c('i', 'i', 'h', 'j'), to = c('h', 'j', 'k', 'k'))
dadn <- data.frame(id = letters[8:11], group = c(1,1,1,1))

dad <- merge(dade, dadn, by.x = 'from', by.y = "id", all = T)

set.seed(12345) 
ggplot(data = dad, aes(from_id = from, to_id = to)) + 
  geom_net(aes(shape = as.factor(group)), directed = T, labelon = T, labelcolour='grey80',vjust = 0.5, hjust =0.5, arrowgap = .15, colour = 'black', size=10) + 
  expand_limits(x=c(-0.1,1.1), y=c(-0.1,1.1)) +
  theme_net() +
  theme(legend.position = "none")
@
\end{subfigure}
\caption{Structural newtwork effects. On the left, a jumping transitive triplet (JTT). On the right, a doubly achieved distance between $i$ and $k$.}
\end{figure}
We these three models using \texttt{RSiena} 1,000 times each. We then looked at the distribution of the fitted values, which are shown in Figure~\ref{fig:distests}. We can see from these distributions that the inclusion of the jumping transitive triplet parameter, $\beta_3$ is obviously affecting the distributions of the other four parameters included in all models, $\alpha_1$, $\alpha_2$, $\beta_1$, and $\beta2$. In comparison with models M1 and M3, model M2 typically has higher estimates of the rate parameter, meaning that the inclusion of the covariate statistic in the model leads to higher estimates of the number of times, on average, a node gets to change its ties. This seems problematic: it is not clear that the addition of a parameter to the objective function \textit{should} effect the estimation of the rate parameters. 

To further investigate this relationship between the parameter values, we look at correlations between each of the parameter estimates in each model. In Figure~\ref{fig:corplots}, we examine correlations between each of pair of parameters within each model and overall.\nocite{ggally} The strongest correlation within each model is between $\beta_1$ and $\beta_2$, with absolute value of correlation between those two parameter values greater than 0.90 in all three models. The $\beta_1$ parameter is also highly correlated with the $\beta_3$ parameter within model M2, but it is not as highly correlated with the $\beta_4$ parameter in model M3. 

<<getdata, results='asis'>>=
nulls <- read.csv("data/distribution_null_model.csv")
alt <- read.csv("data/distribution_jumpTT_model.csv")
alt2nd <- read.csv("data/distribution_dblpairs_model.csv")
simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")
means <- simu2 %>% group_by(Model, parameter) %>% summarize(
  means = mean(estimate)
)
null_mod_sv <- as.numeric(nulls[,-c(1,6,7)] %>% summarise_each(funs(mean)))
alt_mod_sv <- as.numeric(alt[,-c(1, 7,8)] %>% summarise_each(funs(mean)))
alt_mod2_sv <- as.numeric(alt2nd[,-c(1,7,8)] %>% summarise_each(funs(mean)))
@

<<distests, fig.cap="Distribution of fitted parameter values for our three SAOMs. The inclusion of $\\beta_3$ or $\\beta_4$ clearly has an effect on the distribution of the rate parameters, $\\alpha_1$ and $\\alpha_2$.", fig.width=8>>=

#simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")

ggplot(data = simu2) + 
  geom_density(aes(x = estimate, fill = Model), alpha = .5) + 
  facet_wrap(~parameter, scales = 'free') + 
  theme_bw()
@


<<corplots, fig.cap="A matrix of plots demonstrating the strong correlations between parameter estimate in our SAOMs. The strongest correlation within each model is between $\\beta_1$ and $\\beta_2$.", fig.width=8>>=

simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")

simu_spread <- simu2 %>% spread(parameter, estimate)
ggpairs(simu_spread, columns = 3:8, ggplot2::aes(colour=Model, alpha = .5)) + theme_bw()
@

We also explore simulations from these different models given parameter values. Using the means from the 1,000 fitted parameter values, we simulated 1,000 observations from each of our three models. We condition on the first friendship network observation, and the second and third observations are simulated from the SAOM models estimated previously. From these simulations, we first create a visualization that represents an ``average" network. To do this, we count occurences of each possible edge in the simulations, resulting in a summary network with weighted edges representing the number of times an edge appeared in the simulated wave 2 when simulating from the SAOM 1000 times. In Figure~\ref{averagenet}, we show the first wave, the ``average'' network from the three models we fit, and the second wave. Comparing the three averages to waves 1 and 2, we see that they have very similar structure to wave 2. Model 2, which included the transitive triplet parameter, seems to have created more group strucutre overall than models 1 and 3. This could be an indication that, despite the highly significant nature of the parameter $\beta_3$ according to the $t$-tests in \texttt{RSiena}, the transitive triplet parameter should not be included because it does not result in a structure similar to the data. 


<<averagenet>>=
#friend.data.w1 <- as.matrix(read.table("s50_data/s50-network1.dat"))
#friend.data.w2 <- as.matrix(read.table("s50_data/s50-network2.dat"))
#friend.data.w3 <- as.matrix(read.table("s50_data/s50-network3.dat"))
#drink <- as.matrix(read.table("s50_data/s50-alcohol.dat"))
friendshipData <- array(c(fd2.w1, fd2.w2, fd2.w3), dim = c(16, 16, 3))
friendship <- sienaDependent(friendshipData)
alcohol <- varCovar(drink[20:35,])
mydata <- sienaDataCreate(friendship, alcohol)
M1eff <- getEffects(mydata)
M2eff <- includeEffects(M1eff, jumpXTransTrip, interaction1 = "alcohol")
M3eff <- includeEffects(M1eff, nbrDist2twice)
data(M1ests_bigfriends)
M1parms <- (means %>% filter(Model == "M1"))$means
M2parms <- (means %>% filter(Model == "M2"))$means
M3parms <- (means %>% filter(Model == "M3"))$means
M1sims1000 <- saom_simulate(dat = mydata, 
                            struct = M1eff,
                       parms = as.numeric(M1parms), 
                       N = 1000)
M2sims1000 <- saom_simulate(dat = mydata, 
                            struct = M2eff,
                       parms = as.numeric(M2parms), 
                       N = 1000)
M3sims1000 <- saom_simulate(dat = mydata, 
                            struct = M3eff,
                       parms = as.numeric(M3parms), 
                       N = 1000)
M1simsdf <- sims_to_df(M1sims1000)
M2simsdf <- sims_to_df(M2sims1000)
M3simsdf <- sims_to_df(M3sims1000)
M1avg <- M1simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M1avgW2 <- M1simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M1avg2 <- M1simsdf %>% filter(!is.na(to)) %>%
 group_by(from, to, wave) %>%
 summarise(count = n()) %>%
 mutate(weight = ifelse(from == to, 0, count))
M2avg <- M2simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M2avgW2 <- M2simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M2avg2 <- M2simsdf %>% filter(!is.na(to)) %>%
 group_by(from, to, wave) %>%
 summarise(count = n()) %>%
 mutate(weight = ifelse(from == to, 0, count))
M3avg <- M3simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M3avgW2 <- M3simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M3avg2 <- M3simsdf %>% filter(!is.na(to)) %>%
 group_by(from, to, wave) %>%
 summarise(count = n()) %>%
 mutate(weight = ifelse(from == to, 0, count))

# M1avg$from <- paste0("V", M1avg$from)
# M1avg$to <- paste0("V", M1avg$to)
# M1avg2$from <- paste0("V", M1avg2$from)
# M1avg2$to <- paste0("V", M1avg2$to)
# M1avgW2$from <- paste0("V", M1avgW2$from)
# M1avgW2$to <- paste0("V", M1avgW2$to)
# M2avg$from <- paste0("V", M2avg$from)
# M2avg$to <- paste0("V", M2avg$to)
# M2avgW2$from <- paste0("V", M2avgW2$from)
# M2avgW2$to <- paste0("V", M2avgW2$to)
# M2avg2$from <- paste0("V", M2avg2$from)
# M2avg2$to <- paste0("V", M2avg2$to)
# M3avg$from <- paste0("V", M3avg$from)
# M3avg$to <- paste0("V", M3avg$to)
# M3avgW2$from <- paste0("V", M3avgW2$from)
# M3avgW2$to <- paste0("V", M3avgW2$to)
# M3avg2$from <- paste0("V", M3avg2$from)
# M3avg2$to <- paste0("V", M3avg2$to)

ggplot(data = combinedavgs) + 
  geom_net(aes(from_id = from, to_id = to, linewidth= linewidth),
           directed = T, curvature = .2, labelon=T, size = 5,
           arrow = arrow(angle = 20, type = "open", length = unit(.1, "inches")), ecolour = "#969696", fiteach = T, 
           color = "#252525", hjust = .5, vjust = .5, labelcolour = 'white', fontsize = 3, arrowgap = .02,
           singletons = F) + 
  theme_net() + theme(panel.background = element_rect(color = 'black')) + 
  facet_wrap(~cat)

# wave2drink <- drink2[,2]
# wave2drink <- as.data.frame(wave2drink)
# wave2drink$id <- paste0("V", 1:16)
# names(wave2drink)[1] <- "drink"
# wave3drink <- drink2[,3]
# wave3drink <- as.data.frame(wave3drink)
# wave3drink$id <- paste0("V", 1:16)
# names(wave3drink)[1] <- "drink"
# M1avgcov <- merge(M1avg, wave2drink, by.x = "from", by.y = "id", all = T)
# M2avgcov <- merge(M2avg, wave2drink, by.x = "from", by.y = "id", all = T)
# M3avgcov <- merge(M3avg, wave2drink, by.x = "from", by.y = "id", all = T)
# M1avgW2cov <- merge(M1avgW2, wave3drink, by.x = "from", by.y = "id", all = T)
# M2avgW2cov <- merge(M2avgW2, wave3drink, by.x = "from", by.y = "id", all = T)
# M3avgW2cov <- merge(M3avgW2, wave3drink, by.x = "from", by.y = "id", all = T)
# 
# M1avg[,-3] %>% spread(to, weight) %>% data.frame-> distmatM1
# rownames(distmatM1) <- distmatM1$from
# distmatM1 <- distmatM1[,-1]
# distmatM1 <- as.matrix(sqrt(1/distmatM1))
# distmatM1[is.na(distmatM1)] <- 0
# M1avgW2[,-3] %>% spread(to, weight) %>% data.frame-> distmatM1W2
# rownames(distmatM1W2) <- distmatM1W2$from
# distmatM1W2 <- distmatM1W2[,-1]
# distmatM1W2 <- as.matrix(sqrt(1/distmatM1W2))
# distmatM1W2[is.na(distmatM1W2)] <- 0
# M2avg[,-3] %>% spread(to, weight) %>% data.frame-> distmatM2
# rownames(distmatM2) <- distmatM2$from
# distmatM2 <- distmatM2[,-1]
# distmatM2 <- as.matrix(sqrt(1/distmatM2))
# distmatM2[is.na(distmatM2)] <- 0
# M2avgW2[,-3] %>% spread(to, weight) %>% data.frame-> distmatM2W2
# rownames(distmatM2W2) <- distmatM2W2$from
# distmatM2W2 <- distmatM2W2[,-1]
# distmatM2W2 <- as.matrix(sqrt(1/distmatM2W2))
# distmatM2W2[is.na(distmatM2W2)] <- 0
# M3avg[,-3] %>% spread(to, weight) %>% data.frame-> distmatM3
# rownames(distmatM3) <- distmatM3$from
# distmatM3 <- distmatM3[,-1]
# distmatM3 <- as.matrix(sqrt(1/distmatM3))
# distmatM3[is.na(distmatM3)] <- 0
# M3avgW2[,-3] %>% spread(to, weight) %>% data.frame-> distmatM3W2
# rownames(distmatM3W2) <- distmatM3W2$from
# distmatM3W2 <- distmatM3W2[,-1]
# distmatM3W2 <- as.matrix(sqrt(1/distmatM3W2))
# distmatM3W2[is.na(distmatM3W2)] <- 0

# cols <- brewer.pal(n = 4, "YlOrRd")
# 
# plotM1W2 <- ggplot(data = M1avgcov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/251, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM1)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 2, M1") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
#   labs(title = "Model 1 Simulated Wave 2")
# plotM1W3 <- ggplot(data = M1avgW2cov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/388.5, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM1W2)
#            ) + 
#   scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 3, M1") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# plotM2W2 <- ggplot(data = M2avgcov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/325, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM2)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 2, M2") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
#   labs(title = "Model 2 Simulated Wave 2")
# plotM2W3 <- ggplot(data = M2avgW2cov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/357, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM2W2)
#            ) + 
#   scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 3, M2") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# plotM3W2 <- ggplot(data = M3avgcov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/258, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM3)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 2, M3") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
#   labs(title = "Model 3 Simulated Wave 2")
# plotM3W3 <- ggplot(data = M3avgW2cov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/372.5, color = as.factor(drink)), 
#            arrowgap = .03, size = 3,arrowsize = .5,
#            directed = TRUE, curvature = .2,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM3W2)
#            ) + 
#   scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
#   #labs(title = "Wave 3, M3") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net() + 
#   theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# 
# origW1 <- ggplot(data = alldat %>% filter(Wave==1), 
#        aes(from_id = X1, to_id = X2, color = as.ordered(drink))) +
#   geom_net(fiteach = FALSE, directed=TRUE, layout.alg = "kamadakawai", size = 3, arrowsize = .5, curvature = .2,
#            arrowgap = .03) +
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   theme_net() +
#   theme(legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# origW2 <- ggplot(data = alldat %>% filter(Wave==2), 
#        aes(from_id = X1, to_id = X2, color = as.ordered(drink))) +
#   geom_net(fiteach = FALSE, directed=TRUE, layout.alg = "kamadakawai", size = 3, arrowsize = .5, curvature = .2,
#            arrowgap = .03) +
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   theme_net() +
#   theme(legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
#   labs(title = "Observed Wave 2")
# origW3 <- ggplot(data = alldat %>% filter(Wave==3), 
#        aes(from_id = X1, to_id = X2, color = as.ordered(drink))) +
#   geom_net(fiteach = FALSE, directed=TRUE, layout.alg = "kamadakawai", size = 3, arrowsize = .5, curvature = .2,
#            arrowgap = .03) +
#   scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
#   theme_net() +
#   theme(legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
# 
# grid.arrange(origW2, plotM1W2,
#               plotM2W2,plotM3W2, 
#               ncol=2, clip = T)
# alldataviz2 <- alldataviz + theme(legend.position = 'none')
# ggdraw() +
#   draw_plot(alldataviz2, x=0, y=.75, width=1, height=.25) + 
#   draw_plot(plotM1W2, .1, .5, .5, .25) +
#   draw_plot(plotM1W3, .3, .5, .5, .25) +
#   draw_plot(plotM2W2, .1, .25, .5, .25) +
#   draw_plot(plotM2W3, .3, .25, .5, .25) +
#   draw_plot(plotM3W2, .1, 0, .5, .25) +
#   draw_plot(plotM3W3, .3, 0, .5, .25) + 
#   draw_plot_label(label = c("Original Data", "M1: Wave 2", "M1: Wave 3", "M2: Wave 2", "M2: Wave 3", "M3: Wave 2", "M3: Wave 3"))
# 
# M1avg2$Model <- "M1"
# M2avg2$Model <- "M2"
# M3avg2$Model <- "M3"
# allavg <- rbind(as.data.frame(M1avg2), 
#                 as.data.frame(M2avg2),
#                 as.data.frame(M3avg2))
# 
# 
# ggplot(data = allavg) + 
#   geom_histogram(aes(x = weight), binwidth = 25) + 
#   facet_grid(wave~Model, labeller = "label_both") + 
#   labs(x = "Number of times an edge occurs in 1,000 simulations") +
#   theme_bw()

# ggplot(data = M3avgcov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/251, color = as.factor(drink)), 
#            directed = TRUE, curvature = .2,
#            ealpha = .75,
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM2)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net()
# M1avg2$wave <- M1avg2$wave + 1
# alldrink <- data.frame(drink2[,2:3])
# alldrink <- gather(alldrink, wave,drink) %>% mutate(wave = parse_number(wave), id = rep(paste0("V",1:16),2))

# M1avg2cov <- merge(M1avg2, alldrink, by.x = c("from", "wave"), by.y = c("id", "wave"), all = T)
# ggplot(data = M1avg2cov) + # edge appears > 50% of the time. (is this actually what that is????) 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/388, color = as.factor(drink)), 
#            directed = TRUE, curvature = .2, labelon = F,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai", fiteach = T,
#            layout.par = list(elen = distmat)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   facet_wrap(~wave, labeller = "label_both") + theme_net() + labs(title = "Average Wave 2 and Wave 3 Networks from Model M1")
# ggplot(data = M1avg %>% filter(weight >= 51)) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/500), 
#            #directed = TRUE,
#            layout.alg = "mds", 
#            layout.par = list(var = 'user', dist = 'none', exp = 3,vm = distmat))

@


\st{What are some things that could go in here?:
\begin{itemize}
\item "average" networks -- you've already done this! Write it up, make several examples. How does the procedure for averaging generalize? What is it good for? 
\item distributions of fitted parameters. again, you've already done this!!! just write it up and put it in here. 
\item view correlations between model parameters under different model sets. eg 2 betas, 3 betas, 4 betas, etc.
\end{itemize}
 }



\section{Discussion} \label{sec:discussion}

\bibliographystyle{asa}
\bibliography{references}

\end{document}
