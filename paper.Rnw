\documentclass[12pt]{article}
% \usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
% \geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\usepackage{graphicx}
\usepackage[font=small,skip=5pt]{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
% \usepackage{color}
\usepackage{multirow}
\usepackage{rotating}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\usepackage{hyperref}
\graphicspath{{figure/}}
% \usepackage{endfloat} % Figures to the end of the document

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%---------------------------------------------------
%                 Editing Commands
\newcommand{\hh}[1]{{\color{magenta} #1}}
\newcommand{\st}[1]{{\color{orange} #1}}

%---------------------------------------------------
%                 Placing Figures


%---------------------------------------------------
% Define new environment
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}
%---------------------------------------------------

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Model Visualization Techniques for a Social Network Model}
  \author{Samantha Tyner\thanks{
    The authors gratefully acknowledge funding from the National Science Foundation Grant \# DMS 1007697. All data collection has been conducted with approval from the Institutional Review Board IRB 10-347}\hspace{.2cm}\\
    Department of Statistics and Statistical Laboratory, Iowa State University\\
    and \\
    Heike Hofmann\\
    Department of Statistics and Statistical Laboratory, Iowa State University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Model Visualization Techniques for a Social Network Model}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
 \hh{XXX just a placeholder}
\end{abstract}

\noindent%
{\it Keywords:} social network analysis, model visualization, dynamic networks, network visualization, network mapping, animation
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\tableofcontents
\newpage

<<setup, echo = FALSE, message = FALSE, warning = FALSE>>=
options(replace.assign=TRUE,width=70, digits=3)
require(knitr)
opts_chunk$set(fig.path='figure/', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, par=TRUE, cache=TRUE, concordance=TRUE, autodep=TRUE, message=F, warning=F, echo = FALSE, root.dir = "~/Desktop/Dissertation - LETS GO/SAOM-removing-blindfold/")
@

<<pkgs>>=
library(tidyverse)
library(RSiena)
library(network)
library(sna)
library(geomnet)
library(GGally)
library(netvizinf)
library(RColorBrewer)
library(gridExtra)
library(cowplot)
@

\section{Introduction} 
% \st{
% \begin{itemize}
% %\item What? - model visualization and SAOMs. briefly dsecribe what these things are
% %\item Why? - bring light to the RSiena blackbox. Need ways to visualize network models, collections of networks. 
% \item How?
% \end{itemize}
% %also talk about other dynamic network models briefly to put SAOMs in context.
% }

Models for dynamic social networks are extremely important because of how realistic they are. Social networks do not form spontaneously; they evolve over time. Ties are added and removed, and new actors can join the network. Modeling the underlying mechanisms that create network changes over time is very complex but also has the potential to uncover hidden truths. 

\par One set of models for dynamic social networks are the stochastic actor-oriented models (SAOMs) of \citet{saompaper}. These models are uniquely different from other social network models because they incorporate network statistics and actor statistics into the model, whereas most other models just use the network statistics to determine change. Allowing the actor-level statistics to change the structure of the network leads to a more intuitive model for social network change. In the "real-world" we would expect that people with common interests would be more likely to form relationships, and that intuition is no longer ignored in modeling with SAOMs. 

\par The software SIENA, and its \texttt{R} implementation \texttt{RSiena}, was developed by \citet{RSiena} to estimate the parameters of the SAOMs, among other social network models. This software is a huge contribution to the field of social network analysis, and when we began working with it we were eager to understand how it works. To better understand the \texttt{RSiena} software and the SAOMs themselves, we combine the principals of network visualization with those of model visualization from \citet{modelvis}. With the work of \citet{modelvis} guiding us, we explore RSiena and SAOMs with innovative network visualizations, both static and dynamic. By bringing some light to the underlying methods and structures "behind the scenes" of SAOMs and RSiena, we will help other researchers working with the models better understand and better use these models.

\par The guiding principles of model visualization from \citet{modelvis} are (1) display the model in the data space, (2) collections of models are more informative than singletons, and (3) explore how algorithms work, instead of just looking at the final result. Stochastic actor-oriented models are a prime example of a set of models that  benefit greatly from application of model visualization. First, the models themselves include a continuous-time Markov chain (CTMC) that is completely hidden in the model fitting process. Bringing the CTMC out from behind-the-scenes and into the light of visualization  provides researchers with insights into the underlying features of the model. Secondly, SAOMs allow for a great deal of parameters that can be added to the objective function, each of which are attached to a network statistic. These statistics are often somewhat, if not highly, correlated, and so are the associated parameters. By visualizing collections of SAOMs, we can gain a better understanding of these correlations and find ways to deal with them and rectify their effects. Finally, estimation of the parameters in a SAOM relies on a Robbins-Monro algorithm, and convergence checks rely on simulations from the fitted model. Again, in these steps are largely hidden from the researcher computing them with software. We hope that by bringing these background elements and others into the foreground with static and dynamic visualizations, stochastic actor-oriented models will be better understood and more accessible to social network analysts. 

In Section~\ref{sec:netintro}, we introduce the concepts of networks and network visualizations. In Section~\ref{sec:saoms}, we present the SAOM for social network analysis. Then, in Section~\ref{sec:ModelVis}, we combine concepts from Sections~\ref{sec:netintro} and \ref{sec:saoms} in an application of the model-vis paradigm, and conclude with a discussion in Section~\ref{sec:discussion}.

\section{Networks and their Visualizations} \label{sec:netintro}

\subsection{Introduction to Network Structures}

Network data is of frequent interest to researchers in a wide array of fields. There are techonological networks, like power grids or the internet, information networks, such as citation networks or the World Wide Web, biological networks, e.g. neural networks, and social networks, just to name a few \citep{newman}. Each of these examples have one thing in common: their data structure. There are always units of observation, which we refer to throughout as nodes, and connections of some kind between those units, which we will call edges. The networks can also vary over time, for example when new websites and hyperlinks are added on the World Wide Web, or when there are new people and relationships in a friendship network. The nodes and edges themselves also have inherent variables of interest, such as the the affiliation of authors in a coauthorship network, and the number of times two authors have collaborated. 

The multiple layers, so to speak, of network data structures create unique struggles to network analysts. Some questions that researchers aim to answer can be: How does the strength of a connection between two nodes affect the overall structure of the network? Do node-level differences affect the formation or destruction of edges? Which views of the data are most informative for communicating effects and results of statistical analyses? These are, of course, just a few broad questions, and we will focus on only the latter.  

\subsection{Visualizing Network Data} \label{sec:netvizintro}

Network visualization, also called network mapping, is a prominent subfield of network analysis. Visualizing network data is uniquely difficult because of the structure of the data itself.  Most, if not all, data visualizations rely on a well-defined axis inherited from the data. If data are numerical, histograms, scatterplots, or time series plots are straightforward to construct. If the data are categorical, bar charts and mosaic plots are available to to the researcher. If the data are spatial, there is a well-defined region in which to view information. Network data, however, are much less cut-and-dried.

There are two primary methods used to visualize networks: node-link diagrams and matrix visualization. We will use the toy data set show below to demonstrate these two methods.

<<toyex>>=
set.seed(81393)
nodes <- data.frame(id = 1:5, group = sample(LETTERS[1:2], size = 5, replace = T), stringsAsFactors = F)
g <- igraph::as_data_frame(igraph::random.graph.game(n = 5, p = .4))
toynet <- list(nodes = nodes, edges = rbind(g, c(5,4)))
toynet
@

The first method, the node-link diagram, represents nodes with points in 2D Euclidean space and then represents edges by connecting the points with lines when there is an edge between the two nodes. But, because there is no natural placement of the points, a random placement is used, then adjusted iteratively via a layout algorithm, of which there are many. The node-link diagram for our toy network is shown in Figure~\ref{fig:toyplots}.

Some layout algorithms were designed to mimic physical systems, drawing the graphs based on the "forces" connecting them. The network's edges act as springs pushing and pulling the nodes in 2D space. Others use properties of the network's adjaceny matrix in order to place the nodes. Some commonly used layout algorithms that treat networks like physical systems are Kamada-Kawai \citep{kamadakawai}, Fructerman-Reingold  \citep{fruchterman-reingold}. Other algorithms inclue multi-dimensional scaling, which relies on a defined distance matrix, or an eigen value, which depends on the eigenvalues of the network's adjacency matrix. 

The second primary method for network visualization uses the adjacency matrix of the network. An example of this type of visualization is also shown in Figure~\ref{fig:toyplots} for our toy example. This method is particularly useful when the network is very complex, dense, or large. Experimental studies have shown this to be true. For example, with seven basic perceptual tasks on networks, including things like node and edge count, adjacency matrix visualizations outperform node-link diagrams as the size and density of the network increases \citep{adjmatviz}. One drawback of the adjacency matrix visualization that \citeauthor{adjmatviz} found was that edges are overrepresented for undirected graphs. This, however, may actually be an advantage for \emph{directed} graphs, where exactly the correct number of edges is represented in a matrix visualization, but a node-link diagram may underrepresent the edge count with overlapping edges.  

<<toyplots, fig.show='hold', fig.cap="On the left, a node-link diagram of our undirected toy network, with nodes placed using the Kamada-Kawai algorithm. On the right, the adjacency matrix visualization for that same network.", out.width='.49\\linewidth'>>=
toynetdat <- fortify(as.edgedf(toynet$edges), toynet$nodes)
ggplot(data = toynetdat) + 
  geom_net(aes(from_id=from_id, to_id=to_id, shape =group, color = group), labelon = T, size = 7, labelcolour = 'white', ecolour = 'black', vjust = .5, hjust = .5, arrowgap = .02) +
  scale_color_manual(values = c("red", "blue")) + 
  theme_net() + 
  labs(title = "Node-Link Diagram")

ggplot(data = toynet$edges) +
  geom_tile(aes(x = from, y = to), color = 'black', fill = 'black') +
  geom_tile(aes(x = to, y = from), color = 'black', fill = 'black') +
  geom_vline(xintercept = 0:5+.5) + 
  geom_hline(yintercept = 0:5+.5) + 
  scale_x_continuous(breaks = 1:5, labels = 1:5, limits = c(0.5,5.5)) +
  scale_y_continuous(breaks = 1:5, labels = 1:5, limits = c(0.5,5.5)) +
  theme_bw() + 
  labs(title = "Adjacency Matrix Visualization") + 
  theme(aspect.ratio = 1)
@

\section{Stochastic Actor-Oriented Models for Longitudinal Social Networks} \label{sec:saoms}

% \st{What do I need to communicate in this section? 
% \begin{itemize}
% \item What the models are for: dynamic social networks with actor-level covariates
% \item What are the model components? rate function, objective function (network statistics, actor covariates, interactions between covariates and network statistics), time points ($t_m = 1, \dots, M$)
% \item What is RSiena? What methods of estimation, convergence checks does it use? What are the algorithms? What kinds of models can it fit? 
% \end{itemize}}


%The stochastic actor-oriented models are very aptly named: they model dynamic networks that can account for the individual node characteristics when making the edge changes. The object of analysis is a social network that has been observed at several discrete time points. The nodes in the network can have associated covariate variables that may be incorporated into modeling the tie changes in the network. The model itself has two primary pieces: the rate of change of ties, and the objective function of the nodes, which determines the exact change that gets made. 

%We start with a set of network observations at $M$ discrete time points: $x(t_1), x(t_2) \dots, x(t_M)$. The first observation, $x(t_1)$ is always conditioned on. The number of nodes / actors / vertices is constant in all time points and is denoted $n$.  The tie variables are directed and  binary, and are denoted at $x_{ij}(t_m)$ for time point $m$, and this is sometimes abbreviated to $x_{ij}$ in notation. If there is a tie from node $i$ to node $j$, then $x_{ij}=1$, and $x_{ij}=0$ otherwise. 

%\hh{mostly lit review plus a detailed description of the model and fitting process.}

A Stochastic Actor-Oriented Model (SAOM) is a model that is changing in time in order to accomodate for observations from the same network made at different points in time and that allows for changes in network structure due to actor-level covariates. This model was first introduced by Snijders in 1996 \citep{saompaper}. The two main properties of SAOMs, modelling the changes in time and using actor covariates, are crucial to understanding networks as they exist naturally. Most social networks, even holding constant the set of actors over time, are ever-changing as relationships decay or grow, and most actors (or nodes) in social networks have inherent properties that could affect how they change their place within the network. 

\subsection{Definitions, Terminology, and Notation}

In this paper, the term \textit{dynamic network} refers to a network consisting of the same set of $n$ nodes that is changing over time, and is observed at $M$ discrete time points, $t_1, \dots, t_M$. We denote these network observations $x(t_1), \dots, x(t_M)$, and in modelling we condition on the first observation, $x(t_1)$. The SAOM assumes that this longitudinal network is embedded within a continuous time Markov chain (CTMC), call it $X(T)$. This process is almost entirely unobserved: assume that the beginning of the process, $X(0)$ is equivalent to the first network observation $x(t_1)$, while the end of the process $X(\infty)$ is equivalent to the last observation $x(t_M)$. All other parts of the process are unseen. The process $X(T)$ is a series of single tie (or edge) changes, in which one actor at a time is given the opportunity to add or remove one outgoing tie. Once an actor is given the chance to change a tie, it tries to maximize a sort of utility function based on the current and potential future states of the network. 

\subsubsection{The Rate Function}

For the network $x$ and each actor $i$ in the network, the rate function dictates how often the actor $i$ gets to change its ties, $x_{ij}$, to other nodes $j \neq i$ in the network. For this paper, we assume a simple rate function, $\alpha_m$ that is constant for all actors between observations at time $t_m$ to $t_{m+1}$. In other modelling scenarios, this rate can be a function that depends on the time period of observation, some actor-level covariates or some actor-level network statistics. The rate parameter determines how quickly actor $i$ gets an opportunity to change one of its ties, $x_{ij}$ in the time period $t_{m} \leq T < t_{m+1}$. We assume that the actors $i$ are conditionally independent given their current ties, $x_{i1}, \dots, x_{in}$. This assumption leads to the rate for the whole network, $n\alpha$. In order to achieve the memorylessness property of a Markov process, for any time point, $T$, where $t_{m} \leq T < t_{m+1}$, the waiting time to the next change opportunity by actor $i$ is exponentially distributed with expected value $\frac{1}{\alpha_m}$. Thus, the waiting time to the next change opportunity by \textit{any} actor in the network is also exponentially distributed with expected value $\frac{1}{n\alpha_m}$
 
\subsubsection{The Objective Function} 

Thanks to the conditional dependence assumptions in the model, we can consider the objective function for each node separately, since only one tie from one node is changing at a time. The objective function is written as $$f_i(\boldsymbol{\beta}, x) = \sum_k \beta_k s_{ik}(x, \mathbf{Z}),$$
for $x \in \mathcal{X}$, the space of all possible directed networks with the $n$ nodes, and $\mathbf{Z}$ matrix of covariates. The vector $\boldsymbol{\beta}$ are the parameters of the model with corresponding network and covariate statistics, $s_{ik}(x, \mathbf{Z})$, for $k = 1,\dots, K$. Given the ego node, $i$, there are $n$ possible steps for the actor $i$ to take: either one of all current ties $x_{ij} = 1$ will be destroyed, a new tie will be created, or no change will occur. 

The parameters, $\boldsymbol{\beta}$, are attached to various actor-level network statistics, $s_{ik}(x)$. There are always at least two parameters, $\beta_1$ for the outdegree of a node, and $\beta_2$ for the number of reciprocal ties held by a node \citep[p. 371]{snijders01}. There are many possible parameters $\beta_i$ to add to the model. They can be split up into two groups: first, the structural effects, which only depend on the structure of the network. The inclusion of these effects has origin in the classical exponential random graph model (ERGM) for networks. These effects are written in terms of the edge variables $x_{ij}$, for $i \neq j$. The second set of effects are the actor-level or covariate effects. These effects also depend on the structure of the network. They are written in terms of $x_{ij}$ but also in terms of the covariates, $\mathbf{Z}$. A table of some possible structural and covariate effects is given in Table~\ref{tab:effects}. For a complete list of the network and covariate statistics that can be included in the objective function, see \citet{RSiena}.

\begin{table}
\centering
\begin{tabular}{ll}
\textbf{Structural Effects} \\
outdegree  & $s_{i1}(x) = \sum_j x_{ij}$ \\
reciprocity  & $s_{i2}(x) = \sum_j x_{ij}x_{ji}$ \\
transitive triplets  & $s_{i3}(x) = \sum_{j,h} x_{ij}x_{jh}x_{ih}$ \\
\textbf{Covariate Effects}\\
covariate-alter  & $s_{i4}(x) = \sum_j x_{ij}z_j$ \\
covariate-ego  & $s_{i5}(x) = z_i\sum_j x_{ij}$ \\
same covariate & $s_{i6}(x) = \sum_j x_{ij} \mathbb{I}(z_i = z_j)$ \\
jumping transitive triplets  & $s_{i7}(x) = \sum_{j \neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(z_i = z_h \neq z_j)$
\end{tabular}
\caption{\label{tab:effects} Some of the possible effects to be included in the stochastic actor-oriented models in RSiena. There are many more possible effects, but we only consider a select few here. For a complete list, see the RSiena manual \citep{RSiena}.}
\end{table}

When node $i$ is given the chance to change a tie, we assume that they wish to maximize the value of their objective function $f_i(\boldsymbol{\beta}, x)$ plus a random element, $U_i(x)$, where the $U_i(x)$ are from "the type 1 extreme value distribution (or Gumbel distribution) with mean 0 and scale parameter 1" \citep[p. 368]{snijders01}. This distribution, which is also known as the log-Weibull distribution, has probability distribution function, using $\mu$ for the mean parameter and $\sigma$ for the scale parameter, of

$$f(u|\mu, \sigma) = \frac{1}{\sigma}\exp\left\{-\left(\frac{u-\mu}{\sigma} + e^{-\frac{u-\mu}{\sigma}}\right)\right\}.$$

Using this distribution is convenient because it allows the probablity the actor $i$ chooses to change its tie to actor $j$ in terms of the objective function alone. Let $p_{ij}(\boldsymbol{\beta}, x)$ be this probability. Next, write the network $x$ in its potential future state, where the tie $x_{ij}$ has changed to $1-x_{ij}$, as $x(i \leadsto j)$. Then, the probility that the tie $x_{ij}$ changes is 

$$p_{ij}(\boldsymbol{\beta}, x) = \dfrac{\exp\left\{f_i(\boldsymbol{\beta}, x(i\leadsto j))\right\}}{\sum_{h \neq i} \exp\left\{f_i(\boldsymbol{\beta}, x(i \leadsto h))\right\}}$$

%### A SAOM as a CTMC {#saomctmc}

%In order to fit this model definition back into the original context of the CTMC described in Section \@ref(dynamicnets), it must be written in terms of its intensity matrix, $\mathbf{Q}$.  This matrix describes the rate of change between states of the process. For networks, there are a very large number of possible states, $2^{n(n-1)}$, so the intensity matrix is a square matrix of that dimension. But, thanks to the property of SAOMs that the states are allowed to change only one tie at a time, there are only $n$ possible states given the current state, $n-1$ of which are uniquely determined by the node $i$ that is given the opportunity to change. Thus, the intensity matrix $\mathbf{Q}$ is very sparse, with only $n(n-1) + 1$ non-zero entries in each row. Note that $n(n-1)$ of these represent the possible states that are one edge different from a given state, and the additional non-zero entry is for the state to remain the same. All other entries in a row are zero because those column states cannot be reached from the row state by just one change as dictated by the SAOM. The entries of $\mathbf{Q}$ are defined as follows: let $b \neq c \in \{1, 2, \dots, 2^{n(n-1)} \}$ be indices of two different possible states of the network, $x^b, x^c \in \mathcal{X}$.  Then the $bc^{th}$ entry of $Q$ is:
% \[ q(x^b, x^c) = \begin{cases} 
%       q_{ij}(\alpha, \rho, \boldsymbol{\beta}, x^b) = \lambda_i(\alpha, \rho, x^b, m)p_{ij}(\boldsymbol{\beta}, x^b) & \text{if } x^c \in \{x^b(i \leadsto j) | \text{ any } i \neq j \in \mathcal{N}\} \\
%       0 & \text{if } x^c \text{ differs from } x^b \text{ by more than 1 tie} \\
%       -\sum_{i\neq j} q_{ij}(\alpha, \rho, \boldsymbol{\beta}, x^b)  & \text{if } x^b = x^c 
%    \end{cases}
% \]
% 
% Thus, the rate of change between any two states that differ by only one tie, $x_{ij}$, is the product of the rate at which actor $i$ gets to change a tie and the probability that the tie that will change is the tie to node $j$.^[Just to be clear, the change is from $x^b_{ij}$ to $x^c_{ij} = 1 - x^b_{ij}$.] Furthermore, the theory of continuous time Markov chains gives that the matrix of transition probabilities between observation times $t_{m-1}$ and $t_{m}$ is dependent only on the difference between timepoints, $t_m - t_{m-1}$. Following the same definition for transition probabilities in Section \@ref(dynamicnets), the matrix of transition probabilities is 
% $$e^{(t_m - t_{m-1})\mathbf{Q}},$$ 
% where $\mathbf{Q}$ is the matrix defined above and $e^X$ for a real or complex square matrix $X$ is equal to $\sum_{k=0}^{\infty} \frac{1}{k!} X^k$.

\subsection{Fitting Models to Data}

To fit a SAOM to longitudinal network data, we use the package \texttt{RSiena} \citep{RSiena}. This package uses simulation methods to estimate parameter values using either the method of moments or maximum likelihood estimation. In this paper, we focus of the method of moments estimation because the theory behind it was established in \citet{saompaper}, while the maximum likelihood estimation methods were not fully established until \citet{saomsml2010}. We also focus on the score function method for estimating the derivatives of the expected values, as opposed to the finite differences method, both of which are outlined in detail in \citet{SienaAlgs}.

\par For the score function method, the SIENA software uses a Robbins-Monro algorithm (see \citet{robbinsmonro}) to estimate the solution of the moment equation
$$ E_{\theta}S = s_{obs}$$
where $\theta$ is the vector of rate and objective function parameters, and $s_{obs}$ is the observed vector of model statistics, $S$. The entire algorithm is provided in \citet{SienaAlgs}. \par There are three phases in the the SIENA algorithm, as described in \citet{RSienamanual, SienaAlgs}. The first phase performs initial estimation of the score functions used in the Robbins-Monro procedure. The second phase carries out the Robbins-Monro algorithm and obtains iterative estimates of the parameter values. The third phase uses the parameter vector estimated in phase two to estimate the score functions and the covariance matrix of the parameters, as well as performs convergence checks. In each of the the first two phases, the estimation procedure also relies on ``microsteps" that simulate from the model as it exists in its current state in order to update either the score functions or the parameter estimates. These microsteps become observed instances of the continuous-time Markov chain that is the backbone of the stochastic actor-oriented model. In Section~\ref{sec:ModelVis}, we visualize the parts of the SIENA method-of-moments algorithm, bringing them out of the ``black-box" and into the light. 

\subsection{Example Data}

To guide our visual exploration of stochastic actor-oriented models, we used a subset of the 50 actor dataset from the ``Teenage Friends and Lifestyle Study" that is provided on the \texttt{RSiena} webpage. These data come from \citet{friendsdata}, and we chose to only work with a subset of the data to make network visualizations less busy, with changes that are more noticeable. The subset contained actors 20 through 35 and the ties between them, as well as the drinking behavior of each actor at each of the three waves. This specific subset was chosen because it showed somewhat higher connectivity than other subsets, as we've emphasized in the visualizations of the three network adjacency matrices in Figure~\ref{fig:adjmatviz}. For model fitting, we condition on wave 1 and estimate the parameters of the models from the second and third waves. We will also be working with one actor level categorical covariate, drinking behavior. This variable has five values in the original data: (1) does not drink, (2) drinks once or twice a year, (3) drinks once a month, (4) drinks once a week, and (5) drinks more than once a week. The actor covariates adn networks are visualized in Figure~\ref{fig:alldataviz}. 
<<adjmatviz, fig.align='center', fig.cap="A visualization of the adjacency matrices of the three waves of network observations in the ``Teenage Friends and Lifestyle Study'' data. The subset we will be using is outlined in red.", fig.width=8>>=
source("code/00e_small_friends.R")
view_rs2$wave <- factor(view_rs2$wave)
levels(view_rs2$wave) <- c("Wave 1", "Wave 2", "Wave 3")
ggplot(data= view_rs2, aes(x = x, y=y)) + 
  geom_tile(aes(fill = as.factor(value))) + 
  scale_fill_manual("i likes j", labels=c("no", "yes"), 
                    values = c('white', 'black')) +
  geom_rect(data= NULL, inherit.aes = FALSE, color = 'red',
            aes(xmin = 19.5, xmax = 35.5, ymin = 19.5, ymax = 35.5, fill =NA)) + 
  facet_wrap(~wave) + theme_bw() +
  labs(x = "i", y = "j") + 
  theme(aspect.ratio=1, axis.text = element_blank(), axis.ticks = element_blank()) 
@

<<alldataviz, fig.cap="The smaller friendship network data we will be modelling throughout the paper.", fig.width=8>>=
actual1 <- merge(data.frame(as.edgelist(as.network(fd2.w1))), 
                 data.frame(id = 1:16, drink = drink2[,1]),
                 by.x = "X1", by.y = "id", all = T)
actual1$Wave <- 1 
actual2 <- merge(data.frame(as.edgelist(as.network(fd2.w2))),
                 data.frame(id = 1:16,drink = drink2[,2]),
                 by.x = "X1", by.y = "id", all = T)
actual2$Wave <- 2
actual3 <- merge(data.frame(as.edgelist(as.network(fd2.w3))),
                 data.frame(id = 1:16,drink = drink2[,3]),
                 by.x = "X1", by.y = "id", all = T)
actual3$Wave <- 3
alldat <- rbind(actual1, actual2, actual3)
alldat$X1 <- paste0("V", alldat$X1)
alldat$X2 <- ifelse(!is.na(alldat$X2), paste0("V", alldat$X2), NA)
alldataviz <- ggplot(data = alldat, 
       aes(from_id = X1, to_id = X2, color = as.ordered(drink), group = as.ordered(drink))) +
  geom_net(fiteach = FALSE, directed=TRUE, layout.alg = 'fruchtermanreingold', size = 3, arrowsize = .5, arrowgap = .03) +
  scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
  theme_net() +
  facet_wrap(~Wave, labeller = "label_both") + 
  theme(legend.position = 'bottom', panel.background = element_rect(fill = NA, color = "grey30"))
alldataviz
@

\section{Model Visualizations} \label{sec:ModelVis}

\st{Introduce the concepts from Wickham et al. }

The concept of model visualization was developed to complement traditional model diagnostic tools. Typically, numerical summaries such as $R^2$ are used to assess model fit, and visualizations, like residual plots, are restricted to the data. Every good data analysis includes both numerical and visual summaries of the data, so why should model description be restricted to numerical summaries? \citeauthor{modelvis} outline the three different parts of a model: the model family, the model form, and the fitted model. The latter is primarily what one thinks of first when considering a model in a data analysis, where a model is fit to data, and parameter estimates are reported. In the context of SAOMs, the fitted model contains the estimated rate parameters and the estimated objective function parameters. The model form describes the the model before the fitting process and how variables describe the data in the model. In SAOMs, the model form includes description of the rate and objective functions and the variables therein that describe how the network evolves over time. Finally, the model family is the most broad description of the model. This is the type of model that you wish to fit to the data, and is chosen based on the problem and knowledge at had. For example, we chose to use a SAOM to model network data over an exponential random graph model (ERGM) because we believe that actor-level variables effect network dynamics. These three parts of a "model" are should be visualized according to the three principals of model visualization: display the model in the data space, look at a collection of models, and explore the process of fitting the model, not just the end result. 

\subsection{Explore algorithms, not just end result}

The model fitting process in \texttt{RSiena} involves several simulation steps that are hidden from the user. This is practical and efficient if a researcher is primarily interested in fitting one model to a set of longitudinal network data and obtaining parameter estimates. We are more interested in how the models are fit, so we extracted and explored the different steps of that process. 

A key component of each step of the SIENA method of moments algorithm is the ``microstep." This step simulates from the model in its current state, $x(t_m)$ with current parameter values $\theta_{0}$, to the next state, $x(t_{m+1})$. This microstep process stops when the simulated network has acheived the same number of differences, $C$, from $x(t_m)$ as $x(t_{m+1})$, where 
$$C = \sum_{i \neq j} |x_{ij}(t_{m+1}) - x_{ij}(t_{m})|.$$ 
This simulation process follows the steps of the continuous-time Markov chain. First, an ``ego node'' is selected to make a change, and then it randomly makes one change in its ties according to the probability, $p_{ij}$, determined by its objective function. The options for change are (1) removing a current tie, (2) adding a new tie, or (3) making no change at all. Saving all of these steps is not efficient if you're only interested in estimating parameter values, but they can be saved and extracted using options in \texttt{RSiena}, which is exactly what we did to create our visualizations. Between two network observations $x(t_m)$ and $x(t_{m+1})$, there can be dozens, hundreds, or even thousands of microsteps, depending on the size of the network. We wanted to see these in-between steps in order to better understand the behavior of the underlying continuous-time Markov chain. 

The first vizualization we present here is an animation of microsteps simulated to emulate the transition steps from wave 1 to wave 2 the subset of the friends data shown in Figure ~\ref{fig:alldataviz}. Movies similar to our animation were used to visualize the changes of dynamic networks in \citet{dynnetviz}. When each ego node is selected in a microstep, it is emphasized in the animation, then the associated edge either appears or disappears. If there are no changes at a particular microstep, no changes are seen. Some frames of the animations are shown in Figure~\ref{fig:makinggif}, and the full movie can be viewed at INSERT LINK TO GIF. 

<<makinggif, fig.cap='A selection of images in the microstep animation. The selected edges and nodes are emphasized by changing the size and the color, then when edges are removed, they fade out, shrinking in size, while the nodes change color and shrink to blend in with the other nodes.', fig.width=8, cache=TRUE>>=
friend.data.w1 <- as.matrix(read.table("data/s50_data/s50-network1.dat"))
friend.data.w2 <- as.matrix(read.table("data/s50_data/s50-network2.dat"))
friend.data.w3 <- as.matrix(read.table("data/s50_data/s50-network3.dat"))
drink <- as.matrix(read.table("data/s50_data/s50-alcohol.dat"))
fd2.w1 <- friend.data.w1[20:35,20:35]
fd2.w2 <- friend.data.w2[20:35,20:35]
fd2.w3 <- friend.data.w3[20:35,20:35]
friendshipData <- array(c(fd2.w1, fd2.w2,fd2.w3), dim = c(16, 16, 3))

# running Siena
friendship <- sienaDependent(friendshipData)
alcohol <- varCovar(drink[20:35,])
mydata <- sienaDataCreate(friendship, alcohol)
myeffnull <- getEffects(mydata)
myalgorithm <- sienaAlgorithmCreate(projname = 's16_3')
set.seed(823746)
load("data/ansnullpaper.rda")
ansnullchains <- get_chain_info(ansnull)
ansnullchains %>%
  filter(period == 1) %>%  #only look at chains from wave 1 to wave 2
  group_by(rep) %>%
  select(rep, from = ego, to = alter) %>%
  mutate(val = as.numeric(!from == to),
         from = paste0("V", parse_number(from)+1), # make the chains
         to = paste0("V", parse_number(to)+1)) -> ansnullchainsw1w2
colnames(fd2.w1) <- paste0("V", 1:16)
rownames(fd2.w1) <- paste0("V", 1:16)
wave1friends <- fortify(as.adjmat(fd2.w1))
ms1 <- listMicrosteps(dat = wave1friends,
                      microsteps = filter(ansnullchainsw1w2, rep == 1))
microsteps <- ms1[1:9]

microsteps_df <- getMicrostepsDF(microsteps)
pte <- pretween_edges(microsteps = microsteps)
ptv <- pretween_vertices(pte = pte, layoutparams = list(n = 16))

# get tweened data for all microsteps
pall <- tween_microsteps(ptv, pte, microsteps_df)

static_pall <- filter(pall, .frame %in% (1:12 * 3-2))

create_net_animate(static_pall) + 
  facet_wrap(~.frame, nrow=3, labeller = 'label_both') + 
  theme(panel.background = element_rect(color = 'black'))
@


XX INSERT DESCRIPTION OF THE MOVIE - WHAT IT ALLOWS US TO SEE, ETC. XX

We also use animation to view the changing structure of the adjacency matrix the microsteps. The adjacency matrices for the three waves of friendship data as shown in Figure~\ref{fig:alldataviz} are ordered by node id. There are 16 nodes in the data, numbered 1-16, and that order is used on the $x$ and $y$ axes for the matrix visualization. Viewing the adjacency matrices with this arbitrary ordering does not provide much information to the viewer about the underlying structure of the network. This lack of perceived structure would be exacerbated in an animation. So, we adjust the ordering so that the viewer can perceive more of the structure of the graph. This process is known as matrix seriation \citep{seriation}. 

To reorder the vertices for the matrix visualization, we first constructed a cumulative adjaceny matrix, $\mathbf{A}^{cum}$, for the series of microsteps simulating the network from $x(t_m)$ to $x(t_{m+1})$. A single entry in the cumulative adjacency matrix, $\mathbf{A}^{cum}_{ij}$, is the total number of times the edge from node $i$ to node $j$ appears in the network from the initial observation, $x(t_m) \equiv X(0)$ to the final reult of the last microstep, $X(C)$: 
$$ \mathbf{A}^{cum}_{ij} = \sum_{c = 0}^C X_{ij}(c). $$
We then performed a principal component analysis on $\mathbf{A}^{cum}$, and used the values of the first principal component to order the vertices on the $x$ and $y$ axes for the adjacency matrix animation. For one such series of microsteps simulated by \texttt{RSiena}, we present the adjacency matrix ordered by the (arbitrary) vertex id alongside the seriated adjacency microsteps using the first principal component loading on the cumulative adjacency matrix, $A^{cum}$

<<adjmatorder, fig.cap="On the left, the starting friendship network represented in adjacency matrix form, ordered by vertex id. On the right, the same adjacency matrix is presented after ordering the vertices by one repitition of the microstep simulation process from wave one to wave two.", fig.width=8>>=
# first present the original wave 1 adj mat, then present the adj mat after being ordered by the 1st pc of the big adjmat A. 
numordersf <- data.frame(fd2.w1) %>% 
  mutate(from = rownames(fd2.w1)) %>% 
  gather(key = to ,value = value, -from)
pcaordersf <- numordersf

numordersf$from <- factor(numordersf$from, levels = paste0("V", 1:16))
numordersf$to <- factor(numordersf$to, levels = paste0("V", 1:16))

p1 <- ggplot(data = numordersf, aes(x = from, y = to)) + geom_tile(color = 'black', aes(fill = as.factor(value))) + 
  scale_fill_manual(values = c('white', 'black')) +
  theme(legend.position = 'none', axis.text = element_text(size = 8)) + 
  coord_fixed()

sfmsall <- read_csv("data/smallfriendsmicrosteps.csv")

sfmsall %>% 
  group_by(from, to) %>% 
  filter(rep == 1) %>% 
  summarise(count = n()) -> sfmsall_summ
sfmsall_summ$from <- factor(sfmsall_summ$from, levels = paste0('V', 1:16))
sfmsall_summ$to <- factor(sfmsall_summ$to, levels = paste0('V', 1:16))
sfmsall_summ %>% 
  spread(to, count, fill = 0) %>% 
  data.frame -> allmsrep1_adjmat
allmsrep1_adjmat$V16 <- 0
allmsrep1_adjmat <- allmsrep1_adjmat[,-1]
allmsrep1_adjmat <- as.matrix(allmsrep1_adjmat)
rownames(allmsrep1_adjmat) <- paste0("V", 1:16)

pca1 <- prcomp(allmsrep1_adjmat)
order1 <- names(sort(pca1$rotation[,1], decreasing = T))
order2 <- names(sort(abs(pca1$rotation[,1]), decreasing = T))

pcaordersf$from <- factor(pcaordersf$from, levels = order1)
pcaordersf$to <- factor(pcaordersf$to, levels = order1)

p2 <- ggplot(data = pcaordersf, aes(x = from, y = to)) + geom_tile(color = 'black', aes(fill = as.factor(value))) + 
  scale_fill_manual(values = c('white', 'black')) +
  theme(legend.position = 'none', axis.text = element_text(size = 8)) + 
  coord_fixed()

grid.arrange(p1,p2, nrow = 1)
@

<<histogramalters, fig.width = 8, fig.height=11, fig.cap="Probabilities of">>=
ansnullchains %>% group_by(period, rep) %>% 
  mutate(ms = row_number()) %>% 
  filter(ms == 1, period == 1) %>% 
  mutate(alterProb = exp(logAlterProb),
         ego = as.numeric(as.character(ego)),
         alter = as.numeric(as.character(alter))) %>% 
  ungroup() %>% 
  group_by(ego, alter) %>% 
  summarize(count = n(), alterProb = unique(alterProb)) %>% ungroup() %>% 
  group_by(ego) %>% 
  mutate(csum = cumsum(alterProb)) %>% 
  arrange(ego, csum) -> cumprobs 
cumprobs$ego <- factor(paste0("V",cumprobs$ego+1), levels = paste0("V", 1:16))

cumprobs %>% ggplot() + geom_histogram(aes(x = alter, weight = alterProb), color = 'black', fill = 'white', binwidth = 1) + scale_x_continuous(labels = paste0("V",1:16), breaks = 0:15) + coord_flip() + facet_wrap(~ego, labeller = "label_both") + theme_gray()

# cprobslist <- split(cumprobs, cumprobs$ego)
# plots <- list()
# for(i in 1:length(cprobslist)){
#   cprobslist[[i]] %>%
#   ggplot() + 
#   geom_histogram(aes(x = as.numeric(as.character(alter)),
#                      y = alterProb), stat = 'identity', fill = 'white', color = 'black') + 
#   geom_line(aes(x = as.numeric(as.character(alter)), 
#                 y = csum)) + 
#   geom_point(aes(x = as.numeric(as.character(alter)), 
#                 y = csum)) +
#   scale_x_continuous(limits = c(-.5, 15), breaks = 0:15, labels = 0:15) +   
#   geom_hline(yintercept = 1, color = 'red', linetype = 'dashed') + 
#   labs(x = "alter", y = expression(p[ij]), title = paste("Ego node:", names(cprobslist)[i])) + # ylab(expression(p[ij])) + 
#   theme_gray() + 
#     theme(plot.title = element_text(size = 10), 
#           axis.title = element_text(size = 8), 
#           plot.margin = unit(rep(0,4),"pt")) -> p
#   plots[[i]] <- p
# }
# grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]],
#              plots[[5]], plots[[6]], plots[[7]], plots[[8]],
#              #plots[[9]], plots[[10]], plots[[11]], plots[[12]],
#              #plots[[13]], plots[[14]], plots[[15]], plots[[16]], 
#              ncol = 2, padding = unit(0, 'line'))
@

\st{The algorithms here are many!
\begin{enumerate}
%\item First, the microsteps / CTMC between each time point (gifs, other animations)
\item Phases of RSiena: phase 1 = initial parameter estimate
\item phase 2 = iterative update of parameters - trace plots
\item phase 3 = simulations for convergence diagnostics
\end{enumerate}
 } 

\hh{The visualizations should address most elements of structure described before, i.e. follow along in the setup (for descriptive and teaching purposes), the fitting (understanding the model and interpreting the results) and then diagnostics (how well does the model fit the data - where are the most differences to the actual data/what are the sensitive parameters?) }

\hh{Some of the visualizations we talked about
\begin{enumerate}
\item visualization of the MCMC process:  movie of individual actors' step-by-step decisions; could be done with the ndtv package, as long as we can get individual steps out of RSiena
\item global picture: 1000 simulated networks + alpha blending should give a pretty good idea of the most important edges
%\item global structure + principal components (tensor pca)
\end{enumerate}}

\subsection{View the model in the data space}

In \citeauthor{modelvis}, they define the \textit{data space} as ``the region over which we can reliably make inferences, usually a hypercube containing the data" \citep[p.~206]{modelvis}. What does this definition mean for network data? For dynamic social networks, there are a few different data ``spaces." First, we have the actors and their corresponding covariates. Second, we have the dyad variables that describe the relationships between the variables. Finally, we have the time over which the network is allowed to evolve. These three data pieces can be visualized together in various ways. The traditional node-link visualization uses one of many algorithms to layout the actors as points in 2D space, then draws segments connecting the points in 2D if the dyad between two nodes has value greater than zero, and draws nothing otherwise. The time aspect can be visualized by drawing each network observation in time and placing the observed timepoints side-by-side. 

One tool that can bring these three data spaces together in this way is the \texttt{R} package \texttt{geomnet} \citep{geomnet}. Different aesthesic aspects of each piece of the node-link diagram can be tied to the underlying node or edge data. The color, size, and shape of the points can be used to represent variables in the node data, and the color, size, and linetype of the links between points can be used to represent the edge variables. In a social network, node data might be age, gender, and occupation of the person in the network, and node data might be length of connection between two people, how the people first met (school, work, church, etc.), and how often they interact. Pulling all of this information together with \texttt{geomnet} allows the entire data space to be viewed at once. 

XXX INSERT AN EXAMPLE OF DATA WITH LOTS OF EDGE, NODE VARS OF INTEREST XXX 

These visual connections between the node, edge, and time data spaces are themselves part of the data space, because they can be viewed in different ways. Another way to connect the node and the edge data, as discussed in Section~\ref{sec:netvizintro}, is through the network's adjaceny matrix. The time dimension can also be incorporated with side-by-side adjacency matrix visualizations. 

\st{Because of the multi-level data structure of a network, viewing the model in the data space can depend on which aspect of the model you are interested in viewing. XXX Ideas: summary statistic dist. on nets simulated from model and show statistic from data; interactive plots showing objective function values on nodes; select node, select other node, show how the function values change interactively (future); XXX} 


% \st{
% \begin{itemize}
% \item WHat is the model? the network model (SAOM model is just one of many)
% \item What is the data? Edge data, node data, observed time points, unobserved timepoints (CTMC)
% \item Note: specifically dealing with dynamic networks observed at discrete time points. 
% \end{itemize}
% What are some things that could go in here? need more data examples; not just the friends data.
% Harry Potter data??? I want to find my own data, too.}

\subsection{Collections are more informative than singletons}

The second principle of Wickham et al states that ``collections are more informative than singletons" \citep[p.~210]{modelvis}. They describe several different methods for producing such collections, but we focus on only a few here. We look at collections produced ``from exploring the space of all possible models," ``by varying model settings", ``by fitting the same model to different datasets", and ``by treating each iteration as a model" \citep[p.~210-11]{modelvis}. We also fit the same model many times, resulting in distributions of fitted parameters for one data set.

A classical problem with statistical network models generally is the lack of an ``average network" measure. Statisticians frequently rely on averages and expected values, but statistical network models, especially those as complex as SAOMs, lack an intuitive expected value measure. We could talk about expected values of parameters, but there is no way to talk about the expected value of the network based on the model. How then, can we arrive at an ``average" network? We propose to answer this question through visualization.

We first consider the 16 actor subset of the teenage friends and lifestyle data available on the \texttt{RSiena} website \citep{RSiena}. To this data, we fit three different SAOMs. Each SAOM used a simple rate function, $\alpha_m$, and an objective function with two or three parameters. The first model, $M1$, contains the absolute minimum number of parameters in the objective function $f_i(x)$:  $$f_i(x)^{M1} = \beta_1s_{i1} + \beta_2s_{i2}$$, where $s_{i1}$ is the density network statistic and $s_{i2}$ is the reciprocity network statistic for actor $i$. The second and third models, $M2$ and $M3$, contain a third parameter in the objective function that was determined in the\texttt{RSienaTest} software to be significant, with $p$-values less than 0.05 \citep{RSienaTest}. The $M2$ model contains an actor-level covariate parameter, and the $M3$ model contains an additional strutural effect in the objective function.
\begin{align*}
f_i(x)^{M2} & = \beta_1s_{i1} + \beta_2s_{i2} + \beta_3s_{i3} \\
f_i(x)^{M3} & = \beta_1s_{i1} + \beta_2s_{i2} + \beta_4s_{i4}, 
\end{align*} where $s_{i3} = \sum_{j \neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(z_i = z_h \neq z_j)$, and $s_{i4} =  |\{j : x_{ij} = 0, \sum\limits_h x_{ih}x_{hj} \geq 2\}|$. These statistics are known as the number of jumping transitive triplets and the number of doubly achieved distances two effect, respectively. The first statistic emphasizes triad relationships that are formed between actors from different covariate groups, while the other emphasizes indirect ties between actors. These two effects are visually represented in Figure~\ref{fig:jtt} and ~\ref{fig:dad}, respectively. 

\begin{figure}\label{fig:structures}
\begin{subfigure}[t]{.45\textwidth}
\caption{\label{fig:jtt}Realization of a jumping transitive triplet, where $i$ is the focal actor, $j$ is the target actor, and $h$ is the intermediary. The group of the actors is represented by the shape of the node.}
<<jtt, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center'>>=
jTTe <- data.frame(from = c('i', 'i', 'h'), to = c('h', 'j', 'j'))
jTTn <- data.frame(id = letters[8:10], group = c(1,1,2))

jTT <- merge(jTTe, jTTn, by.x = 'from', by.y = "id", all = T)

set.seed(12345) 
ggplot(data = jTT, aes(from_id = from, to_id = to)) + 
  geom_net(aes(shape = as.factor(group)), directed = T, labelon = T, 
           labelcolour='grey80',vjust = 0.5, hjust =0.5, arrowgap = .15, 
           colour = 'black', size=10, 
           ecolour = c("red", "grey40", "grey40", "grey40")) + 
  expand_limits(x=c(-0.1,1.1), y=c(-0.1,1.1)) +
  theme_net() +
  theme(legend.position = "none")
@
\end{subfigure}\hfill
\begin{subfigure}[t]{.45\textwidth}
\caption{\label{fig:dad}Doubly achieved distance between actors $i$ and $k$.}
<<dab, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center'>>=
dade <- data.frame(from = c('i', 'i', 'h', 'j'), to = c('h', 'j', 'k', 'k'))
dadn <- data.frame(id = letters[8:11], group = c(1,1,1,1))

dad <- merge(dade, dadn, by.x = 'from', by.y = "id", all = T)

set.seed(12345) 
ggplot(data = dad, aes(from_id = from, to_id = to)) + 
  geom_net(aes(shape = as.factor(group)), directed = T, labelon = T, labelcolour='grey80',vjust = 0.5, hjust =0.5, arrowgap = .15, colour = 'black', size=10) + 
  expand_limits(x=c(-0.1,1.1), y=c(-0.1,1.1)) +
  theme_net() +
  theme(legend.position = "none")
@
\end{subfigure}
\caption{Structural newtwork effects. On the left, a jumping transitive triplet (JTT). On the right, a doubly achieved distance between $i$ and $k$.}
\end{figure}
We these three models using \texttt{RSiena} 1,000 times each. We then looked at the distribution of the fitted values, which are shown in Figure~\ref{fig:distests}. We can see from these distributions that the inclusion of the jumping transitive triplet parameter, $\beta_3$ is obviously affecting the distributions of the other four parameters included in all models, $\alpha_1$, $\alpha_2$, $\beta_1$, and $\beta2$. In comparison with models M1 and M3, model M2 typically has higher estimates of the rate parameter, meaning that the inclusion of the covariate statistic in the model leads to higher estimates of the number of times, on average, a node gets to change its ties. This seems problematic: it is not clear that the addition of a parameter to the objective function \textit{should} effect the estimation of the rate parameters. 

To further investigate this relationship between the parameter values, we look at correlations between each of the parameter estimates in each model. In Figure~\ref{fig:corplots}, we examine correlations between each of pair of parameters within each model and overall.\nocite{ggally} The strongest correlation within each model is between $\beta_1$ and $\beta_2$, with absolute value of correlation between those two parameter values greater than 0.90 in all three models. The $\beta_1$ parameter is also highly correlated with the $\beta_3$ parameter within model M2, but it is not as highly correlated with the $\beta_4$ parameter in model M3. 

<<getdata, results='asis'>>=
nulls <- read.csv("data/distribution_null_model.csv")
alt <- read.csv("data/distribution_jumpTT_model.csv")
alt2nd <- read.csv("data/distribution_dblpairs_model.csv")
simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")
means <- simu2 %>% group_by(Model, parameter) %>% summarize(
  means = mean(estimate)
)
null_mod_sv <- as.numeric(nulls[,-c(1,6,7)] %>% summarise_each(funs(mean)))
alt_mod_sv <- as.numeric(alt[,-c(1, 7,8)] %>% summarise_each(funs(mean)))
alt_mod2_sv <- as.numeric(alt2nd[,-c(1,7,8)] %>% summarise_each(funs(mean)))
@

<<distests, fig.cap="Distribution of fitted parameter values for our three SAOMs. The inclusion of $\\beta_3$ or $\\beta_4$ clearly has an effect on the distribution of the rate parameters, $\\alpha_1$ and $\\alpha_2$.", fig.width=8>>=

#simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")

ggplot(data = simu2) + 
  geom_density(aes(x = estimate, fill = Model), alpha = .5) + 
  facet_wrap(~parameter, scales = 'free') + 
  theme_bw()
@


<<corplots, fig.cap="A matrix of plots demonstrating the strong correlations between parameter estimate in our SAOMs. The strongest correlation within each model is between $\\beta_1$ and $\\beta_2$.", fig.width=8>>=

simu2 <- read.csv("data/simulation-1000-M1-M2-M3.csv")

simu_spread <- simu2 %>% spread(parameter, estimate)
ggpairs(simu_spread, columns = 3:8, ggplot2::aes(colour=Model, alpha = .5)) + theme_bw()
@

We also explore simulations from these different models given parameter values. Using the means from the 1,000 fitted parameter values, we simulated 1,000 observations from each of our three models. We condition on the first friendship network observation, and the second and third observations are simulated from the SAOM models estimated previously. From these simulations, we first create a visualization that represents an ``average" network. To do this, we count occurences of each possible edge in the simulations, resulting in a summary network with weighted edges representing the number of times an edge appeared when simulating from the SAOM. 

<<averagenet>>=
#friend.data.w1 <- as.matrix(read.table("s50_data/s50-network1.dat"))
#friend.data.w2 <- as.matrix(read.table("s50_data/s50-network2.dat"))
#friend.data.w3 <- as.matrix(read.table("s50_data/s50-network3.dat"))
#drink <- as.matrix(read.table("s50_data/s50-alcohol.dat"))
friendshipData <- array(c(fd2.w1, fd2.w2, fd2.w3), dim = c(16, 16, 3))
friendship <- sienaDependent(friendshipData)
alcohol <- varCovar(drink[20:35,])
mydata <- sienaDataCreate(friendship, alcohol)
M1eff <- getEffects(mydata)
M2eff <- includeEffects(M1eff, jumpXTransTrip, interaction1 = "alcohol")
M3eff <- includeEffects(M1eff, nbrDist2twice)
data(M1ests_bigfriends)
M1parms <- (means %>% filter(Model == "M1"))$means
M2parms <- (means %>% filter(Model == "M2"))$means
M3parms <- (means %>% filter(Model == "M3"))$means
M1sims1000 <- saom_simulate(dat = mydata, 
                            struct = M1eff,
                       parms = as.numeric(M1parms), 
                       N = 1000)
M2sims1000 <- saom_simulate(dat = mydata, 
                            struct = M2eff,
                       parms = as.numeric(M2parms), 
                       N = 1000)
M3sims1000 <- saom_simulate(dat = mydata, 
                            struct = M3eff,
                       parms = as.numeric(M3parms), 
                       N = 1000)
M1simsdf <- sims_to_df(M1sims1000)
M2simsdf <- sims_to_df(M2sims1000)
M3simsdf <- sims_to_df(M3sims1000)
M1avg <- M1simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M1avgW2 <- M1simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M1avg2 <- M1simsdf %>% filter(!is.na(to)) %>%
 group_by(from, to, wave) %>%
 summarise(count = n()) %>%
 mutate(weight = ifelse(from == to, 0, count))
M2avg <- M2simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M2avgW2 <- M2simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M2avg2 <- M2simsdf %>% filter(!is.na(to)) %>%
 group_by(from, to, wave) %>%
 summarise(count = n()) %>%
 mutate(weight = ifelse(from == to, 0, count))
M3avg <- M3simsdf %>% filter(!is.na(to) & wave == 1) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M3avgW2 <- M3simsdf %>% filter(!is.na(to) & wave == 2) %>%
  group_by(from, to) %>% 
  summarise(count = n()) %>% 
  mutate(weight = ifelse(from == to, 0, count))
M3avg2 <- M3simsdf %>% filter(!is.na(to)) %>%
 group_by(from, to, wave) %>%
 summarise(count = n()) %>%
 mutate(weight = ifelse(from == to, 0, count))

M1avg$from <- paste0("V", M1avg$from)
M1avg$to <- paste0("V", M1avg$to)
M1avg2$from <- paste0("V", M1avg2$from)
M1avg2$to <- paste0("V", M1avg2$to)
M1avgW2$from <- paste0("V", M1avgW2$from)
M1avgW2$to <- paste0("V", M1avgW2$to)
M2avg$from <- paste0("V", M2avg$from)
M2avg$to <- paste0("V", M2avg$to)
M2avgW2$from <- paste0("V", M2avgW2$from)
M2avgW2$to <- paste0("V", M2avgW2$to)
M2avg2$from <- paste0("V", M2avg2$from)
M2avg2$to <- paste0("V", M2avg2$to)
M3avg$from <- paste0("V", M3avg$from)
M3avg$to <- paste0("V", M3avg$to)
M3avgW2$from <- paste0("V", M3avgW2$from)
M3avgW2$to <- paste0("V", M3avgW2$to)
M3avg2$from <- paste0("V", M3avg2$from)
M3avg2$to <- paste0("V", M3avg2$to)

wave2drink <- drink2[,2]
wave2drink <- as.data.frame(wave2drink)
wave2drink$id <- paste0("V", 1:16)
names(wave2drink)[1] <- "drink"
wave3drink <- drink2[,3]
wave3drink <- as.data.frame(wave3drink)
wave3drink$id <- paste0("V", 1:16)
names(wave3drink)[1] <- "drink"
M1avgcov <- merge(M1avg, wave2drink, by.x = "from", by.y = "id", all = T)
M2avgcov <- merge(M2avg, wave2drink, by.x = "from", by.y = "id", all = T)
M3avgcov <- merge(M3avg, wave2drink, by.x = "from", by.y = "id", all = T)
M1avgW2cov <- merge(M1avgW2, wave3drink, by.x = "from", by.y = "id", all = T)
M2avgW2cov <- merge(M2avgW2, wave3drink, by.x = "from", by.y = "id", all = T)
M3avgW2cov <- merge(M3avgW2, wave3drink, by.x = "from", by.y = "id", all = T)

M1avg[,-3] %>% spread(to, weight) %>% data.frame-> distmatM1
rownames(distmatM1) <- distmatM1$from
distmatM1 <- distmatM1[,-1]
distmatM1 <- as.matrix(sqrt(1/distmatM1))
distmatM1[is.na(distmatM1)] <- 0
M1avgW2[,-3] %>% spread(to, weight) %>% data.frame-> distmatM1W2
rownames(distmatM1W2) <- distmatM1W2$from
distmatM1W2 <- distmatM1W2[,-1]
distmatM1W2 <- as.matrix(sqrt(1/distmatM1W2))
distmatM1W2[is.na(distmatM1W2)] <- 0
M2avg[,-3] %>% spread(to, weight) %>% data.frame-> distmatM2
rownames(distmatM2) <- distmatM2$from
distmatM2 <- distmatM2[,-1]
distmatM2 <- as.matrix(sqrt(1/distmatM2))
distmatM2[is.na(distmatM2)] <- 0
M2avgW2[,-3] %>% spread(to, weight) %>% data.frame-> distmatM2W2
rownames(distmatM2W2) <- distmatM2W2$from
distmatM2W2 <- distmatM2W2[,-1]
distmatM2W2 <- as.matrix(sqrt(1/distmatM2W2))
distmatM2W2[is.na(distmatM2W2)] <- 0
M3avg[,-3] %>% spread(to, weight) %>% data.frame-> distmatM3
rownames(distmatM3) <- distmatM3$from
distmatM3 <- distmatM3[,-1]
distmatM3 <- as.matrix(sqrt(1/distmatM3))
distmatM3[is.na(distmatM3)] <- 0
M3avgW2[,-3] %>% spread(to, weight) %>% data.frame-> distmatM3W2
rownames(distmatM3W2) <- distmatM3W2$from
distmatM3W2 <- distmatM3W2[,-1]
distmatM3W2 <- as.matrix(sqrt(1/distmatM3W2))
distmatM3W2[is.na(distmatM3W2)] <- 0


cols <- brewer.pal(n = 4, "YlOrRd")

plotM1W2 <- ggplot(data = M1avgcov) + 
  geom_net(aes(from_id = from, to_id = to, linewidth = weight/251, color = as.factor(drink)), 
           arrowgap = .03, size = 3,arrowsize = .5,
           directed = TRUE, curvature = .2,
           #layout.alg = "fruchtermanreingold"
           layout.alg = "kamadakawai",
           layout.par = list(elen = distmatM1)
           ) + 
  scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
  #labs(title = "Wave 2, M1") + 
  scale_size_continuous(name = "Weight") + 
  theme_net() + 
  theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
  labs(title = "Model 1 Simulated Wave 2")
plotM1W3 <- ggplot(data = M1avgW2cov) + 
  geom_net(aes(from_id = from, to_id = to, linewidth = weight/388.5, color = as.factor(drink)), 
           arrowgap = .03, size = 3,arrowsize = .5,
           directed = TRUE, curvature = .2,
           #layout.alg = "fruchtermanreingold"
           layout.alg = "kamadakawai",
           layout.par = list(elen = distmatM1W2)
           ) + 
  scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
  #labs(title = "Wave 3, M1") + 
  scale_size_continuous(name = "Weight") + 
  theme_net() + 
  theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
plotM2W2 <- ggplot(data = M2avgcov) + 
  geom_net(aes(from_id = from, to_id = to, linewidth = weight/325, color = as.factor(drink)), 
           arrowgap = .03, size = 3,arrowsize = .5,
           directed = TRUE, curvature = .2,
           #layout.alg = "fruchtermanreingold"
           layout.alg = "kamadakawai",
           layout.par = list(elen = distmatM2)
           ) + 
  scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
  #labs(title = "Wave 2, M2") + 
  scale_size_continuous(name = "Weight") + 
  theme_net() + 
  theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
  labs(title = "Model 2 Simulated Wave 2")
plotM2W3 <- ggplot(data = M2avgW2cov) + 
  geom_net(aes(from_id = from, to_id = to, linewidth = weight/357, color = as.factor(drink)), 
           arrowgap = .03, size = 3,arrowsize = .5,
           directed = TRUE, curvature = .2,
           #layout.alg = "fruchtermanreingold"
           layout.alg = "kamadakawai",
           layout.par = list(elen = distmatM2W2)
           ) + 
  scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
  #labs(title = "Wave 3, M2") + 
  scale_size_continuous(name = "Weight") + 
  theme_net() + 
  theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
plotM3W2 <- ggplot(data = M3avgcov) + 
  geom_net(aes(from_id = from, to_id = to, linewidth = weight/258, color = as.factor(drink)), 
           arrowgap = .03, size = 3,arrowsize = .5,
           directed = TRUE, curvature = .2,
           #layout.alg = "fruchtermanreingold"
           layout.alg = "kamadakawai",
           layout.par = list(elen = distmatM3)
           ) + 
  scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
  #labs(title = "Wave 2, M3") + 
  scale_size_continuous(name = "Weight") + 
  theme_net() + 
  theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
  labs(title = "Model 3 Simulated Wave 2")
plotM3W3 <- ggplot(data = M3avgW2cov) + 
  geom_net(aes(from_id = from, to_id = to, linewidth = weight/372.5, color = as.factor(drink)), 
           arrowgap = .03, size = 3,arrowsize = .5,
           directed = TRUE, curvature = .2,
           #layout.alg = "fruchtermanreingold"
           layout.alg = "kamadakawai",
           layout.par = list(elen = distmatM3W2)
           ) + 
  scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
  #labs(title = "Wave 3, M3") + 
  scale_size_continuous(name = "Weight") + 
  theme_net() + 
  theme(plot.title = element_text(hjust = .5), legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))

origW1 <- ggplot(data = alldat %>% filter(Wave==1), 
       aes(from_id = X1, to_id = X2, color = as.ordered(drink))) +
  geom_net(fiteach = FALSE, directed=TRUE, layout.alg = "kamadakawai", size = 3, arrowsize = .5, curvature = .2,
           arrowgap = .03) +
  scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
  theme_net() +
  theme(legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))
origW2 <- ggplot(data = alldat %>% filter(Wave==2), 
       aes(from_id = X1, to_id = X2, color = as.ordered(drink))) +
  geom_net(fiteach = FALSE, directed=TRUE, layout.alg = "kamadakawai", size = 3, arrowsize = .5, curvature = .2,
           arrowgap = .03) +
  scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
  theme_net() +
  theme(legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm")) + 
  labs(title = "Observed Wave 2")
origW3 <- ggplot(data = alldat %>% filter(Wave==3), 
       aes(from_id = X1, to_id = X2, color = as.ordered(drink))) +
  geom_net(fiteach = FALSE, directed=TRUE, layout.alg = "kamadakawai", size = 3, arrowsize = .5, curvature = .2,
           arrowgap = .03) +
  scale_color_manual(values = cols[2:4], name = "Drinking\nBehavior") + 
  theme_net() +
  theme(legend.position = 'none',plot.margin=unit(c(0,0,0,0),"mm"))

grid.arrange(origW2, plotM1W2,
              plotM2W2,plotM3W2, 
              ncol=2, clip = T)
alldataviz2 <- alldataviz + theme(legend.position = 'none')
ggdraw() +
  draw_plot(alldataviz2, x=0, y=.75, width=1, height=.25) + 
  draw_plot(plotM1W2, .1, .5, .5, .25) +
  draw_plot(plotM1W3, .3, .5, .5, .25) +
  draw_plot(plotM2W2, .1, .25, .5, .25) +
  draw_plot(plotM2W3, .3, .25, .5, .25) +
  draw_plot(plotM3W2, .1, 0, .5, .25) +
  draw_plot(plotM3W3, .3, 0, .5, .25) + 
  draw_plot_label(label = c("Original Data", "M1: Wave 2", "M1: Wave 3", "M2: Wave 2", "M2: Wave 3", "M3: Wave 2", "M3: Wave 3"))

M1avg2$Model <- "M1"
M2avg2$Model <- "M2"
M3avg2$Model <- "M3"
allavg <- rbind(as.data.frame(M1avg2), 
                as.data.frame(M2avg2),
                as.data.frame(M3avg2))


ggplot(data = allavg) + 
  geom_histogram(aes(x = weight), binwidth = 25) + 
  facet_grid(wave~Model, labeller = "label_both") + 
  labs(x = "Number of times an edge occurs in 1,000 simulations") +
  theme_bw()

# ggplot(data = M3avgcov) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/251, color = as.factor(drink)), 
#            directed = TRUE, curvature = .2,
#            ealpha = .75,
#            layout.alg = "kamadakawai",
#            layout.par = list(elen = distmatM2)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   scale_size_continuous(name = "Weight") + 
#   theme_net()
# M1avg2$wave <- M1avg2$wave + 1
# alldrink <- data.frame(drink2[,2:3])
# alldrink <- gather(alldrink, wave,drink) %>% mutate(wave = parse_number(wave), id = rep(paste0("V",1:16),2))

# M1avg2cov <- merge(M1avg2, alldrink, by.x = c("from", "wave"), by.y = c("id", "wave"), all = T)
# ggplot(data = M1avg2cov) + # edge appears > 50% of the time. (is this actually what that is????) 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/388, color = as.factor(drink)), 
#            directed = TRUE, curvature = .2, labelon = F,
#            #layout.alg = "fruchtermanreingold"
#            layout.alg = "kamadakawai", fiteach = T,
#            layout.par = list(elen = distmat)
#            ) + 
#   scale_color_brewer(palette = "YlOrRd", name = "Drinking\nBehavior") + 
#   facet_wrap(~wave, labeller = "label_both") + theme_net() + labs(title = "Average Wave 2 and Wave 3 Networks from Model M1")
# ggplot(data = M1avg %>% filter(weight >= 51)) + 
#   geom_net(aes(from_id = from, to_id = to, linewidth = weight/500), 
#            #directed = TRUE,
#            layout.alg = "mds", 
#            layout.par = list(var = 'user', dist = 'none', exp = 3,vm = distmat))

@


\st{What are some things that could go in here?:
\begin{itemize}
\item "average" networks -- you've already done this! Write it up, make several examples. How does the procedure for averaging generalize? What is it good for? 
\item distributions of fitted parameters. again, you've already done this!!! just write it up and put it in here. 
\item view correlations between model parameters under different model sets. eg 2 betas, 3 betas, 4 betas, etc.
\end{itemize}
 }



\section{Discussion} \label{sec:discussion}

\bibliographystyle{asa}
\bibliography{references}

\end{document}
